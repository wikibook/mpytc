{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# reinforcement learning related imports\n",
    "import re\n",
    "import atari_py as ap\n",
    "from collections import deque\n",
    "from gym import make, ObservationWrapper, Wrapper\n",
    "from gym.spaces import Box\n",
    "\n",
    "# pytorch imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import save\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, ip_sz, tot_num_acts):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self._ip_sz = ip_sz\n",
    "        self._tot_num_acts = tot_num_acts\n",
    "\n",
    "        self.cnv1 = nn.Conv2d(ip_sz[0], 32, kernel_size=8, stride=4)\n",
    "        self.rl = nn.ReLU()\n",
    "        self.cnv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.cnv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(self.feat_sz, 512)\n",
    "        self.fc2 = nn.Linear(512, tot_num_acts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        op = self.cnv1(x)\n",
    "        op = self.rl(op)\n",
    "        op = self.cnv2(op)\n",
    "        op = self.rl(op)\n",
    "        op = self.cnv3(op)\n",
    "        op = self.rl(op).view(x.size()[0], -1)\n",
    "        op = self.fc1(op)\n",
    "        op = self.rl(op)\n",
    "        op = self.fc2(op)\n",
    "        return op\n",
    "\n",
    "    @property\n",
    "    def feat_sz(self):\n",
    "        x = torch.zeros(1, *self._ip_sz)\n",
    "        x = self.cnv1(x)\n",
    "        x = self.rl(x)\n",
    "        x = self.cnv2(x)\n",
    "        x = self.rl(x)\n",
    "        x = self.cnv3(x)\n",
    "        x = self.rl(x)\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "    def perf_action(self, stt, eps, dvc):\n",
    "        if random.random() > eps:\n",
    "            stt = torch.from_numpy(np.float32(stt)).unsqueeze(0).to(dvc)\n",
    "            q_val = self.forward(stt)\n",
    "            act = q_val.max(1)[1].item()\n",
    "        else:\n",
    "            act = random.randrange(self._tot_num_acts)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_temp_diff_loss(mdl, tgt_mdl, bch, gm, dvc):\n",
    "    st, act, rwd, nxt_st, fin = bch\n",
    "\n",
    "    st = torch.from_numpy(np.float32(st)).to(dvc)\n",
    "    nxt_st = torch.from_numpy(np.float32(nxt_st)).to(dvc)\n",
    "    act = torch.from_numpy(act).to(dvc)\n",
    "    rwd = torch.from_numpy(rwd).to(dvc)\n",
    "    fin = torch.from_numpy(fin).to(dvc)\n",
    "\n",
    "    q_vals = mdl(st)\n",
    "    nxt_q_vals = tgt_mdl(nxt_st)\n",
    "\n",
    "    q_val = q_vals.gather(1, act.unsqueeze(-1)).squeeze(-1)\n",
    "    nxt_q_val = nxt_q_vals.max(1)[0]\n",
    "    exp_q_val = rwd + gm * nxt_q_val * (1 - fin)\n",
    "\n",
    "    loss = (q_val - exp_q_val.data.to(dvc)).pow(2).mean()\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "def upd_eps(epd):\n",
    "    last_eps = EPS_FINL\n",
    "    first_eps = EPS_STRT\n",
    "    eps_decay = EPS_DECAY\n",
    "    eps = last_eps + (first_eps - last_eps) * math.exp(-1 * ((epd + 1) / eps_decay))\n",
    "    return eps\n",
    "\n",
    "\n",
    "def models_init(env, dvc):\n",
    "    mdl = ConvDQN(env.observation_space.shape, env.action_space.n).to(dvc)\n",
    "    tgt_mdl = ConvDQN(env.observation_space.shape, env.action_space.n).to(dvc)\n",
    "    return mdl, tgt_mdl\n",
    "\n",
    "\n",
    "def gym_to_atari_format(gym_env):\n",
    "    return re.sub(r\"(?<!^)(?=[A-Z])\", \"_\", gym_env).lower()\n",
    "\n",
    "\n",
    "def check_atari_env(env):\n",
    "    for f in [\"Deterministic\", \"ramDeterministic\", \"ram\", \"NoFrameskip\", \"ramNoFrameSkip\"]:\n",
    "        env = env.replace(f, \"\")\n",
    "    env = re.sub(r\"-v\\d+\", \"\", env)\n",
    "    env = gym_to_atari_format(env) \n",
    "    return True if env in ap.list_games() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepBfr:\n",
    "    def __init__(self, cap_max):\n",
    "        self._bfr = deque(maxlen=cap_max)\n",
    "\n",
    "    def push(self, st, act, rwd, nxt_st, fin):\n",
    "        self._bfr.append((st, act, rwd, nxt_st, fin))\n",
    "\n",
    "    def smpl(self, bch_sz):\n",
    "        idxs = np.random.choice(len(self._bfr), bch_sz, False)\n",
    "        bch = zip(*[self._bfr[i] for i in idxs])\n",
    "        st, act, rwd, nxt_st, fin = bch\n",
    "        return (np.array(st), np.array(act), np.array(rwd, dtype=np.float32),\n",
    "                np.array(nxt_st), np.array(fin, dtype=np.uint8))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._bfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrMetadata:\n",
    "    def __init__(self):\n",
    "        self._avg = 0.0\n",
    "        self._bst_rwd = -float(\"inf\")\n",
    "        self._bst_avg = -float(\"inf\")\n",
    "        self._rwds = []\n",
    "        self._avg_rng = 100\n",
    "        self._idx = 0\n",
    "\n",
    "    @property\n",
    "    def bst_rwd(self):\n",
    "        return self._bst_rwd\n",
    "\n",
    "    @property\n",
    "    def bst_avg(self):\n",
    "        return self._bst_avg\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        avg_rng = self._avg_rng * -1\n",
    "        return sum(self._rwds[avg_rng:]) / len(self._rwds[avg_rng:])\n",
    "\n",
    "    @property\n",
    "    def idx(self):\n",
    "        return self._idx\n",
    "\n",
    "    def _upd_bst_rwd(self, epd_rwd):\n",
    "        if epd_rwd > self.bst_rwd:\n",
    "            self._bst_rwd = epd_rwd\n",
    "\n",
    "    def _upd_bst_avg(self):\n",
    "        if self.avg > self.bst_avg:\n",
    "            self._bst_avg = self.avg\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def upd_rwds(self, epd_rwd):\n",
    "        self._rwds.append(epd_rwd)\n",
    "        self._upd_bst_rwd(epd_rwd)\n",
    "        return self._upd_bst_avg()\n",
    "\n",
    "    def upd_idx(self):\n",
    "        self._idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCtrl(Wrapper):\n",
    "    def __init__(self, env, is_atari):\n",
    "        super(CCtrl, self).__init__(env)\n",
    "        self._is_atari = is_atari\n",
    "\n",
    "    def reset(self):\n",
    "        if self._is_atari:\n",
    "            return self.env.reset()\n",
    "        else:\n",
    "            self.env.reset()\n",
    "            return self.env.render(mode=\"rgb_array\")\n",
    "\n",
    "\n",
    "class FrmDwSmpl(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FrmDwSmpl, self).__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        self._width = 84\n",
    "        self._height = 84\n",
    "\n",
    "    def observation(self, observation):\n",
    "        frame = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self._width, self._height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "\n",
    "class MaxNSkpEnv(Wrapper):\n",
    "    def __init__(self, env, atari, skip=4):\n",
    "        super(MaxNSkpEnv, self).__init__(env)\n",
    "        self._obs_buffer = deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "        self._atari = atari\n",
    "\n",
    "    def step(self, act):\n",
    "        total_rwd = 0.0\n",
    "        fin = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, rwd, fin, log = self.env.step(act)\n",
    "            if not self._atari:\n",
    "                obs = self.env.render(mode=\"rgb_array\")\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_rwd += rwd\n",
    "            if fin:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_rwd, fin, log\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class FrRstEnv(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        Wrapper.__init__(self, env)\n",
    "        if len(env.unwrapped.get_action_meanings()) < 3:\n",
    "            raise ValueError(\"min required action space of 3!\")\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, fin, _ = self.env.step(1)\n",
    "        if fin:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, fin, _ = self.env.step(2)\n",
    "        if fin:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, act):\n",
    "        return self.env.step(act)\n",
    "\n",
    "\n",
    "class FrmBfr(ObservationWrapper):\n",
    "    def __init__(self, env, num_steps, dtype=np.float32):\n",
    "        super(FrmBfr, self).__init__(env)\n",
    "        obs_space = env.observation_space\n",
    "        self._dtype = dtype\n",
    "        self.observation_space = Box(obs_space.low.repeat(num_steps, axis=0),\n",
    "                                     obs_space.high.repeat(num_steps, axis=0), dtype=self._dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self._dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class Img2Trch(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(Img2Trch, self).__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = Box(low=0.0, high=1.0, shape=(obs_shape[::-1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class NormFlts(ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "        \n",
    "\n",
    "def wrap_env(env_ip):\n",
    "    env = make(env_ip)\n",
    "    is_atari = check_atari_env(env_ip)\n",
    "    env = CCtrl(env, is_atari)\n",
    "    env = MaxNSkpEnv(env, is_atari)\n",
    "    try:\n",
    "        env_acts = env.unwrapped.get_action_meanings()\n",
    "        if \"FIRE\" in env_acts:\n",
    "            env = FrRstEnv(env)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    env = FrmDwSmpl(env)\n",
    "    env = Img2Trch(env)\n",
    "    env = FrmBfr(env, 4)\n",
    "    env = NormFlts(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upd_grph(mdl, tgt_mdl, opt, rpl_bfr, dvc, log):\n",
    "    if len(rpl_bfr) > INIT_LEARN:\n",
    "        if not log.idx % TGT_UPD_FRQ:\n",
    "            tgt_mdl.load_state_dict(mdl.state_dict())\n",
    "        opt.zero_grad()\n",
    "        bch = rpl_bfr.smpl(B_S)\n",
    "        calc_temp_diff_loss(mdl, tgt_mdl, bch, G, dvc)\n",
    "        opt.step()\n",
    "\n",
    "\n",
    "def fin_epsd(mdl, env, log, epd_rwd, epd, eps):\n",
    "    bst_so_fat = log.upd_rwds(epd_rwd)\n",
    "    if bst_so_fat:\n",
    "        print(f\"checkpointing current model weights. highest running_average_reward of\\\n",
    " {round(log.bst_avg, 3)} achieved!\")\n",
    "        save(mdl.state_dict(), f\"{env}.dat\")\n",
    "    print(f\"episode_num {epd}, curr_reward: {epd_rwd}, best_reward: {log.bst_rwd},\\\n",
    " running_avg_reward: {round(log.avg, 3)}, curr_epsilon: {round(eps, 4)}\")\n",
    "\n",
    "\n",
    "def run_epsd(env, mdl, tgt_mdl, opt, rpl_bfr, dvc, log, epd):\n",
    "    epd_rwd = 0.0\n",
    "    st = env.reset()\n",
    "\n",
    "    while True:\n",
    "        eps = upd_eps(log.idx)\n",
    "        act = mdl.perf_action(st, eps, dvc)\n",
    "        if True:\n",
    "            env.render()\n",
    "        nxt_st, rwd, fin, _ = env.step(act)\n",
    "        rpl_bfr.push(st, act, rwd, nxt_st, fin)\n",
    "        st = nxt_st\n",
    "        epd_rwd += rwd\n",
    "        log.upd_idx()\n",
    "        upd_grph(mdl, tgt_mdl, opt, rpl_bfr, dvc, log)\n",
    "        if fin:\n",
    "            fin_epsd(mdl, ENV, log, epd_rwd, epd, eps)\n",
    "            break\n",
    "\n",
    "\n",
    "def train(env, mdl, tgt_mdl, opt, rpl_bfr, dvc):\n",
    "    log = TrMetadata()\n",
    "\n",
    "    for epd in range(N_EPDS):\n",
    "        run_epsd(env, mdl, tgt_mdl, opt, rpl_bfr, dvc, log, epd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_S = 64\n",
    "ENV = \"Pong-v4\"\n",
    "EPS_STRT = 1.0\n",
    "EPS_FINL = 0.005\n",
    "EPS_DECAY = 100000\n",
    "G = 0.99\n",
    "INIT_LEARN = 10000\n",
    "LR = 1e-4\n",
    "MEM_CAP = 20000\n",
    "N_EPDS = 50000\n",
    "TGT_UPD_FRQ = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointing current model weights. highest running_average_reward of -20.0 achieved!\n",
      "episode_num 0, curr_reward: -20.0, best_reward: -20.0, running_avg_reward: -20.0, curr_epsilon: 0.9971\n",
      "checkpointing current model weights. highest running_average_reward of -19.5 achieved!\n",
      "episode_num 1, curr_reward: -19.0, best_reward: -19.0, running_avg_reward: -19.5, curr_epsilon: 0.9937\n",
      "episode_num 2, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.0, curr_epsilon: 0.991\n",
      "episode_num 3, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.25, curr_epsilon: 0.9881\n",
      "episode_num 4, curr_reward: -19.0, best_reward: -19.0, running_avg_reward: -20.0, curr_epsilon: 0.9846\n",
      "episode_num 5, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.0, curr_epsilon: 0.9811\n",
      "episode_num 6, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.143, curr_epsilon: 0.9779\n",
      "episode_num 7, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.25, curr_epsilon: 0.9751\n",
      "episode_num 8, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.333, curr_epsilon: 0.9719\n",
      "episode_num 9, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.3, curr_epsilon: 0.9684\n",
      "episode_num 10, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.364, curr_epsilon: 0.9658\n",
      "episode_num 11, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.417, curr_epsilon: 0.9627\n",
      "episode_num 12, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.462, curr_epsilon: 0.9599\n",
      "episode_num 13, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.429, curr_epsilon: 0.957\n",
      "episode_num 14, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.467, curr_epsilon: 0.9544\n",
      "episode_num 15, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.438, curr_epsilon: 0.9515\n",
      "episode_num 16, curr_reward: -19.0, best_reward: -19.0, running_avg_reward: -20.353, curr_epsilon: 0.9482\n",
      "episode_num 17, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.333, curr_epsilon: 0.9449\n",
      "episode_num 18, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.368, curr_epsilon: 0.9419\n",
      "episode_num 19, curr_reward: -19.0, best_reward: -19.0, running_avg_reward: -20.3, curr_epsilon: 0.9386\n",
      "episode_num 20, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.333, curr_epsilon: 0.9362\n",
      "episode_num 21, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.318, curr_epsilon: 0.933\n",
      "episode_num 22, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.348, curr_epsilon: 0.9305\n",
      "episode_num 23, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.333, curr_epsilon: 0.9276\n",
      "episode_num 24, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.32, curr_epsilon: 0.9245\n",
      "episode_num 25, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.346, curr_epsilon: 0.9219\n",
      "episode_num 26, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.37, curr_epsilon: 0.9191\n",
      "episode_num 27, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.357, curr_epsilon: 0.9161\n",
      "episode_num 28, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.379, curr_epsilon: 0.9134\n",
      "episode_num 29, curr_reward: -19.0, best_reward: -19.0, running_avg_reward: -20.333, curr_epsilon: 0.9105\n",
      "episode_num 30, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.355, curr_epsilon: 0.908\n",
      "episode_num 31, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.375, curr_epsilon: 0.9056\n",
      "episode_num 32, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.394, curr_epsilon: 0.9031\n",
      "episode_num 33, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.412, curr_epsilon: 0.9008\n",
      "episode_num 34, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.429, curr_epsilon: 0.8985\n",
      "episode_num 35, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.444, curr_epsilon: 0.8961\n",
      "episode_num 36, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.459, curr_epsilon: 0.8937\n",
      "episode_num 37, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.474, curr_epsilon: 0.8915\n",
      "episode_num 38, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.487, curr_epsilon: 0.8883\n",
      "episode_num 39, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.475, curr_epsilon: 0.8857\n",
      "episode_num 40, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.488, curr_epsilon: 0.8831\n",
      "episode_num 41, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.476, curr_epsilon: 0.8805\n",
      "episode_num 42, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.465, curr_epsilon: 0.8775\n",
      "episode_num 43, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.477, curr_epsilon: 0.8753\n",
      "episode_num 44, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.489, curr_epsilon: 0.8728\n",
      "episode_num 45, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.5, curr_epsilon: 0.8706\n",
      "episode_num 46, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.489, curr_epsilon: 0.868\n",
      "episode_num 47, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.5, curr_epsilon: 0.8656\n",
      "episode_num 48, curr_reward: -19.0, best_reward: -19.0, running_avg_reward: -20.469, curr_epsilon: 0.8628\n",
      "episode_num 49, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.48, curr_epsilon: 0.86\n",
      "episode_num 50, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.471, curr_epsilon: 0.857\n",
      "episode_num 51, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.481, curr_epsilon: 0.8545\n",
      "episode_num 52, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.472, curr_epsilon: 0.852\n",
      "episode_num 53, curr_reward: -21.0, best_reward: -19.0, running_avg_reward: -20.481, curr_epsilon: 0.8498\n",
      "episode_num 54, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.473, curr_epsilon: 0.8472\n",
      "episode_num 55, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.464, curr_epsilon: 0.8444\n",
      "episode_num 56, curr_reward: -20.0, best_reward: -19.0, running_avg_reward: -20.456, curr_epsilon: 0.8415\n",
      "episode_num 57, curr_reward: -17.0, best_reward: -17.0, running_avg_reward: -20.397, curr_epsilon: 0.8383\n",
      "episode_num 58, curr_reward: -19.0, best_reward: -17.0, running_avg_reward: -20.373, curr_epsilon: 0.8356\n",
      "episode_num 59, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.367, curr_epsilon: 0.833\n",
      "episode_num 60, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.377, curr_epsilon: 0.8306\n",
      "episode_num 61, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.387, curr_epsilon: 0.828\n",
      "episode_num 62, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.397, curr_epsilon: 0.8251\n",
      "episode_num 63, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.391, curr_epsilon: 0.8221\n",
      "episode_num 64, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.385, curr_epsilon: 0.8193\n",
      "episode_num 65, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.394, curr_epsilon: 0.8171\n",
      "episode_num 66, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.403, curr_epsilon: 0.8148\n",
      "episode_num 67, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.412, curr_epsilon: 0.8124\n",
      "episode_num 68, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.42, curr_epsilon: 0.8101\n",
      "episode_num 69, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.429, curr_epsilon: 0.8075\n",
      "episode_num 70, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.437, curr_epsilon: 0.8051\n",
      "episode_num 71, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.431, curr_epsilon: 0.8027\n",
      "episode_num 72, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.425, curr_epsilon: 0.8\n",
      "episode_num 73, curr_reward: -19.0, best_reward: -17.0, running_avg_reward: -20.405, curr_epsilon: 0.7971\n",
      "episode_num 74, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.413, curr_epsilon: 0.7944\n",
      "episode_num 75, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.408, curr_epsilon: 0.7921\n",
      "episode_num 76, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.403, curr_epsilon: 0.7895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 77, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.41, curr_epsilon: 0.7872\n",
      "episode_num 78, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.418, curr_epsilon: 0.785\n",
      "episode_num 79, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.425, curr_epsilon: 0.7829\n",
      "episode_num 80, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.42, curr_epsilon: 0.7807\n",
      "episode_num 81, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.427, curr_epsilon: 0.7784\n",
      "episode_num 82, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.422, curr_epsilon: 0.7758\n",
      "episode_num 83, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.429, curr_epsilon: 0.7737\n",
      "episode_num 84, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.424, curr_epsilon: 0.7714\n",
      "episode_num 85, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.43, curr_epsilon: 0.7692\n",
      "episode_num 86, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.425, curr_epsilon: 0.7665\n",
      "episode_num 87, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.432, curr_epsilon: 0.7641\n",
      "episode_num 88, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.438, curr_epsilon: 0.7616\n",
      "episode_num 89, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.444, curr_epsilon: 0.7585\n",
      "episode_num 90, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.451, curr_epsilon: 0.7562\n",
      "episode_num 91, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.457, curr_epsilon: 0.754\n",
      "episode_num 92, curr_reward: -18.0, best_reward: -17.0, running_avg_reward: -20.43, curr_epsilon: 0.751\n",
      "episode_num 93, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.436, curr_epsilon: 0.7487\n",
      "episode_num 94, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.432, curr_epsilon: 0.7463\n",
      "episode_num 95, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.438, curr_epsilon: 0.744\n",
      "episode_num 96, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.433, curr_epsilon: 0.7419\n",
      "episode_num 97, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.439, curr_epsilon: 0.7398\n",
      "episode_num 98, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.444, curr_epsilon: 0.7377\n",
      "episode_num 99, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.45, curr_epsilon: 0.7355\n",
      "episode_num 100, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.46, curr_epsilon: 0.7332\n",
      "episode_num 101, curr_reward: -19.0, best_reward: -17.0, running_avg_reward: -20.46, curr_epsilon: 0.7308\n",
      "episode_num 102, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.45, curr_epsilon: 0.7277\n",
      "episode_num 103, curr_reward: -18.0, best_reward: -17.0, running_avg_reward: -20.42, curr_epsilon: 0.7245\n",
      "episode_num 104, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.44, curr_epsilon: 0.7226\n",
      "episode_num 105, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.45, curr_epsilon: 0.7205\n",
      "episode_num 106, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.44, curr_epsilon: 0.7184\n",
      "episode_num 107, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.44, curr_epsilon: 0.7164\n",
      "episode_num 108, curr_reward: -18.0, best_reward: -17.0, running_avg_reward: -20.41, curr_epsilon: 0.7132\n",
      "episode_num 109, curr_reward: -18.0, best_reward: -17.0, running_avg_reward: -20.39, curr_epsilon: 0.7106\n",
      "episode_num 110, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.38, curr_epsilon: 0.7083\n",
      "episode_num 111, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.37, curr_epsilon: 0.7056\n",
      "episode_num 112, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.36, curr_epsilon: 0.703\n",
      "episode_num 113, curr_reward: -19.0, best_reward: -17.0, running_avg_reward: -20.35, curr_epsilon: 0.7006\n",
      "episode_num 114, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.34, curr_epsilon: 0.6982\n",
      "episode_num 115, curr_reward: -19.0, best_reward: -17.0, running_avg_reward: -20.33, curr_epsilon: 0.6953\n",
      "episode_num 116, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.34, curr_epsilon: 0.6931\n",
      "episode_num 117, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.34, curr_epsilon: 0.6912\n",
      "episode_num 118, curr_reward: -18.0, best_reward: -17.0, running_avg_reward: -20.31, curr_epsilon: 0.6882\n",
      "episode_num 119, curr_reward: -19.0, best_reward: -17.0, running_avg_reward: -20.31, curr_epsilon: 0.6859\n",
      "episode_num 120, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.3, curr_epsilon: 0.6833\n",
      "episode_num 121, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.3, curr_epsilon: 0.6807\n",
      "episode_num 122, curr_reward: -17.0, best_reward: -17.0, running_avg_reward: -20.26, curr_epsilon: 0.6779\n",
      "episode_num 123, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.27, curr_epsilon: 0.676\n",
      "episode_num 124, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.28, curr_epsilon: 0.6735\n",
      "episode_num 125, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.28, curr_epsilon: 0.6714\n",
      "episode_num 126, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.28, curr_epsilon: 0.6695\n",
      "episode_num 127, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.29, curr_epsilon: 0.6674\n",
      "episode_num 128, curr_reward: -19.0, best_reward: -17.0, running_avg_reward: -20.27, curr_epsilon: 0.665\n",
      "episode_num 129, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.29, curr_epsilon: 0.6631\n",
      "episode_num 130, curr_reward: -19.0, best_reward: -17.0, running_avg_reward: -20.27, curr_epsilon: 0.6606\n",
      "episode_num 131, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.26, curr_epsilon: 0.6583\n",
      "episode_num 132, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.26, curr_epsilon: 0.6562\n",
      "episode_num 133, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.26, curr_epsilon: 0.6541\n",
      "episode_num 134, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.25, curr_epsilon: 0.6516\n",
      "episode_num 135, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.24, curr_epsilon: 0.6495\n",
      "episode_num 136, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.24, curr_epsilon: 0.6479\n",
      "episode_num 137, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.23, curr_epsilon: 0.6456\n",
      "episode_num 138, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.23, curr_epsilon: 0.6434\n",
      "episode_num 139, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.24, curr_epsilon: 0.6418\n",
      "episode_num 140, curr_reward: -18.0, best_reward: -17.0, running_avg_reward: -20.21, curr_epsilon: 0.6392\n",
      "episode_num 141, curr_reward: -17.0, best_reward: -17.0, running_avg_reward: -20.18, curr_epsilon: 0.6365\n",
      "episode_num 142, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.19, curr_epsilon: 0.6346\n",
      "episode_num 143, curr_reward: -19.0, best_reward: -17.0, running_avg_reward: -20.17, curr_epsilon: 0.6323\n",
      "episode_num 144, curr_reward: -18.0, best_reward: -17.0, running_avg_reward: -20.14, curr_epsilon: 0.6296\n",
      "episode_num 145, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.14, curr_epsilon: 0.6272\n",
      "episode_num 146, curr_reward: -19.0, best_reward: -17.0, running_avg_reward: -20.13, curr_epsilon: 0.6251\n",
      "episode_num 147, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.12, curr_epsilon: 0.6227\n",
      "episode_num 148, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.13, curr_epsilon: 0.6204\n",
      "episode_num 149, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.12, curr_epsilon: 0.6182\n",
      "episode_num 150, curr_reward: -18.0, best_reward: -17.0, running_avg_reward: -20.1, curr_epsilon: 0.6157\n",
      "episode_num 151, curr_reward: -21.0, best_reward: -17.0, running_avg_reward: -20.1, curr_epsilon: 0.6136\n",
      "episode_num 152, curr_reward: -20.0, best_reward: -17.0, running_avg_reward: -20.1, curr_epsilon: 0.611\n",
      "episode_num 153, curr_reward: -16.0, best_reward: -16.0, running_avg_reward: -20.05, curr_epsilon: 0.608\n",
      "episode_num 154, curr_reward: -21.0, best_reward: -16.0, running_avg_reward: -20.06, curr_epsilon: 0.6061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 155, curr_reward: -20.0, best_reward: -16.0, running_avg_reward: -20.06, curr_epsilon: 0.6041\n",
      "episode_num 156, curr_reward: -19.0, best_reward: -16.0, running_avg_reward: -20.05, curr_epsilon: 0.6017\n",
      "episode_num 157, curr_reward: -21.0, best_reward: -16.0, running_avg_reward: -20.09, curr_epsilon: 0.5995\n",
      "episode_num 158, curr_reward: -19.0, best_reward: -16.0, running_avg_reward: -20.09, curr_epsilon: 0.5975\n",
      "episode_num 159, curr_reward: -20.0, best_reward: -16.0, running_avg_reward: -20.09, curr_epsilon: 0.5952\n",
      "episode_num 160, curr_reward: -21.0, best_reward: -16.0, running_avg_reward: -20.09, curr_epsilon: 0.5931\n",
      "episode_num 161, curr_reward: -19.0, best_reward: -16.0, running_avg_reward: -20.07, curr_epsilon: 0.5907\n",
      "episode_num 162, curr_reward: -18.0, best_reward: -16.0, running_avg_reward: -20.04, curr_epsilon: 0.5884\n",
      "episode_num 163, curr_reward: -20.0, best_reward: -16.0, running_avg_reward: -20.04, curr_epsilon: 0.5863\n",
      "episode_num 164, curr_reward: -18.0, best_reward: -16.0, running_avg_reward: -20.02, curr_epsilon: 0.584\n",
      "episode_num 165, curr_reward: -19.0, best_reward: -16.0, running_avg_reward: -20.0, curr_epsilon: 0.5817\n",
      "episode_num 166, curr_reward: -20.0, best_reward: -16.0, running_avg_reward: -19.99, curr_epsilon: 0.5795\n",
      "episode_num 167, curr_reward: -20.0, best_reward: -16.0, running_avg_reward: -19.98, curr_epsilon: 0.5774\n",
      "episode_num 168, curr_reward: -19.0, best_reward: -16.0, running_avg_reward: -19.96, curr_epsilon: 0.575\n",
      "episode_num 169, curr_reward: -21.0, best_reward: -16.0, running_avg_reward: -19.96, curr_epsilon: 0.5733\n",
      "episode_num 170, curr_reward: -21.0, best_reward: -16.0, running_avg_reward: -19.96, curr_epsilon: 0.5717\n",
      "episode_num 171, curr_reward: -20.0, best_reward: -16.0, running_avg_reward: -19.96, curr_epsilon: 0.5698\n",
      "episode_num 172, curr_reward: -20.0, best_reward: -16.0, running_avg_reward: -19.96, curr_epsilon: 0.5678\n",
      "episode_num 173, curr_reward: -20.0, best_reward: -16.0, running_avg_reward: -19.97, curr_epsilon: 0.5658\n",
      "episode_num 174, curr_reward: -18.0, best_reward: -16.0, running_avg_reward: -19.94, curr_epsilon: 0.5631\n",
      "episode_num 175, curr_reward: -20.0, best_reward: -16.0, running_avg_reward: -19.94, curr_epsilon: 0.561\n",
      "episode_num 176, curr_reward: -20.0, best_reward: -16.0, running_avg_reward: -19.94, curr_epsilon: 0.559\n",
      "episode_num 177, curr_reward: -21.0, best_reward: -16.0, running_avg_reward: -19.94, curr_epsilon: 0.5569\n",
      "episode_num 178, curr_reward: -19.0, best_reward: -16.0, running_avg_reward: -19.92, curr_epsilon: 0.5545\n",
      "episode_num 179, curr_reward: -18.0, best_reward: -16.0, running_avg_reward: -19.89, curr_epsilon: 0.5523\n",
      "episode_num 180, curr_reward: -19.0, best_reward: -16.0, running_avg_reward: -19.88, curr_epsilon: 0.5499\n",
      "episode_num 181, curr_reward: -20.0, best_reward: -16.0, running_avg_reward: -19.87, curr_epsilon: 0.5481\n",
      "episode_num 182, curr_reward: -15.0, best_reward: -15.0, running_avg_reward: -19.82, curr_epsilon: 0.5457\n",
      "episode_num 183, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.8, curr_epsilon: 0.5438\n",
      "episode_num 184, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.8, curr_epsilon: 0.5418\n",
      "episode_num 185, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.79, curr_epsilon: 0.5393\n",
      "episode_num 186, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.77, curr_epsilon: 0.5371\n",
      "episode_num 187, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.74, curr_epsilon: 0.5347\n",
      "episode_num 188, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.73, curr_epsilon: 0.5325\n",
      "episode_num 189, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.71, curr_epsilon: 0.5303\n",
      "episode_num 190, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.7, curr_epsilon: 0.5286\n",
      "episode_num 191, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.68, curr_epsilon: 0.5263\n",
      "episode_num 192, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.68, curr_epsilon: 0.5241\n",
      "episode_num 193, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.67, curr_epsilon: 0.5217\n",
      "episode_num 194, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.66, curr_epsilon: 0.5198\n",
      "episode_num 195, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.66, curr_epsilon: 0.518\n",
      "episode_num 196, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.65, curr_epsilon: 0.5163\n",
      "episode_num 197, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.64, curr_epsilon: 0.5143\n",
      "episode_num 198, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.63, curr_epsilon: 0.5125\n",
      "episode_num 199, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.63, curr_epsilon: 0.5106\n",
      "episode_num 200, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.6, curr_epsilon: 0.5087\n",
      "episode_num 201, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.62, curr_epsilon: 0.5069\n",
      "episode_num 202, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.63, curr_epsilon: 0.5052\n",
      "episode_num 203, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.66, curr_epsilon: 0.5034\n",
      "episode_num 204, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.64, curr_epsilon: 0.5008\n",
      "episode_num 205, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.64, curr_epsilon: 0.4991\n",
      "episode_num 206, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.64, curr_epsilon: 0.4975\n",
      "episode_num 207, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.63, curr_epsilon: 0.4957\n",
      "episode_num 208, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.64, curr_epsilon: 0.4937\n",
      "episode_num 209, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.66, curr_epsilon: 0.4915\n",
      "episode_num 210, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.66, curr_epsilon: 0.4894\n",
      "episode_num 211, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.67, curr_epsilon: 0.4876\n",
      "episode_num 212, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.67, curr_epsilon: 0.4853\n",
      "episode_num 213, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.69, curr_epsilon: 0.4837\n",
      "episode_num 214, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.67, curr_epsilon: 0.4814\n",
      "episode_num 215, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.68, curr_epsilon: 0.4795\n",
      "episode_num 216, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.68, curr_epsilon: 0.4777\n",
      "episode_num 217, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.68, curr_epsilon: 0.4759\n",
      "episode_num 218, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.7, curr_epsilon: 0.4737\n",
      "episode_num 219, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.7, curr_epsilon: 0.4717\n",
      "episode_num 220, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.7, curr_epsilon: 0.4701\n",
      "episode_num 221, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.71, curr_epsilon: 0.4686\n",
      "episode_num 222, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.74, curr_epsilon: 0.4672\n",
      "episode_num 223, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -19.69, curr_epsilon: 0.4647\n",
      "episode_num 224, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.69, curr_epsilon: 0.463\n",
      "episode_num 225, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.67, curr_epsilon: 0.4609\n",
      "episode_num 226, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.67, curr_epsilon: 0.459\n",
      "episode_num 227, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.66, curr_epsilon: 0.4573\n",
      "episode_num 228, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.67, curr_epsilon: 0.4556\n",
      "episode_num 229, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -19.63, curr_epsilon: 0.4534\n",
      "episode_num 230, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.65, curr_epsilon: 0.4517\n",
      "episode_num 231, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.65, curr_epsilon: 0.45\n",
      "episode_num 232, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.64, curr_epsilon: 0.4483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 233, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.61, curr_epsilon: 0.4462\n",
      "episode_num 234, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.61, curr_epsilon: 0.4446\n",
      "episode_num 235, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.6, curr_epsilon: 0.4428\n",
      "episode_num 236, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.6, curr_epsilon: 0.4413\n",
      "episode_num 237, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.61, curr_epsilon: 0.4399\n",
      "episode_num 238, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.58, curr_epsilon: 0.438\n",
      "episode_num 239, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.56, curr_epsilon: 0.4366\n",
      "episode_num 240, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.58, curr_epsilon: 0.4343\n",
      "episode_num 241, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.59, curr_epsilon: 0.4325\n",
      "episode_num 242, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.59, curr_epsilon: 0.4309\n",
      "episode_num 243, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.59, curr_epsilon: 0.4291\n",
      "episode_num 244, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.62, curr_epsilon: 0.4274\n",
      "episode_num 245, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.62, curr_epsilon: 0.426\n",
      "episode_num 246, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.63, curr_epsilon: 0.4244\n",
      "episode_num 247, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.62, curr_epsilon: 0.4226\n",
      "episode_num 248, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.6, curr_epsilon: 0.4206\n",
      "episode_num 249, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.6, curr_epsilon: 0.4189\n",
      "episode_num 250, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.61, curr_epsilon: 0.4172\n",
      "episode_num 251, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.61, curr_epsilon: 0.4157\n",
      "episode_num 252, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.6, curr_epsilon: 0.4143\n",
      "episode_num 253, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.63, curr_epsilon: 0.4125\n",
      "episode_num 254, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.63, curr_epsilon: 0.411\n",
      "episode_num 255, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.63, curr_epsilon: 0.4093\n",
      "episode_num 256, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.63, curr_epsilon: 0.4076\n",
      "episode_num 257, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -19.58, curr_epsilon: 0.4055\n",
      "episode_num 258, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -19.56, curr_epsilon: 0.4035\n",
      "episode_num 259, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.57, curr_epsilon: 0.4017\n",
      "episode_num 260, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.55, curr_epsilon: 0.3996\n",
      "episode_num 261, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.54, curr_epsilon: 0.398\n",
      "episode_num 262, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.55, curr_epsilon: 0.3962\n",
      "episode_num 263, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.53, curr_epsilon: 0.3945\n",
      "episode_num 264, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.54, curr_epsilon: 0.3931\n",
      "episode_num 265, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.55, curr_epsilon: 0.3914\n",
      "episode_num 266, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.56, curr_epsilon: 0.3902\n",
      "episode_num 267, curr_reward: -15.0, best_reward: -15.0, running_avg_reward: -19.51, curr_epsilon: 0.3884\n",
      "episode_num 268, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.51, curr_epsilon: 0.3866\n",
      "checkpointing current model weights. highest running_average_reward of -19.48 achieved!\n",
      "episode_num 269, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.48, curr_epsilon: 0.3847\n",
      "checkpointing current model weights. highest running_average_reward of -19.46 achieved!\n",
      "episode_num 270, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.46, curr_epsilon: 0.3829\n",
      "episode_num 271, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.47, curr_epsilon: 0.3815\n",
      "episode_num 272, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.47, curr_epsilon: 0.3802\n",
      "episode_num 273, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.47, curr_epsilon: 0.3788\n",
      "episode_num 274, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.48, curr_epsilon: 0.377\n",
      "episode_num 275, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.46, curr_epsilon: 0.3749\n",
      "checkpointing current model weights. highest running_average_reward of -19.45 achieved!\n",
      "episode_num 276, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.45, curr_epsilon: 0.3732\n",
      "checkpointing current model weights. highest running_average_reward of -19.42 achieved!\n",
      "episode_num 277, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.42, curr_epsilon: 0.3715\n",
      "episode_num 278, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.43, curr_epsilon: 0.37\n",
      "episode_num 279, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.44, curr_epsilon: 0.3683\n",
      "episode_num 280, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.43, curr_epsilon: 0.3662\n",
      "checkpointing current model weights. highest running_average_reward of -19.41 achieved!\n",
      "episode_num 281, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.41, curr_epsilon: 0.3644\n",
      "episode_num 282, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.46, curr_epsilon: 0.3628\n",
      "episode_num 283, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -19.44, curr_epsilon: 0.3608\n",
      "episode_num 284, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.42, curr_epsilon: 0.3589\n",
      "episode_num 285, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.41, curr_epsilon: 0.3572\n",
      "checkpointing current model weights. highest running_average_reward of -19.4 achieved!\n",
      "episode_num 286, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -19.4, curr_epsilon: 0.3557\n",
      "episode_num 287, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.4, curr_epsilon: 0.3541\n",
      "checkpointing current model weights. highest running_average_reward of -19.36 achieved!\n",
      "episode_num 288, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -19.36, curr_epsilon: 0.3522\n",
      "checkpointing current model weights. highest running_average_reward of -19.33 achieved!\n",
      "episode_num 289, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -19.33, curr_epsilon: 0.3504\n",
      "checkpointing current model weights. highest running_average_reward of -19.32 achieved!\n",
      "episode_num 290, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.32, curr_epsilon: 0.349\n",
      "episode_num 291, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.34, curr_epsilon: 0.3475\n",
      "episode_num 292, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.35, curr_epsilon: 0.3461\n",
      "episode_num 293, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -19.32, curr_epsilon: 0.3446\n",
      "checkpointing current model weights. highest running_average_reward of -19.3 achieved!\n",
      "episode_num 294, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -19.3, curr_epsilon: 0.3432\n",
      "checkpointing current model weights. highest running_average_reward of -19.28 achieved!\n",
      "episode_num 295, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.28, curr_epsilon: 0.3416\n",
      "episode_num 296, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.28, curr_epsilon: 0.3402\n",
      "episode_num 297, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.28, curr_epsilon: 0.339\n",
      "checkpointing current model weights. highest running_average_reward of -19.25 achieved!\n",
      "episode_num 298, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -19.25, curr_epsilon: 0.3373\n",
      "checkpointing current model weights. highest running_average_reward of -19.23 achieved!\n",
      "episode_num 299, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.23, curr_epsilon: 0.3361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 300, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.24, curr_epsilon: 0.3347\n",
      "episode_num 301, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.23, curr_epsilon: 0.3331\n",
      "checkpointing current model weights. highest running_average_reward of -19.22 achieved!\n",
      "episode_num 302, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.22, curr_epsilon: 0.3318\n",
      "checkpointing current model weights. highest running_average_reward of -19.2 achieved!\n",
      "episode_num 303, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.2, curr_epsilon: 0.3305\n",
      "episode_num 304, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.2, curr_epsilon: 0.3288\n",
      "checkpointing current model weights. highest running_average_reward of -19.18 achieved!\n",
      "episode_num 305, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.18, curr_epsilon: 0.3274\n",
      "episode_num 306, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -19.18, curr_epsilon: 0.3256\n",
      "checkpointing current model weights. highest running_average_reward of -19.17 achieved!\n",
      "episode_num 307, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.17, curr_epsilon: 0.3237\n",
      "checkpointing current model weights. highest running_average_reward of -19.16 achieved!\n",
      "episode_num 308, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.16, curr_epsilon: 0.3222\n",
      "checkpointing current model weights. highest running_average_reward of -19.13 achieved!\n",
      "episode_num 309, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -19.13, curr_epsilon: 0.3208\n",
      "checkpointing current model weights. highest running_average_reward of -19.12 achieved!\n",
      "episode_num 310, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.12, curr_epsilon: 0.3191\n",
      "checkpointing current model weights. highest running_average_reward of -19.09 achieved!\n",
      "episode_num 311, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.09, curr_epsilon: 0.3176\n",
      "checkpointing current model weights. highest running_average_reward of -19.05 achieved!\n",
      "episode_num 312, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -19.05, curr_epsilon: 0.3157\n",
      "checkpointing current model weights. highest running_average_reward of -19.02 achieved!\n",
      "episode_num 313, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.02, curr_epsilon: 0.3138\n",
      "episode_num 314, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -19.02, curr_epsilon: 0.3125\n",
      "episode_num 315, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -19.03, curr_epsilon: 0.3108\n",
      "episode_num 316, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -19.02, curr_epsilon: 0.3095\n",
      "checkpointing current model weights. highest running_average_reward of -18.99 achieved!\n",
      "episode_num 317, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -18.99, curr_epsilon: 0.3079\n",
      "checkpointing current model weights. highest running_average_reward of -18.97 achieved!\n",
      "episode_num 318, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.97, curr_epsilon: 0.3065\n",
      "checkpointing current model weights. highest running_average_reward of -18.95 achieved!\n",
      "episode_num 319, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -18.95, curr_epsilon: 0.305\n",
      "checkpointing current model weights. highest running_average_reward of -18.92 achieved!\n",
      "episode_num 320, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -18.92, curr_epsilon: 0.3034\n",
      "checkpointing current model weights. highest running_average_reward of -18.9 achieved!\n",
      "episode_num 321, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.9, curr_epsilon: 0.3023\n",
      "checkpointing current model weights. highest running_average_reward of -18.89 achieved!\n",
      "episode_num 322, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.89, curr_epsilon: 0.3008\n",
      "episode_num 323, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -18.89, curr_epsilon: 0.2991\n",
      "episode_num 324, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -18.89, curr_epsilon: 0.2981\n",
      "episode_num 325, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -18.91, curr_epsilon: 0.2966\n",
      "episode_num 326, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -18.9, curr_epsilon: 0.2956\n",
      "episode_num 327, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.89, curr_epsilon: 0.2941\n",
      "checkpointing current model weights. highest running_average_reward of -18.87 achieved!\n",
      "episode_num 328, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.87, curr_epsilon: 0.2925\n",
      "episode_num 329, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.89, curr_epsilon: 0.291\n",
      "episode_num 330, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -18.88, curr_epsilon: 0.2895\n",
      "episode_num 331, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.87, curr_epsilon: 0.2881\n",
      "checkpointing current model weights. highest running_average_reward of -18.83 achieved!\n",
      "episode_num 332, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -18.83, curr_epsilon: 0.2866\n",
      "episode_num 333, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.84, curr_epsilon: 0.2852\n",
      "checkpointing current model weights. highest running_average_reward of -18.82 achieved!\n",
      "episode_num 334, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.82, curr_epsilon: 0.2837\n",
      "episode_num 335, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -18.83, curr_epsilon: 0.2825\n",
      "checkpointing current model weights. highest running_average_reward of -18.78 achieved!\n",
      "episode_num 336, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -18.78, curr_epsilon: 0.2808\n",
      "checkpointing current model weights. highest running_average_reward of -18.77 achieved!\n",
      "episode_num 337, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -18.77, curr_epsilon: 0.2793\n",
      "checkpointing current model weights. highest running_average_reward of -18.75 achieved!\n",
      "episode_num 338, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -18.75, curr_epsilon: 0.278\n",
      "checkpointing current model weights. highest running_average_reward of -18.74 achieved!\n",
      "episode_num 339, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.74, curr_epsilon: 0.2764\n",
      "checkpointing current model weights. highest running_average_reward of -18.72 achieved!\n",
      "episode_num 340, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.72, curr_epsilon: 0.275\n",
      "episode_num 341, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -18.74, curr_epsilon: 0.2736\n",
      "checkpointing current model weights. highest running_average_reward of -18.71 achieved!\n",
      "episode_num 342, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.71, curr_epsilon: 0.2721\n",
      "checkpointing current model weights. highest running_average_reward of -18.68 achieved!\n",
      "episode_num 343, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -18.68, curr_epsilon: 0.2705\n",
      "episode_num 344, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -18.68, curr_epsilon: 0.2694\n",
      "checkpointing current model weights. highest running_average_reward of -18.67 achieved!\n",
      "episode_num 345, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -18.67, curr_epsilon: 0.2681\n",
      "checkpointing current model weights. highest running_average_reward of -18.65 achieved!\n",
      "episode_num 346, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.65, curr_epsilon: 0.2669\n",
      "episode_num 347, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.65, curr_epsilon: 0.2656\n",
      "checkpointing current model weights. highest running_average_reward of -18.62 achieved!\n",
      "episode_num 348, curr_reward: -15.0, best_reward: -15.0, running_avg_reward: -18.62, curr_epsilon: 0.2641\n",
      "checkpointing current model weights. highest running_average_reward of -18.61 achieved!\n",
      "episode_num 349, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.61, curr_epsilon: 0.2627\n",
      "episode_num 350, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -18.63, curr_epsilon: 0.2616\n",
      "checkpointing current model weights. highest running_average_reward of -18.6 achieved!\n",
      "episode_num 351, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.6, curr_epsilon: 0.2603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointing current model weights. highest running_average_reward of -18.56 achieved!\n",
      "episode_num 352, curr_reward: -15.0, best_reward: -15.0, running_avg_reward: -18.56, curr_epsilon: 0.259\n",
      "episode_num 353, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -18.57, curr_epsilon: 0.2579\n",
      "checkpointing current model weights. highest running_average_reward of -18.54 achieved!\n",
      "episode_num 354, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.54, curr_epsilon: 0.2566\n",
      "episode_num 355, curr_reward: -21.0, best_reward: -15.0, running_avg_reward: -18.55, curr_epsilon: 0.2556\n",
      "checkpointing current model weights. highest running_average_reward of -18.53 achieved!\n",
      "episode_num 356, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -18.53, curr_epsilon: 0.2541\n",
      "episode_num 357, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.56, curr_epsilon: 0.2529\n",
      "episode_num 358, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.58, curr_epsilon: 0.2516\n",
      "episode_num 359, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.56, curr_epsilon: 0.2505\n",
      "episode_num 360, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.55, curr_epsilon: 0.2493\n",
      "episode_num 361, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -18.54, curr_epsilon: 0.2478\n",
      "checkpointing current model weights. highest running_average_reward of -18.52 achieved!\n",
      "episode_num 362, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -18.52, curr_epsilon: 0.2464\n",
      "checkpointing current model weights. highest running_average_reward of -18.51 achieved!\n",
      "episode_num 363, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -18.51, curr_epsilon: 0.2448\n",
      "checkpointing current model weights. highest running_average_reward of -18.49 achieved!\n",
      "episode_num 364, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -18.49, curr_epsilon: 0.2435\n",
      "checkpointing current model weights. highest running_average_reward of -18.45 achieved!\n",
      "episode_num 365, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -18.45, curr_epsilon: 0.2422\n",
      "checkpointing current model weights. highest running_average_reward of -18.44 achieved!\n",
      "episode_num 366, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -18.44, curr_epsilon: 0.2408\n",
      "episode_num 367, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -18.46, curr_epsilon: 0.2395\n",
      "episode_num 368, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.46, curr_epsilon: 0.2383\n",
      "episode_num 369, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -18.48, curr_epsilon: 0.2372\n",
      "episode_num 370, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -18.49, curr_epsilon: 0.236\n",
      "episode_num 371, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.46, curr_epsilon: 0.2348\n",
      "checkpointing current model weights. highest running_average_reward of -18.42 achieved!\n",
      "episode_num 372, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -18.42, curr_epsilon: 0.2332\n",
      "checkpointing current model weights. highest running_average_reward of -18.4 achieved!\n",
      "episode_num 373, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.4, curr_epsilon: 0.2319\n",
      "episode_num 374, curr_reward: -20.0, best_reward: -15.0, running_avg_reward: -18.41, curr_epsilon: 0.2307\n",
      "checkpointing current model weights. highest running_average_reward of -18.39 achieved!\n",
      "episode_num 375, curr_reward: -16.0, best_reward: -15.0, running_avg_reward: -18.39, curr_epsilon: 0.2294\n",
      "checkpointing current model weights. highest running_average_reward of -18.37 achieved!\n",
      "episode_num 376, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -18.37, curr_epsilon: 0.2281\n",
      "episode_num 377, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.37, curr_epsilon: 0.227\n",
      "checkpointing current model weights. highest running_average_reward of -18.34 achieved!\n",
      "episode_num 378, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -18.34, curr_epsilon: 0.2257\n",
      "checkpointing current model weights. highest running_average_reward of -18.33 achieved!\n",
      "episode_num 379, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.33, curr_epsilon: 0.2245\n",
      "episode_num 380, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.34, curr_epsilon: 0.2231\n",
      "episode_num 381, curr_reward: -17.0, best_reward: -15.0, running_avg_reward: -18.33, curr_epsilon: 0.2218\n",
      "checkpointing current model weights. highest running_average_reward of -18.31 achieved!\n",
      "episode_num 382, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.31, curr_epsilon: 0.2207\n",
      "episode_num 383, curr_reward: -18.0, best_reward: -15.0, running_avg_reward: -18.32, curr_epsilon: 0.2194\n",
      "episode_num 384, curr_reward: -19.0, best_reward: -15.0, running_avg_reward: -18.33, curr_epsilon: 0.2182\n",
      "checkpointing current model weights. highest running_average_reward of -18.28 achieved!\n",
      "episode_num 385, curr_reward: -14.0, best_reward: -14.0, running_avg_reward: -18.28, curr_epsilon: 0.2165\n",
      "episode_num 386, curr_reward: -20.0, best_reward: -14.0, running_avg_reward: -18.31, curr_epsilon: 0.2153\n",
      "episode_num 387, curr_reward: -21.0, best_reward: -14.0, running_avg_reward: -18.34, curr_epsilon: 0.2144\n",
      "episode_num 388, curr_reward: -17.0, best_reward: -14.0, running_avg_reward: -18.35, curr_epsilon: 0.2133\n",
      "episode_num 389, curr_reward: -19.0, best_reward: -14.0, running_avg_reward: -18.38, curr_epsilon: 0.2123\n",
      "episode_num 390, curr_reward: -19.0, best_reward: -14.0, running_avg_reward: -18.38, curr_epsilon: 0.2111\n",
      "episode_num 391, curr_reward: -16.0, best_reward: -14.0, running_avg_reward: -18.33, curr_epsilon: 0.2097\n",
      "episode_num 392, curr_reward: -16.0, best_reward: -14.0, running_avg_reward: -18.3, curr_epsilon: 0.2084\n",
      "episode_num 393, curr_reward: -18.0, best_reward: -14.0, running_avg_reward: -18.31, curr_epsilon: 0.2075\n",
      "episode_num 394, curr_reward: -20.0, best_reward: -14.0, running_avg_reward: -18.34, curr_epsilon: 0.2065\n",
      "episode_num 395, curr_reward: -17.0, best_reward: -14.0, running_avg_reward: -18.32, curr_epsilon: 0.2053\n",
      "episode_num 396, curr_reward: -18.0, best_reward: -14.0, running_avg_reward: -18.31, curr_epsilon: 0.2041\n",
      "episode_num 397, curr_reward: -19.0, best_reward: -14.0, running_avg_reward: -18.3, curr_epsilon: 0.2029\n",
      "episode_num 398, curr_reward: -16.0, best_reward: -14.0, running_avg_reward: -18.29, curr_epsilon: 0.2015\n",
      "episode_num 399, curr_reward: -21.0, best_reward: -14.0, running_avg_reward: -18.31, curr_epsilon: 0.2003\n",
      "episode_num 400, curr_reward: -16.0, best_reward: -14.0, running_avg_reward: -18.28, curr_epsilon: 0.1991\n",
      "episode_num 401, curr_reward: -20.0, best_reward: -14.0, running_avg_reward: -18.28, curr_epsilon: 0.1981\n",
      "checkpointing current model weights. highest running_average_reward of -18.2 achieved!\n",
      "episode_num 402, curr_reward: -12.0, best_reward: -12.0, running_avg_reward: -18.2, curr_epsilon: 0.1967\n",
      "checkpointing current model weights. highest running_average_reward of -18.19 achieved!\n",
      "episode_num 403, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -18.19, curr_epsilon: 0.1957\n",
      "checkpointing current model weights. highest running_average_reward of -18.15 achieved!\n",
      "episode_num 404, curr_reward: -15.0, best_reward: -12.0, running_avg_reward: -18.15, curr_epsilon: 0.1945\n",
      "episode_num 405, curr_reward: -19.0, best_reward: -12.0, running_avg_reward: -18.15, curr_epsilon: 0.1934\n",
      "episode_num 406, curr_reward: -20.0, best_reward: -12.0, running_avg_reward: -18.15, curr_epsilon: 0.1924\n",
      "checkpointing current model weights. highest running_average_reward of -18.13 achieved!\n",
      "episode_num 407, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -18.13, curr_epsilon: 0.1914\n",
      "checkpointing current model weights. highest running_average_reward of -18.12 achieved!\n",
      "episode_num 408, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -18.12, curr_epsilon: 0.1904\n",
      "episode_num 409, curr_reward: -20.0, best_reward: -12.0, running_avg_reward: -18.15, curr_epsilon: 0.1894\n",
      "episode_num 410, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -18.13, curr_epsilon: 0.1881\n",
      "episode_num 411, curr_reward: -19.0, best_reward: -12.0, running_avg_reward: -18.14, curr_epsilon: 0.1872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 412, curr_reward: -19.0, best_reward: -12.0, running_avg_reward: -18.17, curr_epsilon: 0.1862\n",
      "episode_num 413, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -18.17, curr_epsilon: 0.1852\n",
      "checkpointing current model weights. highest running_average_reward of -18.11 achieved!\n",
      "episode_num 414, curr_reward: -12.0, best_reward: -12.0, running_avg_reward: -18.11, curr_epsilon: 0.1839\n",
      "checkpointing current model weights. highest running_average_reward of -18.08 achieved!\n",
      "episode_num 415, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -18.08, curr_epsilon: 0.1829\n",
      "episode_num 416, curr_reward: -19.0, best_reward: -12.0, running_avg_reward: -18.08, curr_epsilon: 0.1819\n",
      "checkpointing current model weights. highest running_average_reward of -18.07 achieved!\n",
      "episode_num 417, curr_reward: -16.0, best_reward: -12.0, running_avg_reward: -18.07, curr_epsilon: 0.181\n",
      "episode_num 418, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -18.07, curr_epsilon: 0.1798\n",
      "episode_num 419, curr_reward: -21.0, best_reward: -12.0, running_avg_reward: -18.11, curr_epsilon: 0.1788\n",
      "episode_num 420, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -18.11, curr_epsilon: 0.1777\n",
      "episode_num 421, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -18.09, curr_epsilon: 0.1766\n",
      "checkpointing current model weights. highest running_average_reward of -18.03 achieved!\n",
      "episode_num 422, curr_reward: -13.0, best_reward: -12.0, running_avg_reward: -18.03, curr_epsilon: 0.1754\n",
      "episode_num 423, curr_reward: -16.0, best_reward: -12.0, running_avg_reward: -18.03, curr_epsilon: 0.1743\n",
      "checkpointing current model weights. highest running_average_reward of -18.01 achieved!\n",
      "episode_num 424, curr_reward: -19.0, best_reward: -12.0, running_avg_reward: -18.01, curr_epsilon: 0.1733\n",
      "checkpointing current model weights. highest running_average_reward of -17.97 achieved!\n",
      "episode_num 425, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -17.97, curr_epsilon: 0.1725\n",
      "checkpointing current model weights. highest running_average_reward of -17.95 achieved!\n",
      "episode_num 426, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -17.95, curr_epsilon: 0.1716\n",
      "checkpointing current model weights. highest running_average_reward of -17.91 achieved!\n",
      "episode_num 427, curr_reward: -15.0, best_reward: -12.0, running_avg_reward: -17.91, curr_epsilon: 0.1702\n",
      "checkpointing current model weights. highest running_average_reward of -17.89 achieved!\n",
      "episode_num 428, curr_reward: -16.0, best_reward: -12.0, running_avg_reward: -17.89, curr_epsilon: 0.1691\n",
      "checkpointing current model weights. highest running_average_reward of -17.87 achieved!\n",
      "episode_num 429, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -17.87, curr_epsilon: 0.1678\n",
      "checkpointing current model weights. highest running_average_reward of -17.86 achieved!\n",
      "episode_num 430, curr_reward: -19.0, best_reward: -12.0, running_avg_reward: -17.86, curr_epsilon: 0.1669\n",
      "checkpointing current model weights. highest running_average_reward of -17.8 achieved!\n",
      "episode_num 431, curr_reward: -13.0, best_reward: -12.0, running_avg_reward: -17.8, curr_epsilon: 0.1657\n",
      "episode_num 432, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -17.82, curr_epsilon: 0.1648\n",
      "episode_num 433, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -17.81, curr_epsilon: 0.1637\n",
      "episode_num 434, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -17.81, curr_epsilon: 0.1629\n",
      "checkpointing current model weights. highest running_average_reward of -17.79 achieved!\n",
      "episode_num 435, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -17.79, curr_epsilon: 0.1619\n",
      "episode_num 436, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -17.81, curr_epsilon: 0.1609\n",
      "checkpointing current model weights. highest running_average_reward of -17.73 achieved!\n",
      "episode_num 437, curr_reward: -12.0, best_reward: -12.0, running_avg_reward: -17.73, curr_epsilon: 0.1597\n",
      "checkpointing current model weights. highest running_average_reward of -17.72 achieved!\n",
      "episode_num 438, curr_reward: -15.0, best_reward: -12.0, running_avg_reward: -17.72, curr_epsilon: 0.1585\n",
      "episode_num 439, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -17.72, curr_epsilon: 0.1575\n",
      "episode_num 440, curr_reward: -19.0, best_reward: -12.0, running_avg_reward: -17.73, curr_epsilon: 0.1568\n",
      "checkpointing current model weights. highest running_average_reward of -17.67 achieved!\n",
      "episode_num 441, curr_reward: -14.0, best_reward: -12.0, running_avg_reward: -17.67, curr_epsilon: 0.1558\n",
      "checkpointing current model weights. highest running_average_reward of -17.65 achieved!\n",
      "episode_num 442, curr_reward: -16.0, best_reward: -12.0, running_avg_reward: -17.65, curr_epsilon: 0.1546\n",
      "checkpointing current model weights. highest running_average_reward of -17.64 achieved!\n",
      "episode_num 443, curr_reward: -15.0, best_reward: -12.0, running_avg_reward: -17.64, curr_epsilon: 0.1535\n",
      "checkpointing current model weights. highest running_average_reward of -17.6 achieved!\n",
      "episode_num 444, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -17.6, curr_epsilon: 0.1524\n",
      "checkpointing current model weights. highest running_average_reward of -17.57 achieved!\n",
      "episode_num 445, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -17.57, curr_epsilon: 0.1514\n",
      "episode_num 446, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -17.57, curr_epsilon: 0.1505\n",
      "checkpointing current model weights. highest running_average_reward of -17.52 achieved!\n",
      "episode_num 447, curr_reward: -14.0, best_reward: -12.0, running_avg_reward: -17.52, curr_epsilon: 0.1496\n",
      "episode_num 448, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -17.55, curr_epsilon: 0.1487\n",
      "episode_num 449, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -17.53, curr_epsilon: 0.1478\n",
      "checkpointing current model weights. highest running_average_reward of -17.49 achieved!\n",
      "episode_num 450, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -17.49, curr_epsilon: 0.1468\n",
      "episode_num 451, curr_reward: -19.0, best_reward: -12.0, running_avg_reward: -17.5, curr_epsilon: 0.1459\n",
      "episode_num 452, curr_reward: -21.0, best_reward: -12.0, running_avg_reward: -17.56, curr_epsilon: 0.1452\n",
      "episode_num 453, curr_reward: -18.0, best_reward: -12.0, running_avg_reward: -17.54, curr_epsilon: 0.1444\n",
      "episode_num 454, curr_reward: -13.0, best_reward: -12.0, running_avg_reward: -17.49, curr_epsilon: 0.1434\n",
      "checkpointing current model weights. highest running_average_reward of -17.4 achieved!\n",
      "episode_num 455, curr_reward: -12.0, best_reward: -12.0, running_avg_reward: -17.4, curr_epsilon: 0.1424\n",
      "episode_num 456, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -17.4, curr_epsilon: 0.1414\n",
      "checkpointing current model weights. highest running_average_reward of -17.38 achieved!\n",
      "episode_num 457, curr_reward: -17.0, best_reward: -12.0, running_avg_reward: -17.38, curr_epsilon: 0.1406\n",
      "checkpointing current model weights. highest running_average_reward of -17.3 achieved!\n",
      "episode_num 458, curr_reward: -11.0, best_reward: -11.0, running_avg_reward: -17.3, curr_epsilon: 0.1393\n",
      "checkpointing current model weights. highest running_average_reward of -17.26 achieved!\n",
      "episode_num 459, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -17.26, curr_epsilon: 0.1384\n",
      "episode_num 460, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -17.26, curr_epsilon: 0.1373\n",
      "episode_num 461, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -17.27, curr_epsilon: 0.1365\n",
      "episode_num 462, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -17.28, curr_epsilon: 0.1357\n",
      "episode_num 463, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -17.28, curr_epsilon: 0.1349\n",
      "episode_num 464, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -17.26, curr_epsilon: 0.134\n",
      "checkpointing current model weights. highest running_average_reward of -17.25 achieved!\n",
      "episode_num 465, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -17.25, curr_epsilon: 0.1331\n",
      "checkpointing current model weights. highest running_average_reward of -17.21 achieved!\n",
      "episode_num 466, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -17.21, curr_epsilon: 0.1323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointing current model weights. highest running_average_reward of -17.16 achieved!\n",
      "episode_num 467, curr_reward: -12.0, best_reward: -11.0, running_avg_reward: -17.16, curr_epsilon: 0.1313\n",
      "checkpointing current model weights. highest running_average_reward of -17.15 achieved!\n",
      "episode_num 468, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -17.15, curr_epsilon: 0.1306\n",
      "checkpointing current model weights. highest running_average_reward of -17.12 achieved!\n",
      "episode_num 469, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -17.12, curr_epsilon: 0.1297\n",
      "checkpointing current model weights. highest running_average_reward of -17.1 achieved!\n",
      "episode_num 470, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -17.1, curr_epsilon: 0.129\n",
      "checkpointing current model weights. highest running_average_reward of -17.08 achieved!\n",
      "episode_num 471, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -17.08, curr_epsilon: 0.128\n",
      "checkpointing current model weights. highest running_average_reward of -17.07 achieved!\n",
      "episode_num 472, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -17.07, curr_epsilon: 0.127\n",
      "episode_num 473, curr_reward: -19.0, best_reward: -11.0, running_avg_reward: -17.08, curr_epsilon: 0.1263\n",
      "checkpointing current model weights. highest running_average_reward of -17.05 achieved!\n",
      "episode_num 474, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -17.05, curr_epsilon: 0.1255\n",
      "checkpointing current model weights. highest running_average_reward of -17.01 achieved!\n",
      "episode_num 475, curr_reward: -12.0, best_reward: -11.0, running_avg_reward: -17.01, curr_epsilon: 0.1245\n",
      "checkpointing current model weights. highest running_average_reward of -17.0 achieved!\n",
      "episode_num 476, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -17.0, curr_epsilon: 0.1235\n",
      "checkpointing current model weights. highest running_average_reward of -16.97 achieved!\n",
      "episode_num 477, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -16.97, curr_epsilon: 0.1225\n",
      "checkpointing current model weights. highest running_average_reward of -16.96 achieved!\n",
      "episode_num 478, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -16.96, curr_epsilon: 0.1216\n",
      "checkpointing current model weights. highest running_average_reward of -16.95 achieved!\n",
      "episode_num 479, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -16.95, curr_epsilon: 0.1209\n",
      "checkpointing current model weights. highest running_average_reward of -16.91 achieved!\n",
      "episode_num 480, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -16.91, curr_epsilon: 0.12\n",
      "episode_num 481, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -16.91, curr_epsilon: 0.1192\n",
      "checkpointing current model weights. highest running_average_reward of -16.9 achieved!\n",
      "episode_num 482, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -16.9, curr_epsilon: 0.1185\n",
      "checkpointing current model weights. highest running_average_reward of -16.86 achieved!\n",
      "episode_num 483, curr_reward: -14.0, best_reward: -11.0, running_avg_reward: -16.86, curr_epsilon: 0.1177\n",
      "checkpointing current model weights. highest running_average_reward of -16.78 achieved!\n",
      "episode_num 484, curr_reward: -11.0, best_reward: -11.0, running_avg_reward: -16.78, curr_epsilon: 0.1167\n",
      "episode_num 485, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -16.81, curr_epsilon: 0.1161\n",
      "episode_num 486, curr_reward: -19.0, best_reward: -11.0, running_avg_reward: -16.8, curr_epsilon: 0.1154\n",
      "episode_num 487, curr_reward: -20.0, best_reward: -11.0, running_avg_reward: -16.79, curr_epsilon: 0.1148\n",
      "checkpointing current model weights. highest running_average_reward of -16.76 achieved!\n",
      "episode_num 488, curr_reward: -14.0, best_reward: -11.0, running_avg_reward: -16.76, curr_epsilon: 0.114\n",
      "checkpointing current model weights. highest running_average_reward of -16.71 achieved!\n",
      "episode_num 489, curr_reward: -14.0, best_reward: -11.0, running_avg_reward: -16.71, curr_epsilon: 0.1132\n",
      "checkpointing current model weights. highest running_average_reward of -16.69 achieved!\n",
      "episode_num 490, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -16.69, curr_epsilon: 0.1125\n",
      "episode_num 491, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -16.69, curr_epsilon: 0.1118\n",
      "checkpointing current model weights. highest running_average_reward of -16.64 achieved!\n",
      "episode_num 492, curr_reward: -11.0, best_reward: -11.0, running_avg_reward: -16.64, curr_epsilon: 0.1108\n",
      "episode_num 493, curr_reward: -21.0, best_reward: -11.0, running_avg_reward: -16.67, curr_epsilon: 0.1102\n",
      "episode_num 494, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -16.65, curr_epsilon: 0.1097\n",
      "episode_num 495, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -16.66, curr_epsilon: 0.1091\n",
      "checkpointing current model weights. highest running_average_reward of -16.61 achieved!\n",
      "episode_num 496, curr_reward: -13.0, best_reward: -11.0, running_avg_reward: -16.61, curr_epsilon: 0.1082\n",
      "checkpointing current model weights. highest running_average_reward of -16.56 achieved!\n",
      "episode_num 497, curr_reward: -14.0, best_reward: -11.0, running_avg_reward: -16.56, curr_epsilon: 0.1073\n",
      "episode_num 498, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -16.57, curr_epsilon: 0.1067\n",
      "checkpointing current model weights. highest running_average_reward of -16.55 achieved!\n",
      "episode_num 499, curr_reward: -19.0, best_reward: -11.0, running_avg_reward: -16.55, curr_epsilon: 0.106\n",
      "checkpointing current model weights. highest running_average_reward of -16.52 achieved!\n",
      "episode_num 500, curr_reward: -13.0, best_reward: -11.0, running_avg_reward: -16.52, curr_epsilon: 0.1053\n",
      "episode_num 501, curr_reward: -20.0, best_reward: -11.0, running_avg_reward: -16.52, curr_epsilon: 0.1049\n",
      "episode_num 502, curr_reward: -19.0, best_reward: -11.0, running_avg_reward: -16.59, curr_epsilon: 0.1041\n",
      "episode_num 503, curr_reward: -12.0, best_reward: -11.0, running_avg_reward: -16.53, curr_epsilon: 0.1034\n",
      "checkpointing current model weights. highest running_average_reward of -16.51 achieved!\n",
      "episode_num 504, curr_reward: -13.0, best_reward: -11.0, running_avg_reward: -16.51, curr_epsilon: 0.1026\n",
      "checkpointing current model weights. highest running_average_reward of -16.5 achieved!\n",
      "episode_num 505, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -16.5, curr_epsilon: 0.1019\n",
      "checkpointing current model weights. highest running_average_reward of -16.46 achieved!\n",
      "episode_num 506, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -16.46, curr_epsilon: 0.1013\n",
      "checkpointing current model weights. highest running_average_reward of -16.43 achieved!\n",
      "episode_num 507, curr_reward: -14.0, best_reward: -11.0, running_avg_reward: -16.43, curr_epsilon: 0.1006\n",
      "episode_num 508, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -16.44, curr_epsilon: 0.0999\n",
      "checkpointing current model weights. highest running_average_reward of -16.39 achieved!\n",
      "episode_num 509, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -16.39, curr_epsilon: 0.099\n",
      "episode_num 510, curr_reward: -20.0, best_reward: -11.0, running_avg_reward: -16.42, curr_epsilon: 0.0985\n",
      "checkpointing current model weights. highest running_average_reward of -16.34 achieved!\n",
      "episode_num 511, curr_reward: -11.0, best_reward: -11.0, running_avg_reward: -16.34, curr_epsilon: 0.0977\n",
      "checkpointing current model weights. highest running_average_reward of -16.28 achieved!\n",
      "episode_num 512, curr_reward: -13.0, best_reward: -11.0, running_avg_reward: -16.28, curr_epsilon: 0.097\n",
      "checkpointing current model weights. highest running_average_reward of -16.25 achieved!\n",
      "episode_num 513, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -16.25, curr_epsilon: 0.0963\n",
      "episode_num 514, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -16.28, curr_epsilon: 0.0954\n",
      "episode_num 515, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -16.26, curr_epsilon: 0.0947\n",
      "checkpointing current model weights. highest running_average_reward of -16.21 achieved!\n",
      "episode_num 516, curr_reward: -14.0, best_reward: -11.0, running_avg_reward: -16.21, curr_epsilon: 0.0941\n",
      "episode_num 517, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -16.22, curr_epsilon: 0.0935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointing current model weights. highest running_average_reward of -16.18 achieved!\n",
      "episode_num 518, curr_reward: -14.0, best_reward: -11.0, running_avg_reward: -16.18, curr_epsilon: 0.0927\n",
      "checkpointing current model weights. highest running_average_reward of -16.14 achieved!\n",
      "episode_num 519, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -16.14, curr_epsilon: 0.0922\n",
      "episode_num 520, curr_reward: -21.0, best_reward: -11.0, running_avg_reward: -16.18, curr_epsilon: 0.0917\n",
      "episode_num 521, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -16.17, curr_epsilon: 0.0911\n",
      "episode_num 522, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -16.21, curr_epsilon: 0.0905\n",
      "episode_num 523, curr_reward: -13.0, best_reward: -11.0, running_avg_reward: -16.18, curr_epsilon: 0.0898\n",
      "checkpointing current model weights. highest running_average_reward of -16.12 achieved!\n",
      "episode_num 524, curr_reward: -13.0, best_reward: -11.0, running_avg_reward: -16.12, curr_epsilon: 0.0891\n",
      "episode_num 525, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -16.13, curr_epsilon: 0.0886\n",
      "episode_num 526, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -16.12, curr_epsilon: 0.0881\n",
      "episode_num 527, curr_reward: -19.0, best_reward: -11.0, running_avg_reward: -16.16, curr_epsilon: 0.0875\n",
      "checkpointing current model weights. highest running_average_reward of -16.11 achieved!\n",
      "episode_num 528, curr_reward: -11.0, best_reward: -11.0, running_avg_reward: -16.11, curr_epsilon: 0.0868\n",
      "episode_num 529, curr_reward: -19.0, best_reward: -11.0, running_avg_reward: -16.13, curr_epsilon: 0.0863\n",
      "checkpointing current model weights. highest running_average_reward of -16.1 achieved!\n",
      "episode_num 530, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -16.1, curr_epsilon: 0.0856\n",
      "episode_num 531, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -16.12, curr_epsilon: 0.0851\n",
      "checkpointing current model weights. highest running_average_reward of -16.09 achieved!\n",
      "episode_num 532, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -16.09, curr_epsilon: 0.0845\n",
      "checkpointing current model weights. highest running_average_reward of -16.02 achieved!\n",
      "episode_num 533, curr_reward: -11.0, best_reward: -11.0, running_avg_reward: -16.02, curr_epsilon: 0.0839\n",
      "checkpointing current model weights. highest running_average_reward of -15.95 achieved!\n",
      "episode_num 534, curr_reward: -11.0, best_reward: -11.0, running_avg_reward: -15.95, curr_epsilon: 0.0831\n",
      "checkpointing current model weights. highest running_average_reward of -15.94 achieved!\n",
      "episode_num 535, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -15.94, curr_epsilon: 0.0825\n",
      "checkpointing current model weights. highest running_average_reward of -15.9 achieved!\n",
      "episode_num 536, curr_reward: -14.0, best_reward: -11.0, running_avg_reward: -15.9, curr_epsilon: 0.0819\n",
      "episode_num 537, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -15.93, curr_epsilon: 0.0814\n",
      "episode_num 538, curr_reward: -20.0, best_reward: -11.0, running_avg_reward: -15.98, curr_epsilon: 0.081\n",
      "episode_num 539, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -15.95, curr_epsilon: 0.0804\n",
      "episode_num 540, curr_reward: -14.0, best_reward: -11.0, running_avg_reward: -15.9, curr_epsilon: 0.0799\n",
      "checkpointing current model weights. highest running_average_reward of -15.89 achieved!\n",
      "episode_num 541, curr_reward: -13.0, best_reward: -11.0, running_avg_reward: -15.89, curr_epsilon: 0.0792\n",
      "checkpointing current model weights. highest running_average_reward of -15.84 achieved!\n",
      "episode_num 542, curr_reward: -11.0, best_reward: -11.0, running_avg_reward: -15.84, curr_epsilon: 0.0784\n",
      "episode_num 543, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -15.84, curr_epsilon: 0.0778\n",
      "episode_num 544, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -15.84, curr_epsilon: 0.0773\n",
      "checkpointing current model weights. highest running_average_reward of -15.83 achieved!\n",
      "episode_num 545, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -15.83, curr_epsilon: 0.0768\n",
      "checkpointing current model weights. highest running_average_reward of -15.82 achieved!\n",
      "episode_num 546, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -15.82, curr_epsilon: 0.0763\n",
      "episode_num 547, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -15.85, curr_epsilon: 0.0758\n",
      "episode_num 548, curr_reward: -19.0, best_reward: -11.0, running_avg_reward: -15.86, curr_epsilon: 0.0753\n",
      "episode_num 549, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -15.87, curr_epsilon: 0.0749\n",
      "episode_num 550, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -15.86, curr_epsilon: 0.0744\n",
      "episode_num 551, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -15.85, curr_epsilon: 0.074\n",
      "checkpointing current model weights. highest running_average_reward of -15.8 achieved!\n",
      "episode_num 552, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -15.8, curr_epsilon: 0.0735\n",
      "checkpointing current model weights. highest running_average_reward of -15.75 achieved!\n",
      "episode_num 553, curr_reward: -13.0, best_reward: -11.0, running_avg_reward: -15.75, curr_epsilon: 0.0729\n",
      "episode_num 554, curr_reward: -14.0, best_reward: -11.0, running_avg_reward: -15.76, curr_epsilon: 0.0723\n",
      "episode_num 555, curr_reward: -16.0, best_reward: -11.0, running_avg_reward: -15.8, curr_epsilon: 0.0718\n",
      "episode_num 556, curr_reward: -15.0, best_reward: -11.0, running_avg_reward: -15.78, curr_epsilon: 0.0712\n",
      "episode_num 557, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -15.79, curr_epsilon: 0.0708\n",
      "episode_num 558, curr_reward: -18.0, best_reward: -11.0, running_avg_reward: -15.86, curr_epsilon: 0.0703\n",
      "episode_num 559, curr_reward: -11.0, best_reward: -11.0, running_avg_reward: -15.82, curr_epsilon: 0.0696\n",
      "episode_num 560, curr_reward: -12.0, best_reward: -11.0, running_avg_reward: -15.76, curr_epsilon: 0.069\n",
      "checkpointing current model weights. highest running_average_reward of -15.7 achieved!\n",
      "episode_num 561, curr_reward: -12.0, best_reward: -11.0, running_avg_reward: -15.7, curr_epsilon: 0.0685\n",
      "checkpointing current model weights. highest running_average_reward of -15.69 achieved!\n",
      "episode_num 562, curr_reward: -17.0, best_reward: -11.0, running_avg_reward: -15.69, curr_epsilon: 0.068\n",
      "episode_num 563, curr_reward: -19.0, best_reward: -11.0, running_avg_reward: -15.71, curr_epsilon: 0.0675\n",
      "checkpointing current model weights. highest running_average_reward of -15.67 achieved!\n",
      "episode_num 564, curr_reward: -11.0, best_reward: -11.0, running_avg_reward: -15.67, curr_epsilon: 0.0669\n",
      "checkpointing current model weights. highest running_average_reward of -15.66 achieved!\n",
      "episode_num 565, curr_reward: -14.0, best_reward: -11.0, running_avg_reward: -15.66, curr_epsilon: 0.0664\n",
      "checkpointing current model weights. highest running_average_reward of -15.63 achieved!\n",
      "episode_num 566, curr_reward: -13.0, best_reward: -11.0, running_avg_reward: -15.63, curr_epsilon: 0.0658\n",
      "episode_num 567, curr_reward: -12.0, best_reward: -11.0, running_avg_reward: -15.63, curr_epsilon: 0.0652\n",
      "checkpointing current model weights. highest running_average_reward of -15.51 achieved!\n",
      "episode_num 568, curr_reward: -6.0, best_reward: -6.0, running_avg_reward: -15.51, curr_epsilon: 0.0645\n",
      "checkpointing current model weights. highest running_average_reward of -15.47 achieved!\n",
      "episode_num 569, curr_reward: -13.0, best_reward: -6.0, running_avg_reward: -15.47, curr_epsilon: 0.064\n",
      "checkpointing current model weights. highest running_average_reward of -15.38 achieved!\n",
      "episode_num 570, curr_reward: -9.0, best_reward: -6.0, running_avg_reward: -15.38, curr_epsilon: 0.0633\n",
      "checkpointing current model weights. highest running_average_reward of -15.37 achieved!\n",
      "episode_num 571, curr_reward: -15.0, best_reward: -6.0, running_avg_reward: -15.37, curr_epsilon: 0.0628\n",
      "checkpointing current model weights. highest running_average_reward of -15.36 achieved!\n",
      "episode_num 572, curr_reward: -14.0, best_reward: -6.0, running_avg_reward: -15.36, curr_epsilon: 0.0623\n",
      "checkpointing current model weights. highest running_average_reward of -15.31 achieved!\n",
      "episode_num 573, curr_reward: -14.0, best_reward: -6.0, running_avg_reward: -15.31, curr_epsilon: 0.0618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointing current model weights. highest running_average_reward of -15.22 achieved!\n",
      "episode_num 574, curr_reward: -8.0, best_reward: -6.0, running_avg_reward: -15.22, curr_epsilon: 0.0612\n",
      "checkpointing current model weights. highest running_average_reward of -15.18 achieved!\n",
      "episode_num 575, curr_reward: -8.0, best_reward: -6.0, running_avg_reward: -15.18, curr_epsilon: 0.0606\n",
      "checkpointing current model weights. highest running_average_reward of -15.14 achieved!\n",
      "episode_num 576, curr_reward: -12.0, best_reward: -6.0, running_avg_reward: -15.14, curr_epsilon: 0.0601\n",
      "episode_num 577, curr_reward: -17.0, best_reward: -6.0, running_avg_reward: -15.16, curr_epsilon: 0.0597\n",
      "checkpointing current model weights. highest running_average_reward of -15.09 achieved!\n",
      "episode_num 578, curr_reward: -9.0, best_reward: -6.0, running_avg_reward: -15.09, curr_epsilon: 0.0592\n",
      "checkpointing current model weights. highest running_average_reward of -15.04 achieved!\n",
      "episode_num 579, curr_reward: -12.0, best_reward: -6.0, running_avg_reward: -15.04, curr_epsilon: 0.0587\n",
      "checkpointing current model weights. highest running_average_reward of -15.03 achieved!\n",
      "episode_num 580, curr_reward: -14.0, best_reward: -6.0, running_avg_reward: -15.03, curr_epsilon: 0.0582\n",
      "checkpointing current model weights. highest running_average_reward of -15.01 achieved!\n",
      "episode_num 581, curr_reward: -15.0, best_reward: -6.0, running_avg_reward: -15.01, curr_epsilon: 0.0578\n",
      "checkpointing current model weights. highest running_average_reward of -14.93 achieved!\n",
      "episode_num 582, curr_reward: -9.0, best_reward: -6.0, running_avg_reward: -14.93, curr_epsilon: 0.0573\n",
      "episode_num 583, curr_reward: -15.0, best_reward: -6.0, running_avg_reward: -14.94, curr_epsilon: 0.0568\n",
      "episode_num 584, curr_reward: -12.0, best_reward: -6.0, running_avg_reward: -14.95, curr_epsilon: 0.0564\n",
      "checkpointing current model weights. highest running_average_reward of -14.86 achieved!\n",
      "episode_num 585, curr_reward: -8.0, best_reward: -6.0, running_avg_reward: -14.86, curr_epsilon: 0.0559\n",
      "checkpointing current model weights. highest running_average_reward of -14.79 achieved!\n",
      "episode_num 586, curr_reward: -12.0, best_reward: -6.0, running_avg_reward: -14.79, curr_epsilon: 0.0554\n",
      "checkpointing current model weights. highest running_average_reward of -14.75 achieved!\n",
      "episode_num 587, curr_reward: -16.0, best_reward: -6.0, running_avg_reward: -14.75, curr_epsilon: 0.055\n",
      "checkpointing current model weights. highest running_average_reward of -14.73 achieved!\n",
      "episode_num 588, curr_reward: -12.0, best_reward: -6.0, running_avg_reward: -14.73, curr_epsilon: 0.0545\n",
      "checkpointing current model weights. highest running_average_reward of -14.72 achieved!\n",
      "episode_num 589, curr_reward: -13.0, best_reward: -6.0, running_avg_reward: -14.72, curr_epsilon: 0.0541\n",
      "episode_num 590, curr_reward: -20.0, best_reward: -6.0, running_avg_reward: -14.75, curr_epsilon: 0.0538\n",
      "episode_num 591, curr_reward: -18.0, best_reward: -6.0, running_avg_reward: -14.77, curr_epsilon: 0.0535\n",
      "episode_num 592, curr_reward: -12.0, best_reward: -6.0, running_avg_reward: -14.78, curr_epsilon: 0.0531\n",
      "checkpointing current model weights. highest running_average_reward of -14.68 achieved!\n",
      "episode_num 593, curr_reward: -11.0, best_reward: -6.0, running_avg_reward: -14.68, curr_epsilon: 0.0526\n",
      "checkpointing current model weights. highest running_average_reward of -14.66 achieved!\n",
      "episode_num 594, curr_reward: -16.0, best_reward: -6.0, running_avg_reward: -14.66, curr_epsilon: 0.0523\n",
      "checkpointing current model weights. highest running_average_reward of -14.65 achieved!\n",
      "episode_num 595, curr_reward: -17.0, best_reward: -6.0, running_avg_reward: -14.65, curr_epsilon: 0.0519\n",
      "checkpointing current model weights. highest running_average_reward of -14.63 achieved!\n",
      "episode_num 596, curr_reward: -11.0, best_reward: -6.0, running_avg_reward: -14.63, curr_epsilon: 0.0515\n",
      "checkpointing current model weights. highest running_average_reward of -14.59 achieved!\n",
      "episode_num 597, curr_reward: -10.0, best_reward: -6.0, running_avg_reward: -14.59, curr_epsilon: 0.051\n",
      "checkpointing current model weights. highest running_average_reward of -14.57 achieved!\n",
      "episode_num 598, curr_reward: -15.0, best_reward: -6.0, running_avg_reward: -14.57, curr_epsilon: 0.0507\n",
      "checkpointing current model weights. highest running_average_reward of -14.55 achieved!\n",
      "episode_num 599, curr_reward: -17.0, best_reward: -6.0, running_avg_reward: -14.55, curr_epsilon: 0.0504\n",
      "checkpointing current model weights. highest running_average_reward of -14.52 achieved!\n",
      "episode_num 600, curr_reward: -10.0, best_reward: -6.0, running_avg_reward: -14.52, curr_epsilon: 0.0499\n",
      "checkpointing current model weights. highest running_average_reward of -14.47 achieved!\n",
      "episode_num 601, curr_reward: -15.0, best_reward: -6.0, running_avg_reward: -14.47, curr_epsilon: 0.0496\n",
      "checkpointing current model weights. highest running_average_reward of -14.4 achieved!\n",
      "episode_num 602, curr_reward: -12.0, best_reward: -6.0, running_avg_reward: -14.4, curr_epsilon: 0.0492\n",
      "episode_num 603, curr_reward: -18.0, best_reward: -6.0, running_avg_reward: -14.46, curr_epsilon: 0.0489\n",
      "episode_num 604, curr_reward: -9.0, best_reward: -6.0, running_avg_reward: -14.42, curr_epsilon: 0.0484\n",
      "checkpointing current model weights. highest running_average_reward of -14.37 achieved!\n",
      "episode_num 605, curr_reward: -13.0, best_reward: -6.0, running_avg_reward: -14.37, curr_epsilon: 0.0481\n",
      "checkpointing current model weights. highest running_average_reward of -14.32 achieved!\n",
      "episode_num 606, curr_reward: -11.0, best_reward: -6.0, running_avg_reward: -14.32, curr_epsilon: 0.0477\n",
      "episode_num 607, curr_reward: -18.0, best_reward: -6.0, running_avg_reward: -14.36, curr_epsilon: 0.0474\n",
      "checkpointing current model weights. highest running_average_reward of -14.31 achieved!\n",
      "episode_num 608, curr_reward: -13.0, best_reward: -6.0, running_avg_reward: -14.31, curr_epsilon: 0.0471\n",
      "checkpointing current model weights. highest running_average_reward of -14.26 achieved!\n",
      "episode_num 609, curr_reward: -10.0, best_reward: -6.0, running_avg_reward: -14.26, curr_epsilon: 0.0466\n",
      "checkpointing current model weights. highest running_average_reward of -14.18 achieved!\n",
      "episode_num 610, curr_reward: -12.0, best_reward: -6.0, running_avg_reward: -14.18, curr_epsilon: 0.0463\n",
      "episode_num 611, curr_reward: -15.0, best_reward: -6.0, running_avg_reward: -14.22, curr_epsilon: 0.046\n",
      "episode_num 612, curr_reward: -14.0, best_reward: -6.0, running_avg_reward: -14.23, curr_epsilon: 0.0456\n",
      "episode_num 613, curr_reward: -14.0, best_reward: -6.0, running_avg_reward: -14.22, curr_epsilon: 0.0453\n",
      "episode_num 614, curr_reward: -11.0, best_reward: -6.0, running_avg_reward: -14.18, curr_epsilon: 0.045\n",
      "episode_num 615, curr_reward: -17.0, best_reward: -6.0, running_avg_reward: -14.19, curr_epsilon: 0.0447\n",
      "episode_num 616, curr_reward: -19.0, best_reward: -6.0, running_avg_reward: -14.24, curr_epsilon: 0.0446\n",
      "checkpointing current model weights. highest running_average_reward of -14.14 achieved!\n",
      "episode_num 617, curr_reward: -7.0, best_reward: -6.0, running_avg_reward: -14.14, curr_epsilon: 0.0441\n",
      "episode_num 618, curr_reward: -18.0, best_reward: -6.0, running_avg_reward: -14.18, curr_epsilon: 0.0438\n",
      "episode_num 619, curr_reward: -17.0, best_reward: -6.0, running_avg_reward: -14.18, curr_epsilon: 0.0436\n",
      "checkpointing current model weights. highest running_average_reward of -14.06 achieved!\n",
      "episode_num 620, curr_reward: -9.0, best_reward: -6.0, running_avg_reward: -14.06, curr_epsilon: 0.0432\n",
      "episode_num 621, curr_reward: -16.0, best_reward: -6.0, running_avg_reward: -14.06, curr_epsilon: 0.0429\n",
      "checkpointing current model weights. highest running_average_reward of -14.05 achieved!\n",
      "episode_num 622, curr_reward: -16.0, best_reward: -6.0, running_avg_reward: -14.05, curr_epsilon: 0.0426\n",
      "checkpointing current model weights. highest running_average_reward of -13.99 achieved!\n",
      "episode_num 623, curr_reward: -7.0, best_reward: -6.0, running_avg_reward: -13.99, curr_epsilon: 0.0421\n",
      "episode_num 624, curr_reward: -20.0, best_reward: -6.0, running_avg_reward: -14.06, curr_epsilon: 0.0419\n",
      "checkpointing current model weights. highest running_average_reward of -13.97 achieved!\n",
      "episode_num 625, curr_reward: -9.0, best_reward: -6.0, running_avg_reward: -13.97, curr_epsilon: 0.0415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointing current model weights. highest running_average_reward of -13.86 achieved!\n",
      "episode_num 626, curr_reward: -6.0, best_reward: -6.0, running_avg_reward: -13.86, curr_epsilon: 0.0411\n",
      "episode_num 627, curr_reward: -20.0, best_reward: -6.0, running_avg_reward: -13.87, curr_epsilon: 0.0408\n",
      "episode_num 628, curr_reward: -10.0, best_reward: -6.0, running_avg_reward: -13.86, curr_epsilon: 0.0405\n",
      "checkpointing current model weights. highest running_average_reward of -13.84 achieved!\n",
      "episode_num 629, curr_reward: -17.0, best_reward: -6.0, running_avg_reward: -13.84, curr_epsilon: 0.0402\n",
      "checkpointing current model weights. highest running_average_reward of -13.82 achieved!\n",
      "episode_num 630, curr_reward: -14.0, best_reward: -6.0, running_avg_reward: -13.82, curr_epsilon: 0.0399\n",
      "checkpointing current model weights. highest running_average_reward of -13.75 achieved!\n",
      "episode_num 631, curr_reward: -8.0, best_reward: -6.0, running_avg_reward: -13.75, curr_epsilon: 0.0395\n",
      "episode_num 632, curr_reward: -19.0, best_reward: -6.0, running_avg_reward: -13.79, curr_epsilon: 0.0392\n",
      "episode_num 633, curr_reward: -14.0, best_reward: -6.0, running_avg_reward: -13.82, curr_epsilon: 0.0389\n",
      "episode_num 634, curr_reward: -17.0, best_reward: -6.0, running_avg_reward: -13.88, curr_epsilon: 0.0387\n",
      "episode_num 635, curr_reward: -11.0, best_reward: -6.0, running_avg_reward: -13.82, curr_epsilon: 0.0384\n",
      "episode_num 636, curr_reward: -12.0, best_reward: -6.0, running_avg_reward: -13.8, curr_epsilon: 0.0381\n",
      "episode_num 637, curr_reward: -11.0, best_reward: -6.0, running_avg_reward: -13.76, curr_epsilon: 0.0378\n",
      "checkpointing current model weights. highest running_average_reward of -13.66 achieved!\n",
      "episode_num 638, curr_reward: -10.0, best_reward: -6.0, running_avg_reward: -13.66, curr_epsilon: 0.0376\n",
      "checkpointing current model weights. highest running_average_reward of -13.61 achieved!\n",
      "episode_num 639, curr_reward: -10.0, best_reward: -6.0, running_avg_reward: -13.61, curr_epsilon: 0.0372\n",
      "checkpointing current model weights. highest running_average_reward of -13.53 achieved!\n",
      "episode_num 640, curr_reward: -6.0, best_reward: -6.0, running_avg_reward: -13.53, curr_epsilon: 0.0369\n",
      "checkpointing current model weights. highest running_average_reward of -13.52 achieved!\n",
      "episode_num 641, curr_reward: -12.0, best_reward: -6.0, running_avg_reward: -13.52, curr_epsilon: 0.0366\n",
      "episode_num 642, curr_reward: -11.0, best_reward: -6.0, running_avg_reward: -13.52, curr_epsilon: 0.0363\n",
      "checkpointing current model weights. highest running_average_reward of -13.46 achieved!\n",
      "episode_num 643, curr_reward: -9.0, best_reward: -6.0, running_avg_reward: -13.46, curr_epsilon: 0.036\n",
      "checkpointing current model weights. highest running_average_reward of -13.44 achieved!\n",
      "episode_num 644, curr_reward: -15.0, best_reward: -6.0, running_avg_reward: -13.44, curr_epsilon: 0.0358\n",
      "checkpointing current model weights. highest running_average_reward of -13.42 achieved!\n",
      "episode_num 645, curr_reward: -14.0, best_reward: -6.0, running_avg_reward: -13.42, curr_epsilon: 0.0355\n",
      "checkpointing current model weights. highest running_average_reward of -13.34 achieved!\n",
      "episode_num 646, curr_reward: -9.0, best_reward: -6.0, running_avg_reward: -13.34, curr_epsilon: 0.0352\n",
      "checkpointing current model weights. highest running_average_reward of -13.32 achieved!\n",
      "episode_num 647, curr_reward: -15.0, best_reward: -6.0, running_avg_reward: -13.32, curr_epsilon: 0.035\n",
      "episode_num 648, curr_reward: -20.0, best_reward: -6.0, running_avg_reward: -13.33, curr_epsilon: 0.0348\n",
      "checkpointing current model weights. highest running_average_reward of -13.28 achieved!\n",
      "episode_num 649, curr_reward: -13.0, best_reward: -6.0, running_avg_reward: -13.28, curr_epsilon: 0.0346\n",
      "checkpointing current model weights. highest running_average_reward of -13.17 achieved!\n",
      "episode_num 650, curr_reward: -5.0, best_reward: -5.0, running_avg_reward: -13.17, curr_epsilon: 0.0342\n",
      "checkpointing current model weights. highest running_average_reward of -12.98 achieved!\n",
      "episode_num 651, curr_reward: 1.0, best_reward: 1.0, running_avg_reward: -12.98, curr_epsilon: 0.0339\n",
      "episode_num 652, curr_reward: -17.0, best_reward: 1.0, running_avg_reward: -12.99, curr_epsilon: 0.0337\n",
      "episode_num 653, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -13.0, curr_epsilon: 0.0334\n",
      "episode_num 654, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -13.0, curr_epsilon: 0.0332\n",
      "episode_num 655, curr_reward: -17.0, best_reward: 1.0, running_avg_reward: -13.01, curr_epsilon: 0.033\n",
      "episode_num 656, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -13.0, curr_epsilon: 0.0328\n",
      "checkpointing current model weights. highest running_average_reward of -12.97 achieved!\n",
      "episode_num 657, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -12.97, curr_epsilon: 0.0325\n",
      "checkpointing current model weights. highest running_average_reward of -12.86 achieved!\n",
      "episode_num 658, curr_reward: -7.0, best_reward: 1.0, running_avg_reward: -12.86, curr_epsilon: 0.0322\n",
      "episode_num 659, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -12.9, curr_epsilon: 0.032\n",
      "episode_num 660, curr_reward: -8.0, best_reward: 1.0, running_avg_reward: -12.86, curr_epsilon: 0.0317\n",
      "checkpointing current model weights. highest running_average_reward of -12.79 achieved!\n",
      "episode_num 661, curr_reward: -5.0, best_reward: 1.0, running_avg_reward: -12.79, curr_epsilon: 0.0314\n",
      "checkpointing current model weights. highest running_average_reward of -12.75 achieved!\n",
      "episode_num 662, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -12.75, curr_epsilon: 0.0311\n",
      "checkpointing current model weights. highest running_average_reward of -12.66 achieved!\n",
      "episode_num 663, curr_reward: -10.0, best_reward: 1.0, running_avg_reward: -12.66, curr_epsilon: 0.0308\n",
      "episode_num 664, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -12.69, curr_epsilon: 0.0306\n",
      "episode_num 665, curr_reward: -17.0, best_reward: 1.0, running_avg_reward: -12.72, curr_epsilon: 0.0305\n",
      "episode_num 666, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -12.72, curr_epsilon: 0.0302\n",
      "episode_num 667, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -12.75, curr_epsilon: 0.03\n",
      "episode_num 668, curr_reward: -8.0, best_reward: 1.0, running_avg_reward: -12.77, curr_epsilon: 0.0298\n",
      "episode_num 669, curr_reward: -6.0, best_reward: 1.0, running_avg_reward: -12.7, curr_epsilon: 0.0295\n",
      "checkpointing current model weights. highest running_average_reward of -12.63 achieved!\n",
      "episode_num 670, curr_reward: -2.0, best_reward: 1.0, running_avg_reward: -12.63, curr_epsilon: 0.0292\n",
      "episode_num 671, curr_reward: -20.0, best_reward: 1.0, running_avg_reward: -12.68, curr_epsilon: 0.0291\n",
      "episode_num 672, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -12.67, curr_epsilon: 0.0289\n",
      "checkpointing current model weights. highest running_average_reward of -12.58 achieved!\n",
      "episode_num 673, curr_reward: -5.0, best_reward: 1.0, running_avg_reward: -12.58, curr_epsilon: 0.0286\n",
      "episode_num 674, curr_reward: -11.0, best_reward: 1.0, running_avg_reward: -12.61, curr_epsilon: 0.0284\n",
      "checkpointing current model weights. highest running_average_reward of -12.57 achieved!\n",
      "episode_num 675, curr_reward: -4.0, best_reward: 1.0, running_avg_reward: -12.57, curr_epsilon: 0.0281\n",
      "episode_num 676, curr_reward: -17.0, best_reward: 1.0, running_avg_reward: -12.62, curr_epsilon: 0.0279\n",
      "episode_num 677, curr_reward: -18.0, best_reward: 1.0, running_avg_reward: -12.63, curr_epsilon: 0.0278\n",
      "episode_num 678, curr_reward: -19.0, best_reward: 1.0, running_avg_reward: -12.73, curr_epsilon: 0.0276\n",
      "episode_num 679, curr_reward: -9.0, best_reward: 1.0, running_avg_reward: -12.7, curr_epsilon: 0.0273\n",
      "episode_num 680, curr_reward: -2.0, best_reward: 1.0, running_avg_reward: -12.58, curr_epsilon: 0.0271\n",
      "checkpointing current model weights. highest running_average_reward of -12.53 achieved!\n",
      "episode_num 681, curr_reward: -10.0, best_reward: 1.0, running_avg_reward: -12.53, curr_epsilon: 0.0269\n",
      "checkpointing current model weights. highest running_average_reward of -12.52 achieved!\n",
      "episode_num 682, curr_reward: -8.0, best_reward: 1.0, running_avg_reward: -12.52, curr_epsilon: 0.0266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointing current model weights. highest running_average_reward of -12.51 achieved!\n",
      "episode_num 683, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -12.51, curr_epsilon: 0.0265\n",
      "episode_num 684, curr_reward: -16.0, best_reward: 1.0, running_avg_reward: -12.55, curr_epsilon: 0.0263\n",
      "episode_num 685, curr_reward: -12.0, best_reward: 1.0, running_avg_reward: -12.59, curr_epsilon: 0.0261\n",
      "episode_num 686, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -12.61, curr_epsilon: 0.0259\n",
      "episode_num 687, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -12.58, curr_epsilon: 0.0257\n",
      "episode_num 688, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -12.59, curr_epsilon: 0.0255\n",
      "episode_num 689, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -12.6, curr_epsilon: 0.0253\n",
      "episode_num 690, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -12.54, curr_epsilon: 0.0252\n",
      "checkpointing current model weights. highest running_average_reward of -12.43 achieved!\n",
      "episode_num 691, curr_reward: -7.0, best_reward: 1.0, running_avg_reward: -12.43, curr_epsilon: 0.025\n",
      "checkpointing current model weights. highest running_average_reward of -12.39 achieved!\n",
      "episode_num 692, curr_reward: -8.0, best_reward: 1.0, running_avg_reward: -12.39, curr_epsilon: 0.0247\n",
      "checkpointing current model weights. highest running_average_reward of -12.38 achieved!\n",
      "episode_num 693, curr_reward: -10.0, best_reward: 1.0, running_avg_reward: -12.38, curr_epsilon: 0.0245\n",
      "checkpointing current model weights. highest running_average_reward of -12.27 achieved!\n",
      "episode_num 694, curr_reward: -5.0, best_reward: 1.0, running_avg_reward: -12.27, curr_epsilon: 0.0243\n",
      "checkpointing current model weights. highest running_average_reward of -12.22 achieved!\n",
      "episode_num 695, curr_reward: -12.0, best_reward: 1.0, running_avg_reward: -12.22, curr_epsilon: 0.0242\n",
      "checkpointing current model weights. highest running_average_reward of -12.18 achieved!\n",
      "episode_num 696, curr_reward: -7.0, best_reward: 1.0, running_avg_reward: -12.18, curr_epsilon: 0.024\n",
      "episode_num 697, curr_reward: -11.0, best_reward: 1.0, running_avg_reward: -12.19, curr_epsilon: 0.0238\n",
      "checkpointing current model weights. highest running_average_reward of -12.16 achieved!\n",
      "episode_num 698, curr_reward: -12.0, best_reward: 1.0, running_avg_reward: -12.16, curr_epsilon: 0.0236\n",
      "checkpointing current model weights. highest running_average_reward of -12.14 achieved!\n",
      "episode_num 699, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -12.14, curr_epsilon: 0.0235\n",
      "episode_num 700, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -12.17, curr_epsilon: 0.0233\n",
      "checkpointing current model weights. highest running_average_reward of -12.07 achieved!\n",
      "episode_num 701, curr_reward: -5.0, best_reward: 1.0, running_avg_reward: -12.07, curr_epsilon: 0.0231\n",
      "episode_num 702, curr_reward: -12.0, best_reward: 1.0, running_avg_reward: -12.07, curr_epsilon: 0.0229\n",
      "checkpointing current model weights. highest running_average_reward of -11.96 achieved!\n",
      "episode_num 703, curr_reward: -7.0, best_reward: 1.0, running_avg_reward: -11.96, curr_epsilon: 0.0227\n",
      "checkpointing current model weights. highest running_average_reward of -11.95 achieved!\n",
      "episode_num 704, curr_reward: -8.0, best_reward: 1.0, running_avg_reward: -11.95, curr_epsilon: 0.0225\n",
      "checkpointing current model weights. highest running_average_reward of -11.93 achieved!\n",
      "episode_num 705, curr_reward: -11.0, best_reward: 1.0, running_avg_reward: -11.93, curr_epsilon: 0.0224\n",
      "checkpointing current model weights. highest running_average_reward of -11.91 achieved!\n",
      "episode_num 706, curr_reward: -9.0, best_reward: 1.0, running_avg_reward: -11.91, curr_epsilon: 0.0222\n",
      "checkpointing current model weights. highest running_average_reward of -11.81 achieved!\n",
      "episode_num 707, curr_reward: -8.0, best_reward: 1.0, running_avg_reward: -11.81, curr_epsilon: 0.022\n",
      "episode_num 708, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -11.82, curr_epsilon: 0.0219\n",
      "checkpointing current model weights. highest running_average_reward of -11.76 achieved!\n",
      "episode_num 709, curr_reward: -4.0, best_reward: 1.0, running_avg_reward: -11.76, curr_epsilon: 0.0217\n",
      "episode_num 710, curr_reward: -12.0, best_reward: 1.0, running_avg_reward: -11.76, curr_epsilon: 0.0215\n",
      "checkpointing current model weights. highest running_average_reward of -11.68 achieved!\n",
      "episode_num 711, curr_reward: -7.0, best_reward: 1.0, running_avg_reward: -11.68, curr_epsilon: 0.0214\n",
      "checkpointing current model weights. highest running_average_reward of -11.62 achieved!\n",
      "episode_num 712, curr_reward: -8.0, best_reward: 1.0, running_avg_reward: -11.62, curr_epsilon: 0.0212\n",
      "episode_num 713, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -11.62, curr_epsilon: 0.0211\n",
      "episode_num 714, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -11.64, curr_epsilon: 0.0209\n",
      "episode_num 715, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -11.62, curr_epsilon: 0.0208\n",
      "checkpointing current model weights. highest running_average_reward of -11.55 achieved!\n",
      "episode_num 716, curr_reward: -12.0, best_reward: 1.0, running_avg_reward: -11.55, curr_epsilon: 0.0206\n",
      "episode_num 717, curr_reward: -11.0, best_reward: 1.0, running_avg_reward: -11.59, curr_epsilon: 0.0205\n",
      "checkpointing current model weights. highest running_average_reward of -11.51 achieved!\n",
      "episode_num 718, curr_reward: -10.0, best_reward: 1.0, running_avg_reward: -11.51, curr_epsilon: 0.0203\n",
      "checkpointing current model weights. highest running_average_reward of -11.45 achieved!\n",
      "episode_num 719, curr_reward: -11.0, best_reward: 1.0, running_avg_reward: -11.45, curr_epsilon: 0.0202\n",
      "episode_num 720, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -11.5, curr_epsilon: 0.02\n",
      "episode_num 721, curr_reward: -11.0, best_reward: 1.0, running_avg_reward: -11.45, curr_epsilon: 0.0199\n",
      "checkpointing current model weights. highest running_average_reward of -11.42 achieved!\n",
      "episode_num 722, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -11.42, curr_epsilon: 0.0198\n",
      "episode_num 723, curr_reward: -9.0, best_reward: 1.0, running_avg_reward: -11.44, curr_epsilon: 0.0196\n",
      "checkpointing current model weights. highest running_average_reward of -11.32 achieved!\n",
      "episode_num 724, curr_reward: -8.0, best_reward: 1.0, running_avg_reward: -11.32, curr_epsilon: 0.0194\n",
      "checkpointing current model weights. highest running_average_reward of -11.31 achieved!\n",
      "episode_num 725, curr_reward: -8.0, best_reward: 1.0, running_avg_reward: -11.31, curr_epsilon: 0.0193\n",
      "episode_num 726, curr_reward: -11.0, best_reward: 1.0, running_avg_reward: -11.36, curr_epsilon: 0.0192\n",
      "checkpointing current model weights. highest running_average_reward of -11.19 achieved!\n",
      "episode_num 727, curr_reward: -3.0, best_reward: 1.0, running_avg_reward: -11.19, curr_epsilon: 0.019\n",
      "checkpointing current model weights. highest running_average_reward of -11.15 achieved!\n",
      "episode_num 728, curr_reward: -6.0, best_reward: 1.0, running_avg_reward: -11.15, curr_epsilon: 0.0188\n",
      "checkpointing current model weights. highest running_average_reward of -11.03 achieved!\n",
      "episode_num 729, curr_reward: -5.0, best_reward: 1.0, running_avg_reward: -11.03, curr_epsilon: 0.0186\n",
      "episode_num 730, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -11.04, curr_epsilon: 0.0185\n",
      "episode_num 731, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -11.11, curr_epsilon: 0.0184\n",
      "episode_num 732, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -11.07, curr_epsilon: 0.0183\n",
      "episode_num 733, curr_reward: -18.0, best_reward: 1.0, running_avg_reward: -11.11, curr_epsilon: 0.0182\n",
      "checkpointing current model weights. highest running_average_reward of -11.0 achieved!\n",
      "episode_num 734, curr_reward: -6.0, best_reward: 1.0, running_avg_reward: -11.0, curr_epsilon: 0.0181\n",
      "episode_num 735, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -11.02, curr_epsilon: 0.0179\n",
      "episode_num 736, curr_reward: -18.0, best_reward: 1.0, running_avg_reward: -11.08, curr_epsilon: 0.0178\n",
      "episode_num 737, curr_reward: -10.0, best_reward: 1.0, running_avg_reward: -11.07, curr_epsilon: 0.0177\n",
      "episode_num 738, curr_reward: -10.0, best_reward: 1.0, running_avg_reward: -11.07, curr_epsilon: 0.0176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 739, curr_reward: -16.0, best_reward: 1.0, running_avg_reward: -11.13, curr_epsilon: 0.0175\n",
      "episode_num 740, curr_reward: -9.0, best_reward: 1.0, running_avg_reward: -11.16, curr_epsilon: 0.0174\n",
      "episode_num 741, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -11.19, curr_epsilon: 0.0173\n",
      "episode_num 742, curr_reward: -6.0, best_reward: 1.0, running_avg_reward: -11.14, curr_epsilon: 0.0171\n",
      "episode_num 743, curr_reward: -16.0, best_reward: 1.0, running_avg_reward: -11.21, curr_epsilon: 0.017\n",
      "episode_num 744, curr_reward: -4.0, best_reward: 1.0, running_avg_reward: -11.1, curr_epsilon: 0.0169\n",
      "episode_num 745, curr_reward: -11.0, best_reward: 1.0, running_avg_reward: -11.07, curr_epsilon: 0.0167\n",
      "episode_num 746, curr_reward: -10.0, best_reward: 1.0, running_avg_reward: -11.08, curr_epsilon: 0.0167\n",
      "episode_num 747, curr_reward: -7.0, best_reward: 1.0, running_avg_reward: -11.0, curr_epsilon: 0.0165\n",
      "checkpointing current model weights. highest running_average_reward of -10.86 achieved!\n",
      "episode_num 748, curr_reward: -6.0, best_reward: 1.0, running_avg_reward: -10.86, curr_epsilon: 0.0164\n",
      "checkpointing current model weights. highest running_average_reward of -10.83 achieved!\n",
      "episode_num 749, curr_reward: -10.0, best_reward: 1.0, running_avg_reward: -10.83, curr_epsilon: 0.0163\n",
      "episode_num 750, curr_reward: -18.0, best_reward: 1.0, running_avg_reward: -10.96, curr_epsilon: 0.0162\n",
      "episode_num 751, curr_reward: -10.0, best_reward: 1.0, running_avg_reward: -11.07, curr_epsilon: 0.0161\n",
      "episode_num 752, curr_reward: -9.0, best_reward: 1.0, running_avg_reward: -10.99, curr_epsilon: 0.016\n",
      "episode_num 753, curr_reward: -16.0, best_reward: 1.0, running_avg_reward: -11.01, curr_epsilon: 0.0159\n",
      "episode_num 754, curr_reward: -5.0, best_reward: 1.0, running_avg_reward: -10.92, curr_epsilon: 0.0158\n",
      "episode_num 755, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -10.88, curr_epsilon: 0.0157\n",
      "checkpointing current model weights. highest running_average_reward of -10.81 achieved!\n",
      "episode_num 756, curr_reward: -7.0, best_reward: 1.0, running_avg_reward: -10.81, curr_epsilon: 0.0156\n",
      "checkpointing current model weights. highest running_average_reward of -10.79 achieved!\n",
      "episode_num 757, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -10.79, curr_epsilon: 0.0155\n",
      "episode_num 758, curr_reward: -11.0, best_reward: 1.0, running_avg_reward: -10.83, curr_epsilon: 0.0154\n",
      "episode_num 759, curr_reward: -16.0, best_reward: 1.0, running_avg_reward: -10.84, curr_epsilon: 0.0153\n",
      "episode_num 760, curr_reward: -12.0, best_reward: 1.0, running_avg_reward: -10.88, curr_epsilon: 0.0152\n",
      "episode_num 761, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -10.98, curr_epsilon: 0.0151\n",
      "episode_num 762, curr_reward: -5.0, best_reward: 1.0, running_avg_reward: -10.9, curr_epsilon: 0.015\n",
      "episode_num 763, curr_reward: -2.0, best_reward: 1.0, running_avg_reward: -10.82, curr_epsilon: 0.0149\n",
      "episode_num 764, curr_reward: -11.0, best_reward: 1.0, running_avg_reward: -10.79, curr_epsilon: 0.0148\n",
      "checkpointing current model weights. highest running_average_reward of -10.71 achieved!\n",
      "episode_num 765, curr_reward: -9.0, best_reward: 1.0, running_avg_reward: -10.71, curr_epsilon: 0.0147\n",
      "checkpointing current model weights. highest running_average_reward of -10.67 achieved!\n",
      "episode_num 766, curr_reward: -9.0, best_reward: 1.0, running_avg_reward: -10.67, curr_epsilon: 0.0146\n",
      "checkpointing current model weights. highest running_average_reward of -10.66 achieved!\n",
      "episode_num 767, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -10.66, curr_epsilon: 0.0145\n",
      "episode_num 768, curr_reward: -10.0, best_reward: 1.0, running_avg_reward: -10.68, curr_epsilon: 0.0144\n",
      "checkpointing current model weights. highest running_average_reward of -10.61 achieved!\n",
      "episode_num 769, curr_reward: 1.0, best_reward: 1.0, running_avg_reward: -10.61, curr_epsilon: 0.0143\n",
      "episode_num 770, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -10.74, curr_epsilon: 0.0142\n",
      "episode_num 771, curr_reward: -11.0, best_reward: 1.0, running_avg_reward: -10.65, curr_epsilon: 0.0141\n",
      "checkpointing current model weights. highest running_average_reward of -10.57 achieved!\n",
      "episode_num 772, curr_reward: -5.0, best_reward: 1.0, running_avg_reward: -10.57, curr_epsilon: 0.014\n",
      "episode_num 773, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -10.67, curr_epsilon: 0.0139\n",
      "episode_num 774, curr_reward: -10.0, best_reward: 1.0, running_avg_reward: -10.66, curr_epsilon: 0.0138\n",
      "episode_num 775, curr_reward: -4.0, best_reward: 1.0, running_avg_reward: -10.66, curr_epsilon: 0.0137\n",
      "episode_num 776, curr_reward: -14.0, best_reward: 1.0, running_avg_reward: -10.63, curr_epsilon: 0.0137\n",
      "checkpointing current model weights. highest running_average_reward of -10.51 achieved!\n",
      "episode_num 777, curr_reward: -6.0, best_reward: 1.0, running_avg_reward: -10.51, curr_epsilon: 0.0136\n",
      "checkpointing current model weights. highest running_average_reward of -10.31 achieved!\n",
      "episode_num 778, curr_reward: 1.0, best_reward: 1.0, running_avg_reward: -10.31, curr_epsilon: 0.0135\n",
      "checkpointing current model weights. highest running_average_reward of -10.3 achieved!\n",
      "episode_num 779, curr_reward: -8.0, best_reward: 1.0, running_avg_reward: -10.3, curr_epsilon: 0.0134\n",
      "episode_num 780, curr_reward: -12.0, best_reward: 1.0, running_avg_reward: -10.4, curr_epsilon: 0.0133\n",
      "episode_num 781, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -10.43, curr_epsilon: 0.0132\n",
      "episode_num 782, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -10.48, curr_epsilon: 0.0131\n",
      "episode_num 783, curr_reward: -9.0, best_reward: 1.0, running_avg_reward: -10.43, curr_epsilon: 0.013\n",
      "episode_num 784, curr_reward: -8.0, best_reward: 1.0, running_avg_reward: -10.35, curr_epsilon: 0.013\n",
      "episode_num 785, curr_reward: -17.0, best_reward: 1.0, running_avg_reward: -10.4, curr_epsilon: 0.0129\n",
      "episode_num 786, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -10.39, curr_epsilon: 0.0128\n",
      "episode_num 787, curr_reward: -10.0, best_reward: 1.0, running_avg_reward: -10.36, curr_epsilon: 0.0128\n",
      "episode_num 788, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -10.36, curr_epsilon: 0.0127\n",
      "episode_num 789, curr_reward: -13.0, best_reward: 1.0, running_avg_reward: -10.35, curr_epsilon: 0.0126\n",
      "episode_num 790, curr_reward: -15.0, best_reward: 1.0, running_avg_reward: -10.36, curr_epsilon: 0.0126\n",
      "episode_num 791, curr_reward: -2.0, best_reward: 1.0, running_avg_reward: -10.31, curr_epsilon: 0.0125\n",
      "checkpointing current model weights. highest running_average_reward of -10.28 achieved!\n",
      "episode_num 792, curr_reward: -5.0, best_reward: 1.0, running_avg_reward: -10.28, curr_epsilon: 0.0124\n",
      "episode_num 793, curr_reward: -19.0, best_reward: 1.0, running_avg_reward: -10.37, curr_epsilon: 0.0124\n",
      "episode_num 794, curr_reward: -4.0, best_reward: 1.0, running_avg_reward: -10.36, curr_epsilon: 0.0123\n",
      "episode_num 795, curr_reward: -4.0, best_reward: 1.0, running_avg_reward: -10.28, curr_epsilon: 0.0122\n",
      "checkpointing current model weights. highest running_average_reward of -10.25 achieved!\n",
      "episode_num 796, curr_reward: -4.0, best_reward: 1.0, running_avg_reward: -10.25, curr_epsilon: 0.0121\n",
      "checkpointing current model weights. highest running_average_reward of -10.08 achieved!\n",
      "episode_num 797, curr_reward: 6.0, best_reward: 6.0, running_avg_reward: -10.08, curr_epsilon: 0.012\n",
      "checkpointing current model weights. highest running_average_reward of -10.07 achieved!\n",
      "episode_num 798, curr_reward: -11.0, best_reward: 6.0, running_avg_reward: -10.07, curr_epsilon: 0.012\n",
      "checkpointing current model weights. highest running_average_reward of -10.0 achieved!\n",
      "episode_num 799, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -10.0, curr_epsilon: 0.0119\n",
      "checkpointing current model weights. highest running_average_reward of -9.85 achieved!\n",
      "episode_num 800, curr_reward: 2.0, best_reward: 6.0, running_avg_reward: -9.85, curr_epsilon: 0.0118\n",
      "episode_num 801, curr_reward: -10.0, best_reward: 6.0, running_avg_reward: -9.9, curr_epsilon: 0.0118\n",
      "episode_num 802, curr_reward: -7.0, best_reward: 6.0, running_avg_reward: -9.85, curr_epsilon: 0.0117\n",
      "episode_num 803, curr_reward: -9.0, best_reward: 6.0, running_avg_reward: -9.87, curr_epsilon: 0.0116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 804, curr_reward: -12.0, best_reward: 6.0, running_avg_reward: -9.91, curr_epsilon: 0.0116\n",
      "episode_num 805, curr_reward: -19.0, best_reward: 6.0, running_avg_reward: -9.99, curr_epsilon: 0.0115\n",
      "episode_num 806, curr_reward: -11.0, best_reward: 6.0, running_avg_reward: -10.01, curr_epsilon: 0.0114\n",
      "episode_num 807, curr_reward: -13.0, best_reward: 6.0, running_avg_reward: -10.06, curr_epsilon: 0.0114\n",
      "episode_num 808, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -10.0, curr_epsilon: 0.0113\n",
      "episode_num 809, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -10.04, curr_epsilon: 0.0113\n",
      "episode_num 810, curr_reward: -14.0, best_reward: 6.0, running_avg_reward: -10.06, curr_epsilon: 0.0112\n",
      "episode_num 811, curr_reward: -6.0, best_reward: 6.0, running_avg_reward: -10.05, curr_epsilon: 0.0112\n",
      "episode_num 812, curr_reward: -6.0, best_reward: 6.0, running_avg_reward: -10.03, curr_epsilon: 0.0111\n",
      "episode_num 813, curr_reward: -12.0, best_reward: 6.0, running_avg_reward: -10.01, curr_epsilon: 0.0111\n",
      "episode_num 814, curr_reward: -9.0, best_reward: 6.0, running_avg_reward: -9.97, curr_epsilon: 0.011\n",
      "episode_num 815, curr_reward: -4.0, best_reward: 6.0, running_avg_reward: -9.86, curr_epsilon: 0.0109\n",
      "checkpointing current model weights. highest running_average_reward of -9.81 achieved!\n",
      "episode_num 816, curr_reward: -7.0, best_reward: 6.0, running_avg_reward: -9.81, curr_epsilon: 0.0108\n",
      "checkpointing current model weights. highest running_average_reward of -9.8 achieved!\n",
      "episode_num 817, curr_reward: -10.0, best_reward: 6.0, running_avg_reward: -9.8, curr_epsilon: 0.0108\n",
      "episode_num 818, curr_reward: -11.0, best_reward: 6.0, running_avg_reward: -9.81, curr_epsilon: 0.0107\n",
      "checkpointing current model weights. highest running_average_reward of -9.78 achieved!\n",
      "episode_num 819, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -9.78, curr_epsilon: 0.0107\n",
      "checkpointing current model weights. highest running_average_reward of -9.73 achieved!\n",
      "episode_num 820, curr_reward: -9.0, best_reward: 6.0, running_avg_reward: -9.73, curr_epsilon: 0.0106\n",
      "episode_num 821, curr_reward: -11.0, best_reward: 6.0, running_avg_reward: -9.73, curr_epsilon: 0.0106\n",
      "checkpointing current model weights. highest running_average_reward of -9.72 achieved!\n",
      "episode_num 822, curr_reward: -12.0, best_reward: 6.0, running_avg_reward: -9.72, curr_epsilon: 0.0105\n",
      "episode_num 823, curr_reward: -11.0, best_reward: 6.0, running_avg_reward: -9.74, curr_epsilon: 0.0105\n",
      "episode_num 824, curr_reward: -17.0, best_reward: 6.0, running_avg_reward: -9.83, curr_epsilon: 0.0104\n",
      "episode_num 825, curr_reward: -4.0, best_reward: 6.0, running_avg_reward: -9.79, curr_epsilon: 0.0104\n",
      "episode_num 826, curr_reward: -10.0, best_reward: 6.0, running_avg_reward: -9.78, curr_epsilon: 0.0103\n",
      "episode_num 827, curr_reward: -11.0, best_reward: 6.0, running_avg_reward: -9.86, curr_epsilon: 0.0103\n",
      "episode_num 828, curr_reward: -12.0, best_reward: 6.0, running_avg_reward: -9.92, curr_epsilon: 0.0102\n",
      "episode_num 829, curr_reward: -14.0, best_reward: 6.0, running_avg_reward: -10.01, curr_epsilon: 0.0102\n",
      "episode_num 830, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -9.94, curr_epsilon: 0.0101\n",
      "episode_num 831, curr_reward: -13.0, best_reward: 6.0, running_avg_reward: -9.92, curr_epsilon: 0.0101\n",
      "episode_num 832, curr_reward: -6.0, best_reward: 6.0, running_avg_reward: -9.83, curr_epsilon: 0.01\n",
      "episode_num 833, curr_reward: -13.0, best_reward: 6.0, running_avg_reward: -9.78, curr_epsilon: 0.01\n",
      "episode_num 834, curr_reward: -4.0, best_reward: 6.0, running_avg_reward: -9.76, curr_epsilon: 0.0099\n",
      "checkpointing current model weights. highest running_average_reward of -9.71 achieved!\n",
      "episode_num 835, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -9.71, curr_epsilon: 0.0099\n",
      "checkpointing current model weights. highest running_average_reward of -9.67 achieved!\n",
      "episode_num 836, curr_reward: -14.0, best_reward: 6.0, running_avg_reward: -9.67, curr_epsilon: 0.0098\n",
      "episode_num 837, curr_reward: -14.0, best_reward: 6.0, running_avg_reward: -9.71, curr_epsilon: 0.0098\n",
      "episode_num 838, curr_reward: -13.0, best_reward: 6.0, running_avg_reward: -9.74, curr_epsilon: 0.0097\n",
      "checkpointing current model weights. highest running_average_reward of -9.59 achieved!\n",
      "episode_num 839, curr_reward: -1.0, best_reward: 6.0, running_avg_reward: -9.59, curr_epsilon: 0.0097\n",
      "episode_num 840, curr_reward: -14.0, best_reward: 6.0, running_avg_reward: -9.64, curr_epsilon: 0.0096\n",
      "episode_num 841, curr_reward: -11.0, best_reward: 6.0, running_avg_reward: -9.6, curr_epsilon: 0.0096\n",
      "episode_num 842, curr_reward: -15.0, best_reward: 6.0, running_avg_reward: -9.69, curr_epsilon: 0.0095\n",
      "episode_num 843, curr_reward: -6.0, best_reward: 6.0, running_avg_reward: -9.59, curr_epsilon: 0.0095\n",
      "episode_num 844, curr_reward: -13.0, best_reward: 6.0, running_avg_reward: -9.68, curr_epsilon: 0.0095\n",
      "episode_num 845, curr_reward: -10.0, best_reward: 6.0, running_avg_reward: -9.67, curr_epsilon: 0.0094\n",
      "episode_num 846, curr_reward: -4.0, best_reward: 6.0, running_avg_reward: -9.61, curr_epsilon: 0.0094\n",
      "episode_num 847, curr_reward: -7.0, best_reward: 6.0, running_avg_reward: -9.61, curr_epsilon: 0.0093\n",
      "episode_num 848, curr_reward: -13.0, best_reward: 6.0, running_avg_reward: -9.68, curr_epsilon: 0.0093\n",
      "episode_num 849, curr_reward: -12.0, best_reward: 6.0, running_avg_reward: -9.7, curr_epsilon: 0.0092\n",
      "episode_num 850, curr_reward: -10.0, best_reward: 6.0, running_avg_reward: -9.62, curr_epsilon: 0.0092\n",
      "episode_num 851, curr_reward: -19.0, best_reward: 6.0, running_avg_reward: -9.71, curr_epsilon: 0.0092\n",
      "episode_num 852, curr_reward: -16.0, best_reward: 6.0, running_avg_reward: -9.78, curr_epsilon: 0.0092\n",
      "episode_num 853, curr_reward: -9.0, best_reward: 6.0, running_avg_reward: -9.71, curr_epsilon: 0.0091\n",
      "episode_num 854, curr_reward: -15.0, best_reward: 6.0, running_avg_reward: -9.81, curr_epsilon: 0.0091\n",
      "episode_num 855, curr_reward: -14.0, best_reward: 6.0, running_avg_reward: -9.82, curr_epsilon: 0.0091\n",
      "episode_num 856, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -9.83, curr_epsilon: 0.009\n",
      "episode_num 857, curr_reward: -7.0, best_reward: 6.0, running_avg_reward: -9.77, curr_epsilon: 0.009\n",
      "episode_num 858, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -9.74, curr_epsilon: 0.0089\n",
      "episode_num 859, curr_reward: -5.0, best_reward: 6.0, running_avg_reward: -9.63, curr_epsilon: 0.0089\n",
      "checkpointing current model weights. highest running_average_reward of -9.49 achieved!\n",
      "episode_num 860, curr_reward: 2.0, best_reward: 6.0, running_avg_reward: -9.49, curr_epsilon: 0.0089\n",
      "checkpointing current model weights. highest running_average_reward of -9.48 achieved!\n",
      "episode_num 861, curr_reward: -14.0, best_reward: 6.0, running_avg_reward: -9.48, curr_epsilon: 0.0088\n",
      "checkpointing current model weights. highest running_average_reward of -9.44 achieved!\n",
      "episode_num 862, curr_reward: -1.0, best_reward: 6.0, running_avg_reward: -9.44, curr_epsilon: 0.0088\n",
      "episode_num 863, curr_reward: -13.0, best_reward: 6.0, running_avg_reward: -9.55, curr_epsilon: 0.0087\n",
      "episode_num 864, curr_reward: -15.0, best_reward: 6.0, running_avg_reward: -9.59, curr_epsilon: 0.0087\n",
      "episode_num 865, curr_reward: -12.0, best_reward: 6.0, running_avg_reward: -9.62, curr_epsilon: 0.0087\n",
      "episode_num 866, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -9.61, curr_epsilon: 0.0086\n",
      "episode_num 867, curr_reward: -2.0, best_reward: 6.0, running_avg_reward: -9.49, curr_epsilon: 0.0086\n",
      "checkpointing current model weights. highest running_average_reward of -9.43 achieved!\n",
      "episode_num 868, curr_reward: -4.0, best_reward: 6.0, running_avg_reward: -9.43, curr_epsilon: 0.0086\n",
      "episode_num 869, curr_reward: -6.0, best_reward: 6.0, running_avg_reward: -9.5, curr_epsilon: 0.0085\n",
      "episode_num 870, curr_reward: -14.0, best_reward: 6.0, running_avg_reward: -9.49, curr_epsilon: 0.0085\n",
      "episode_num 871, curr_reward: -15.0, best_reward: 6.0, running_avg_reward: -9.53, curr_epsilon: 0.0085\n",
      "episode_num 872, curr_reward: -9.0, best_reward: 6.0, running_avg_reward: -9.57, curr_epsilon: 0.0084\n",
      "episode_num 873, curr_reward: -13.0, best_reward: 6.0, running_avg_reward: -9.55, curr_epsilon: 0.0084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 874, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -9.53, curr_epsilon: 0.0084\n",
      "episode_num 875, curr_reward: -6.0, best_reward: 6.0, running_avg_reward: -9.55, curr_epsilon: 0.0083\n",
      "episode_num 876, curr_reward: -6.0, best_reward: 6.0, running_avg_reward: -9.47, curr_epsilon: 0.0083\n",
      "episode_num 877, curr_reward: -10.0, best_reward: 6.0, running_avg_reward: -9.51, curr_epsilon: 0.0083\n",
      "episode_num 878, curr_reward: 3.0, best_reward: 6.0, running_avg_reward: -9.49, curr_epsilon: 0.0082\n",
      "episode_num 879, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -9.49, curr_epsilon: 0.0082\n",
      "checkpointing current model weights. highest running_average_reward of -9.42 achieved!\n",
      "episode_num 880, curr_reward: -5.0, best_reward: 6.0, running_avg_reward: -9.42, curr_epsilon: 0.0082\n",
      "checkpointing current model weights. highest running_average_reward of -9.38 achieved!\n",
      "episode_num 881, curr_reward: -9.0, best_reward: 6.0, running_avg_reward: -9.38, curr_epsilon: 0.0081\n",
      "checkpointing current model weights. highest running_average_reward of -9.33 achieved!\n",
      "episode_num 882, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -9.33, curr_epsilon: 0.0081\n",
      "episode_num 883, curr_reward: -17.0, best_reward: 6.0, running_avg_reward: -9.41, curr_epsilon: 0.0081\n",
      "episode_num 884, curr_reward: -13.0, best_reward: 6.0, running_avg_reward: -9.46, curr_epsilon: 0.008\n",
      "episode_num 885, curr_reward: -18.0, best_reward: 6.0, running_avg_reward: -9.47, curr_epsilon: 0.008\n",
      "episode_num 886, curr_reward: -6.0, best_reward: 6.0, running_avg_reward: -9.4, curr_epsilon: 0.008\n",
      "episode_num 887, curr_reward: -15.0, best_reward: 6.0, running_avg_reward: -9.45, curr_epsilon: 0.008\n",
      "episode_num 888, curr_reward: -17.0, best_reward: 6.0, running_avg_reward: -9.49, curr_epsilon: 0.0079\n",
      "episode_num 889, curr_reward: -3.0, best_reward: 6.0, running_avg_reward: -9.39, curr_epsilon: 0.0079\n",
      "episode_num 890, curr_reward: -13.0, best_reward: 6.0, running_avg_reward: -9.37, curr_epsilon: 0.0079\n",
      "episode_num 891, curr_reward: -4.0, best_reward: 6.0, running_avg_reward: -9.39, curr_epsilon: 0.0079\n",
      "episode_num 892, curr_reward: -10.0, best_reward: 6.0, running_avg_reward: -9.44, curr_epsilon: 0.0078\n",
      "checkpointing current model weights. highest running_average_reward of -9.3 achieved!\n",
      "episode_num 893, curr_reward: -5.0, best_reward: 6.0, running_avg_reward: -9.3, curr_epsilon: 0.0078\n",
      "episode_num 894, curr_reward: -5.0, best_reward: 6.0, running_avg_reward: -9.31, curr_epsilon: 0.0078\n",
      "episode_num 895, curr_reward: -9.0, best_reward: 6.0, running_avg_reward: -9.36, curr_epsilon: 0.0077\n",
      "episode_num 896, curr_reward: -17.0, best_reward: 6.0, running_avg_reward: -9.49, curr_epsilon: 0.0077\n",
      "episode_num 897, curr_reward: -2.0, best_reward: 6.0, running_avg_reward: -9.57, curr_epsilon: 0.0077\n",
      "episode_num 898, curr_reward: -16.0, best_reward: 6.0, running_avg_reward: -9.62, curr_epsilon: 0.0077\n",
      "episode_num 899, curr_reward: -18.0, best_reward: 6.0, running_avg_reward: -9.72, curr_epsilon: 0.0077\n",
      "episode_num 900, curr_reward: -14.0, best_reward: 6.0, running_avg_reward: -9.88, curr_epsilon: 0.0076\n",
      "episode_num 901, curr_reward: -12.0, best_reward: 6.0, running_avg_reward: -9.9, curr_epsilon: 0.0076\n",
      "episode_num 902, curr_reward: -4.0, best_reward: 6.0, running_avg_reward: -9.87, curr_epsilon: 0.0076\n",
      "episode_num 903, curr_reward: -9.0, best_reward: 6.0, running_avg_reward: -9.87, curr_epsilon: 0.0076\n",
      "episode_num 904, curr_reward: -1.0, best_reward: 6.0, running_avg_reward: -9.76, curr_epsilon: 0.0075\n",
      "episode_num 905, curr_reward: -15.0, best_reward: 6.0, running_avg_reward: -9.72, curr_epsilon: 0.0075\n",
      "episode_num 906, curr_reward: -8.0, best_reward: 6.0, running_avg_reward: -9.69, curr_epsilon: 0.0075\n",
      "episode_num 907, curr_reward: -9.0, best_reward: 6.0, running_avg_reward: -9.65, curr_epsilon: 0.0075\n",
      "episode_num 908, curr_reward: -11.0, best_reward: 6.0, running_avg_reward: -9.68, curr_epsilon: 0.0074\n",
      "episode_num 909, curr_reward: -10.0, best_reward: 6.0, running_avg_reward: -9.7, curr_epsilon: 0.0074\n",
      "episode_num 910, curr_reward: -4.0, best_reward: 6.0, running_avg_reward: -9.6, curr_epsilon: 0.0074\n",
      "episode_num 911, curr_reward: -4.0, best_reward: 6.0, running_avg_reward: -9.58, curr_epsilon: 0.0073\n",
      "episode_num 912, curr_reward: -13.0, best_reward: 6.0, running_avg_reward: -9.65, curr_epsilon: 0.0073\n",
      "episode_num 913, curr_reward: -9.0, best_reward: 6.0, running_avg_reward: -9.62, curr_epsilon: 0.0073\n",
      "episode_num 914, curr_reward: -5.0, best_reward: 6.0, running_avg_reward: -9.58, curr_epsilon: 0.0073\n",
      "episode_num 915, curr_reward: -15.0, best_reward: 6.0, running_avg_reward: -9.69, curr_epsilon: 0.0073\n",
      "episode_num 916, curr_reward: -14.0, best_reward: 6.0, running_avg_reward: -9.76, curr_epsilon: 0.0072\n",
      "episode_num 917, curr_reward: -13.0, best_reward: 6.0, running_avg_reward: -9.79, curr_epsilon: 0.0072\n",
      "episode_num 918, curr_reward: -7.0, best_reward: 6.0, running_avg_reward: -9.75, curr_epsilon: 0.0072\n",
      "episode_num 919, curr_reward: -7.0, best_reward: 6.0, running_avg_reward: -9.74, curr_epsilon: 0.0072\n",
      "episode_num 920, curr_reward: -11.0, best_reward: 6.0, running_avg_reward: -9.76, curr_epsilon: 0.0072\n",
      "episode_num 921, curr_reward: 13.0, best_reward: 13.0, running_avg_reward: -9.52, curr_epsilon: 0.0071\n",
      "episode_num 922, curr_reward: -16.0, best_reward: 13.0, running_avg_reward: -9.56, curr_epsilon: 0.0071\n",
      "episode_num 923, curr_reward: -10.0, best_reward: 13.0, running_avg_reward: -9.55, curr_epsilon: 0.0071\n",
      "episode_num 924, curr_reward: -14.0, best_reward: 13.0, running_avg_reward: -9.52, curr_epsilon: 0.0071\n",
      "episode_num 925, curr_reward: -4.0, best_reward: 13.0, running_avg_reward: -9.52, curr_epsilon: 0.0071\n",
      "episode_num 926, curr_reward: 1.0, best_reward: 13.0, running_avg_reward: -9.41, curr_epsilon: 0.007\n",
      "episode_num 927, curr_reward: -7.0, best_reward: 13.0, running_avg_reward: -9.37, curr_epsilon: 0.007\n",
      "episode_num 928, curr_reward: -10.0, best_reward: 13.0, running_avg_reward: -9.35, curr_epsilon: 0.007\n",
      "checkpointing current model weights. highest running_average_reward of -9.23 achieved!\n",
      "episode_num 929, curr_reward: -2.0, best_reward: 13.0, running_avg_reward: -9.23, curr_epsilon: 0.007\n",
      "episode_num 930, curr_reward: -15.0, best_reward: 13.0, running_avg_reward: -9.3, curr_epsilon: 0.0069\n",
      "episode_num 931, curr_reward: -10.0, best_reward: 13.0, running_avg_reward: -9.27, curr_epsilon: 0.0069\n",
      "episode_num 932, curr_reward: -9.0, best_reward: 13.0, running_avg_reward: -9.3, curr_epsilon: 0.0069\n",
      "checkpointing current model weights. highest running_average_reward of -9.2 achieved!\n",
      "episode_num 933, curr_reward: -3.0, best_reward: 13.0, running_avg_reward: -9.2, curr_epsilon: 0.0069\n",
      "episode_num 934, curr_reward: -9.0, best_reward: 13.0, running_avg_reward: -9.25, curr_epsilon: 0.0069\n",
      "episode_num 935, curr_reward: -11.0, best_reward: 13.0, running_avg_reward: -9.28, curr_epsilon: 0.0068\n",
      "checkpointing current model weights. highest running_average_reward of -9.15 achieved!\n",
      "episode_num 936, curr_reward: -1.0, best_reward: 13.0, running_avg_reward: -9.15, curr_epsilon: 0.0068\n",
      "checkpointing current model weights. highest running_average_reward of -9.14 achieved!\n",
      "episode_num 937, curr_reward: -13.0, best_reward: 13.0, running_avg_reward: -9.14, curr_epsilon: 0.0068\n",
      "checkpointing current model weights. highest running_average_reward of -9.09 achieved!\n",
      "episode_num 938, curr_reward: -8.0, best_reward: 13.0, running_avg_reward: -9.09, curr_epsilon: 0.0068\n",
      "episode_num 939, curr_reward: -7.0, best_reward: 13.0, running_avg_reward: -9.15, curr_epsilon: 0.0068\n",
      "episode_num 940, curr_reward: -17.0, best_reward: 13.0, running_avg_reward: -9.18, curr_epsilon: 0.0067\n",
      "episode_num 941, curr_reward: -8.0, best_reward: 13.0, running_avg_reward: -9.15, curr_epsilon: 0.0067\n",
      "checkpointing current model weights. highest running_average_reward of -9.05 achieved!\n",
      "episode_num 942, curr_reward: -5.0, best_reward: 13.0, running_avg_reward: -9.05, curr_epsilon: 0.0067\n",
      "checkpointing current model weights. highest running_average_reward of -9.0 achieved!\n",
      "episode_num 943, curr_reward: -1.0, best_reward: 13.0, running_avg_reward: -9.0, curr_epsilon: 0.0067\n",
      "checkpointing current model weights. highest running_average_reward of -8.92 achieved!\n",
      "episode_num 944, curr_reward: -5.0, best_reward: 13.0, running_avg_reward: -8.92, curr_epsilon: 0.0067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointing current model weights. highest running_average_reward of -8.9 achieved!\n",
      "episode_num 945, curr_reward: -8.0, best_reward: 13.0, running_avg_reward: -8.9, curr_epsilon: 0.0066\n",
      "checkpointing current model weights. highest running_average_reward of -8.88 achieved!\n",
      "episode_num 946, curr_reward: -2.0, best_reward: 13.0, running_avg_reward: -8.88, curr_epsilon: 0.0066\n",
      "checkpointing current model weights. highest running_average_reward of -8.86 achieved!\n",
      "episode_num 947, curr_reward: -5.0, best_reward: 13.0, running_avg_reward: -8.86, curr_epsilon: 0.0066\n",
      "checkpointing current model weights. highest running_average_reward of -8.82 achieved!\n",
      "episode_num 948, curr_reward: -9.0, best_reward: 13.0, running_avg_reward: -8.82, curr_epsilon: 0.0066\n",
      "checkpointing current model weights. highest running_average_reward of -8.81 achieved!\n",
      "episode_num 949, curr_reward: -11.0, best_reward: 13.0, running_avg_reward: -8.81, curr_epsilon: 0.0066\n",
      "episode_num 950, curr_reward: -12.0, best_reward: 13.0, running_avg_reward: -8.83, curr_epsilon: 0.0066\n",
      "checkpointing current model weights. highest running_average_reward of -8.73 achieved!\n",
      "episode_num 951, curr_reward: -9.0, best_reward: 13.0, running_avg_reward: -8.73, curr_epsilon: 0.0065\n",
      "checkpointing current model weights. highest running_average_reward of -8.59 achieved!\n",
      "episode_num 952, curr_reward: -2.0, best_reward: 13.0, running_avg_reward: -8.59, curr_epsilon: 0.0065\n",
      "checkpointing current model weights. highest running_average_reward of -8.57 achieved!\n",
      "episode_num 953, curr_reward: -7.0, best_reward: 13.0, running_avg_reward: -8.57, curr_epsilon: 0.0065\n",
      "checkpointing current model weights. highest running_average_reward of -8.44 achieved!\n",
      "episode_num 954, curr_reward: -2.0, best_reward: 13.0, running_avg_reward: -8.44, curr_epsilon: 0.0065\n",
      "checkpointing current model weights. highest running_average_reward of -8.34 achieved!\n",
      "episode_num 955, curr_reward: -4.0, best_reward: 13.0, running_avg_reward: -8.34, curr_epsilon: 0.0065\n",
      "checkpointing current model weights. highest running_average_reward of -8.29 achieved!\n",
      "episode_num 956, curr_reward: -3.0, best_reward: 13.0, running_avg_reward: -8.29, curr_epsilon: 0.0064\n",
      "episode_num 957, curr_reward: -12.0, best_reward: 13.0, running_avg_reward: -8.34, curr_epsilon: 0.0064\n",
      "episode_num 958, curr_reward: -7.0, best_reward: 13.0, running_avg_reward: -8.33, curr_epsilon: 0.0064\n",
      "episode_num 959, curr_reward: -8.0, best_reward: 13.0, running_avg_reward: -8.36, curr_epsilon: 0.0064\n",
      "episode_num 960, curr_reward: -14.0, best_reward: 13.0, running_avg_reward: -8.52, curr_epsilon: 0.0064\n",
      "episode_num 961, curr_reward: -6.0, best_reward: 13.0, running_avg_reward: -8.44, curr_epsilon: 0.0064\n",
      "episode_num 962, curr_reward: -9.0, best_reward: 13.0, running_avg_reward: -8.52, curr_epsilon: 0.0064\n",
      "episode_num 963, curr_reward: -10.0, best_reward: 13.0, running_avg_reward: -8.49, curr_epsilon: 0.0063\n",
      "episode_num 964, curr_reward: 5.0, best_reward: 13.0, running_avg_reward: -8.29, curr_epsilon: 0.0063\n",
      "checkpointing current model weights. highest running_average_reward of -8.12 achieved!\n",
      "episode_num 965, curr_reward: 5.0, best_reward: 13.0, running_avg_reward: -8.12, curr_epsilon: 0.0063\n",
      "checkpointing current model weights. highest running_average_reward of -8.11 achieved!\n",
      "episode_num 966, curr_reward: -7.0, best_reward: 13.0, running_avg_reward: -8.11, curr_epsilon: 0.0063\n",
      "episode_num 967, curr_reward: -11.0, best_reward: 13.0, running_avg_reward: -8.2, curr_epsilon: 0.0063\n",
      "episode_num 968, curr_reward: -11.0, best_reward: 13.0, running_avg_reward: -8.27, curr_epsilon: 0.0063\n",
      "episode_num 969, curr_reward: -5.0, best_reward: 13.0, running_avg_reward: -8.26, curr_epsilon: 0.0063\n",
      "episode_num 970, curr_reward: -10.0, best_reward: 13.0, running_avg_reward: -8.22, curr_epsilon: 0.0063\n",
      "episode_num 971, curr_reward: -8.0, best_reward: 13.0, running_avg_reward: -8.15, curr_epsilon: 0.0062\n",
      "checkpointing current model weights. highest running_average_reward of -8.07 achieved!\n",
      "episode_num 972, curr_reward: -1.0, best_reward: 13.0, running_avg_reward: -8.07, curr_epsilon: 0.0062\n",
      "checkpointing current model weights. highest running_average_reward of -7.88 achieved!\n",
      "episode_num 973, curr_reward: 6.0, best_reward: 13.0, running_avg_reward: -7.88, curr_epsilon: 0.0062\n",
      "checkpointing current model weights. highest running_average_reward of -7.83 achieved!\n",
      "episode_num 974, curr_reward: -3.0, best_reward: 13.0, running_avg_reward: -7.83, curr_epsilon: 0.0062\n",
      "episode_num 975, curr_reward: -8.0, best_reward: 13.0, running_avg_reward: -7.85, curr_epsilon: 0.0062\n",
      "episode_num 976, curr_reward: -16.0, best_reward: 13.0, running_avg_reward: -7.95, curr_epsilon: 0.0062\n",
      "episode_num 977, curr_reward: -19.0, best_reward: 13.0, running_avg_reward: -8.04, curr_epsilon: 0.0062\n",
      "episode_num 978, curr_reward: -7.0, best_reward: 13.0, running_avg_reward: -8.14, curr_epsilon: 0.0061\n",
      "episode_num 979, curr_reward: -7.0, best_reward: 13.0, running_avg_reward: -8.13, curr_epsilon: 0.0061\n",
      "episode_num 980, curr_reward: 8.0, best_reward: 13.0, running_avg_reward: -8.0, curr_epsilon: 0.0061\n",
      "episode_num 981, curr_reward: -4.0, best_reward: 13.0, running_avg_reward: -7.95, curr_epsilon: 0.0061\n",
      "episode_num 982, curr_reward: -13.0, best_reward: 13.0, running_avg_reward: -8.0, curr_epsilon: 0.0061\n",
      "episode_num 983, curr_reward: -13.0, best_reward: 13.0, running_avg_reward: -7.96, curr_epsilon: 0.0061\n",
      "episode_num 984, curr_reward: -6.0, best_reward: 13.0, running_avg_reward: -7.89, curr_epsilon: 0.0061\n",
      "checkpointing current model weights. highest running_average_reward of -7.7 achieved!\n",
      "episode_num 985, curr_reward: 1.0, best_reward: 13.0, running_avg_reward: -7.7, curr_epsilon: 0.0061\n",
      "episode_num 986, curr_reward: -6.0, best_reward: 13.0, running_avg_reward: -7.7, curr_epsilon: 0.0061\n",
      "checkpointing current model weights. highest running_average_reward of -7.64 achieved!\n",
      "episode_num 987, curr_reward: -9.0, best_reward: 13.0, running_avg_reward: -7.64, curr_epsilon: 0.0061\n",
      "checkpointing current model weights. highest running_average_reward of -7.59 achieved!\n",
      "episode_num 988, curr_reward: -12.0, best_reward: 13.0, running_avg_reward: -7.59, curr_epsilon: 0.006\n",
      "checkpointing current model weights. highest running_average_reward of -7.57 achieved!\n",
      "episode_num 989, curr_reward: -1.0, best_reward: 13.0, running_avg_reward: -7.57, curr_epsilon: 0.006\n",
      "checkpointing current model weights. highest running_average_reward of -7.5 achieved!\n",
      "episode_num 990, curr_reward: -6.0, best_reward: 13.0, running_avg_reward: -7.5, curr_epsilon: 0.006\n",
      "episode_num 991, curr_reward: -5.0, best_reward: 13.0, running_avg_reward: -7.51, curr_epsilon: 0.006\n",
      "checkpointing current model weights. highest running_average_reward of -7.4 achieved!\n",
      "episode_num 992, curr_reward: 1.0, best_reward: 13.0, running_avg_reward: -7.4, curr_epsilon: 0.006\n",
      "episode_num 993, curr_reward: -11.0, best_reward: 13.0, running_avg_reward: -7.46, curr_epsilon: 0.006\n",
      "episode_num 994, curr_reward: -6.0, best_reward: 13.0, running_avg_reward: -7.47, curr_epsilon: 0.006\n",
      "checkpointing current model weights. highest running_average_reward of -7.26 achieved!\n",
      "episode_num 995, curr_reward: 12.0, best_reward: 13.0, running_avg_reward: -7.26, curr_epsilon: 0.006\n",
      "checkpointing current model weights. highest running_average_reward of -7.2 achieved!\n",
      "episode_num 996, curr_reward: -11.0, best_reward: 13.0, running_avg_reward: -7.2, curr_epsilon: 0.006\n",
      "episode_num 997, curr_reward: -3.0, best_reward: 13.0, running_avg_reward: -7.21, curr_epsilon: 0.0059\n",
      "checkpointing current model weights. highest running_average_reward of -6.98 achieved!\n",
      "episode_num 998, curr_reward: 7.0, best_reward: 13.0, running_avg_reward: -6.98, curr_epsilon: 0.0059\n",
      "checkpointing current model weights. highest running_average_reward of -6.74 achieved!\n",
      "episode_num 999, curr_reward: 6.0, best_reward: 13.0, running_avg_reward: -6.74, curr_epsilon: 0.0059\n",
      "checkpointing current model weights. highest running_average_reward of -6.64 achieved!\n",
      "episode_num 1000, curr_reward: -4.0, best_reward: 13.0, running_avg_reward: -6.64, curr_epsilon: 0.0059\n",
      "checkpointing current model weights. highest running_average_reward of -6.61 achieved!\n",
      "episode_num 1001, curr_reward: -9.0, best_reward: 13.0, running_avg_reward: -6.61, curr_epsilon: 0.0059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 1002, curr_reward: -15.0, best_reward: 13.0, running_avg_reward: -6.72, curr_epsilon: 0.0059\n",
      "episode_num 1003, curr_reward: -3.0, best_reward: 13.0, running_avg_reward: -6.66, curr_epsilon: 0.0059\n",
      "episode_num 1004, curr_reward: -7.0, best_reward: 13.0, running_avg_reward: -6.72, curr_epsilon: 0.0059\n",
      "episode_num 1005, curr_reward: -12.0, best_reward: 13.0, running_avg_reward: -6.69, curr_epsilon: 0.0059\n",
      "episode_num 1006, curr_reward: -6.0, best_reward: 13.0, running_avg_reward: -6.67, curr_epsilon: 0.0059\n",
      "episode_num 1007, curr_reward: -5.0, best_reward: 13.0, running_avg_reward: -6.63, curr_epsilon: 0.0059\n",
      "checkpointing current model weights. highest running_average_reward of -6.57 achieved!\n",
      "episode_num 1008, curr_reward: -5.0, best_reward: 13.0, running_avg_reward: -6.57, curr_epsilon: 0.0058\n",
      "episode_num 1009, curr_reward: -10.0, best_reward: 13.0, running_avg_reward: -6.57, curr_epsilon: 0.0058\n",
      "episode_num 1010, curr_reward: -10.0, best_reward: 13.0, running_avg_reward: -6.63, curr_epsilon: 0.0058\n",
      "episode_num 1011, curr_reward: 1.0, best_reward: 13.0, running_avg_reward: -6.58, curr_epsilon: 0.0058\n",
      "checkpointing current model weights. highest running_average_reward of -6.31 achieved!\n",
      "episode_num 1012, curr_reward: 14.0, best_reward: 14.0, running_avg_reward: -6.31, curr_epsilon: 0.0058\n",
      "checkpointing current model weights. highest running_average_reward of -6.3 achieved!\n",
      "episode_num 1013, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -6.3, curr_epsilon: 0.0058\n",
      "checkpointing current model weights. highest running_average_reward of -6.17 achieved!\n",
      "episode_num 1014, curr_reward: 8.0, best_reward: 14.0, running_avg_reward: -6.17, curr_epsilon: 0.0058\n",
      "checkpointing current model weights. highest running_average_reward of -5.94 achieved!\n",
      "episode_num 1015, curr_reward: 8.0, best_reward: 14.0, running_avg_reward: -5.94, curr_epsilon: 0.0058\n",
      "checkpointing current model weights. highest running_average_reward of -5.87 achieved!\n",
      "episode_num 1016, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -5.87, curr_epsilon: 0.0058\n",
      "checkpointing current model weights. highest running_average_reward of -5.85 achieved!\n",
      "episode_num 1017, curr_reward: -11.0, best_reward: 14.0, running_avg_reward: -5.85, curr_epsilon: 0.0058\n",
      "checkpointing current model weights. highest running_average_reward of -5.66 achieved!\n",
      "episode_num 1018, curr_reward: 12.0, best_reward: 14.0, running_avg_reward: -5.66, curr_epsilon: 0.0058\n",
      "episode_num 1019, curr_reward: -13.0, best_reward: 14.0, running_avg_reward: -5.72, curr_epsilon: 0.0058\n",
      "episode_num 1020, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -5.68, curr_epsilon: 0.0057\n",
      "episode_num 1021, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.87, curr_epsilon: 0.0057\n",
      "episode_num 1022, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.79, curr_epsilon: 0.0057\n",
      "episode_num 1023, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.79, curr_epsilon: 0.0057\n",
      "checkpointing current model weights. highest running_average_reward of -5.62 achieved!\n",
      "episode_num 1024, curr_reward: 3.0, best_reward: 14.0, running_avg_reward: -5.62, curr_epsilon: 0.0057\n",
      "episode_num 1025, curr_reward: -11.0, best_reward: 14.0, running_avg_reward: -5.69, curr_epsilon: 0.0057\n",
      "episode_num 1026, curr_reward: -15.0, best_reward: 14.0, running_avg_reward: -5.85, curr_epsilon: 0.0057\n",
      "episode_num 1027, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -5.82, curr_epsilon: 0.0057\n",
      "episode_num 1028, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.82, curr_epsilon: 0.0057\n",
      "episode_num 1029, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.81, curr_epsilon: 0.0057\n",
      "episode_num 1030, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.72, curr_epsilon: 0.0057\n",
      "episode_num 1031, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.7, curr_epsilon: 0.0057\n",
      "episode_num 1032, curr_reward: -11.0, best_reward: 14.0, running_avg_reward: -5.72, curr_epsilon: 0.0057\n",
      "episode_num 1033, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.7, curr_epsilon: 0.0056\n",
      "checkpointing current model weights. highest running_average_reward of -5.6 achieved!\n",
      "episode_num 1034, curr_reward: 1.0, best_reward: 14.0, running_avg_reward: -5.6, curr_epsilon: 0.0056\n",
      "checkpointing current model weights. highest running_average_reward of -5.58 achieved!\n",
      "episode_num 1035, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.58, curr_epsilon: 0.0056\n",
      "episode_num 1036, curr_reward: -11.0, best_reward: 14.0, running_avg_reward: -5.68, curr_epsilon: 0.0056\n",
      "episode_num 1037, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -5.62, curr_epsilon: 0.0056\n",
      "checkpointing current model weights. highest running_average_reward of -5.57 achieved!\n",
      "episode_num 1038, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -5.57, curr_epsilon: 0.0056\n",
      "episode_num 1039, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.59, curr_epsilon: 0.0056\n",
      "checkpointing current model weights. highest running_average_reward of -5.36 achieved!\n",
      "episode_num 1040, curr_reward: 6.0, best_reward: 14.0, running_avg_reward: -5.36, curr_epsilon: 0.0056\n",
      "checkpointing current model weights. highest running_average_reward of -5.29 achieved!\n",
      "episode_num 1041, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.29, curr_epsilon: 0.0056\n",
      "checkpointing current model weights. highest running_average_reward of -5.28 achieved!\n",
      "episode_num 1042, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -5.28, curr_epsilon: 0.0056\n",
      "checkpointing current model weights. highest running_average_reward of -5.18 achieved!\n",
      "episode_num 1043, curr_reward: 9.0, best_reward: 14.0, running_avg_reward: -5.18, curr_epsilon: 0.0056\n",
      "checkpointing current model weights. highest running_average_reward of -5.12 achieved!\n",
      "episode_num 1044, curr_reward: 1.0, best_reward: 14.0, running_avg_reward: -5.12, curr_epsilon: 0.0056\n",
      "episode_num 1045, curr_reward: -13.0, best_reward: 14.0, running_avg_reward: -5.17, curr_epsilon: 0.0056\n",
      "episode_num 1046, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.2, curr_epsilon: 0.0056\n",
      "episode_num 1047, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.29, curr_epsilon: 0.0056\n",
      "episode_num 1048, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.28, curr_epsilon: 0.0055\n",
      "episode_num 1049, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.18, curr_epsilon: 0.0055\n",
      "episode_num 1050, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -5.13, curr_epsilon: 0.0055\n",
      "checkpointing current model weights. highest running_average_reward of -5.08 achieved!\n",
      "episode_num 1051, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -5.08, curr_epsilon: 0.0055\n",
      "episode_num 1052, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -5.1, curr_epsilon: 0.0055\n",
      "checkpointing current model weights. highest running_average_reward of -5.0 achieved!\n",
      "episode_num 1053, curr_reward: 3.0, best_reward: 14.0, running_avg_reward: -5.0, curr_epsilon: 0.0055\n",
      "episode_num 1054, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -5.0, curr_epsilon: 0.0055\n",
      "episode_num 1055, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.01, curr_epsilon: 0.0055\n",
      "episode_num 1056, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.07, curr_epsilon: 0.0055\n",
      "checkpointing current model weights. highest running_average_reward of -4.98 achieved!\n",
      "episode_num 1057, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -4.98, curr_epsilon: 0.0055\n",
      "episode_num 1058, curr_reward: -20.0, best_reward: 14.0, running_avg_reward: -5.11, curr_epsilon: 0.0055\n",
      "episode_num 1059, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -5.15, curr_epsilon: 0.0055\n",
      "episode_num 1060, curr_reward: -13.0, best_reward: 14.0, running_avg_reward: -5.14, curr_epsilon: 0.0055\n",
      "episode_num 1061, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -5.15, curr_epsilon: 0.0055\n",
      "episode_num 1062, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.15, curr_epsilon: 0.0055\n",
      "episode_num 1063, curr_reward: 2.0, best_reward: 14.0, running_avg_reward: -5.03, curr_epsilon: 0.0055\n",
      "episode_num 1064, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -5.12, curr_epsilon: 0.0055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 1065, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.23, curr_epsilon: 0.0055\n",
      "episode_num 1066, curr_reward: 5.0, best_reward: 14.0, running_avg_reward: -5.11, curr_epsilon: 0.0054\n",
      "episode_num 1067, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -5.02, curr_epsilon: 0.0054\n",
      "episode_num 1068, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.05, curr_epsilon: 0.0054\n",
      "episode_num 1069, curr_reward: -13.0, best_reward: 14.0, running_avg_reward: -5.13, curr_epsilon: 0.0054\n",
      "episode_num 1070, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -5.15, curr_epsilon: 0.0054\n",
      "episode_num 1071, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.08, curr_epsilon: 0.0054\n",
      "episode_num 1072, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.21, curr_epsilon: 0.0054\n",
      "episode_num 1073, curr_reward: 1.0, best_reward: 14.0, running_avg_reward: -5.26, curr_epsilon: 0.0054\n",
      "episode_num 1074, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -5.3, curr_epsilon: 0.0054\n",
      "episode_num 1075, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.23, curr_epsilon: 0.0054\n",
      "episode_num 1076, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.12, curr_epsilon: 0.0054\n",
      "checkpointing current model weights. highest running_average_reward of -4.92 achieved!\n",
      "episode_num 1077, curr_reward: 1.0, best_reward: 14.0, running_avg_reward: -4.92, curr_epsilon: 0.0054\n",
      "checkpointing current model weights. highest running_average_reward of -4.89 achieved!\n",
      "episode_num 1078, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -4.89, curr_epsilon: 0.0054\n",
      "checkpointing current model weights. highest running_average_reward of -4.85 achieved!\n",
      "episode_num 1079, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -4.85, curr_epsilon: 0.0054\n",
      "episode_num 1080, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.03, curr_epsilon: 0.0054\n",
      "episode_num 1081, curr_reward: 8.0, best_reward: 14.0, running_avg_reward: -4.91, curr_epsilon: 0.0054\n",
      "checkpointing current model weights. highest running_average_reward of -4.84 achieved!\n",
      "episode_num 1082, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -4.84, curr_epsilon: 0.0054\n",
      "episode_num 1083, curr_reward: -13.0, best_reward: 14.0, running_avg_reward: -4.84, curr_epsilon: 0.0054\n",
      "episode_num 1084, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -4.84, curr_epsilon: 0.0054\n",
      "episode_num 1085, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -4.86, curr_epsilon: 0.0054\n",
      "episode_num 1086, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -4.89, curr_epsilon: 0.0054\n",
      "episode_num 1087, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -4.86, curr_epsilon: 0.0054\n",
      "episode_num 1088, curr_reward: -11.0, best_reward: 14.0, running_avg_reward: -4.85, curr_epsilon: 0.0054\n",
      "episode_num 1089, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -4.89, curr_epsilon: 0.0053\n",
      "episode_num 1090, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -4.92, curr_epsilon: 0.0053\n",
      "episode_num 1091, curr_reward: -13.0, best_reward: 14.0, running_avg_reward: -5.0, curr_epsilon: 0.0053\n",
      "episode_num 1092, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.07, curr_epsilon: 0.0053\n",
      "episode_num 1093, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -4.98, curr_epsilon: 0.0053\n",
      "episode_num 1094, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -4.99, curr_epsilon: 0.0053\n",
      "episode_num 1095, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.21, curr_epsilon: 0.0053\n",
      "episode_num 1096, curr_reward: 2.0, best_reward: 14.0, running_avg_reward: -5.08, curr_epsilon: 0.0053\n",
      "episode_num 1097, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.06, curr_epsilon: 0.0053\n",
      "episode_num 1098, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.19, curr_epsilon: 0.0053\n",
      "episode_num 1099, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.35, curr_epsilon: 0.0053\n",
      "episode_num 1100, curr_reward: 4.0, best_reward: 14.0, running_avg_reward: -5.27, curr_epsilon: 0.0053\n",
      "episode_num 1101, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.26, curr_epsilon: 0.0053\n",
      "episode_num 1102, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.16, curr_epsilon: 0.0053\n",
      "episode_num 1103, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.21, curr_epsilon: 0.0053\n",
      "episode_num 1104, curr_reward: 13.0, best_reward: 14.0, running_avg_reward: -5.01, curr_epsilon: 0.0053\n",
      "episode_num 1105, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -4.9, curr_epsilon: 0.0053\n",
      "checkpointing current model weights. highest running_average_reward of -4.8 achieved!\n",
      "episode_num 1106, curr_reward: 4.0, best_reward: 14.0, running_avg_reward: -4.8, curr_epsilon: 0.0053\n",
      "checkpointing current model weights. highest running_average_reward of -4.77 achieved!\n",
      "episode_num 1107, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -4.77, curr_epsilon: 0.0053\n",
      "checkpointing current model weights. highest running_average_reward of -4.66 achieved!\n",
      "episode_num 1108, curr_reward: 6.0, best_reward: 14.0, running_avg_reward: -4.66, curr_epsilon: 0.0053\n",
      "checkpointing current model weights. highest running_average_reward of -4.61 achieved!\n",
      "episode_num 1109, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -4.61, curr_epsilon: 0.0053\n",
      "checkpointing current model weights. highest running_average_reward of -4.52 achieved!\n",
      "episode_num 1110, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -4.52, curr_epsilon: 0.0053\n",
      "episode_num 1111, curr_reward: -11.0, best_reward: 14.0, running_avg_reward: -4.64, curr_epsilon: 0.0053\n",
      "episode_num 1112, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -4.82, curr_epsilon: 0.0053\n",
      "episode_num 1113, curr_reward: -13.0, best_reward: 14.0, running_avg_reward: -4.87, curr_epsilon: 0.0053\n",
      "episode_num 1114, curr_reward: 2.0, best_reward: 14.0, running_avg_reward: -4.93, curr_epsilon: 0.0053\n",
      "episode_num 1115, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.09, curr_epsilon: 0.0053\n",
      "episode_num 1116, curr_reward: 11.0, best_reward: 14.0, running_avg_reward: -4.91, curr_epsilon: 0.0053\n",
      "episode_num 1117, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -4.81, curr_epsilon: 0.0053\n",
      "episode_num 1118, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.07, curr_epsilon: 0.0052\n",
      "episode_num 1119, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.03, curr_epsilon: 0.0052\n",
      "episode_num 1120, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -4.97, curr_epsilon: 0.0052\n",
      "episode_num 1121, curr_reward: -13.0, best_reward: 14.0, running_avg_reward: -5.04, curr_epsilon: 0.0052\n",
      "episode_num 1122, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.01, curr_epsilon: 0.0052\n",
      "episode_num 1123, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -4.95, curr_epsilon: 0.0052\n",
      "episode_num 1124, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.04, curr_epsilon: 0.0052\n",
      "episode_num 1125, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.02, curr_epsilon: 0.0052\n",
      "episode_num 1126, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -4.92, curr_epsilon: 0.0052\n",
      "episode_num 1127, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -4.96, curr_epsilon: 0.0052\n",
      "episode_num 1128, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -4.87, curr_epsilon: 0.0052\n",
      "episode_num 1129, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -4.95, curr_epsilon: 0.0052\n",
      "episode_num 1130, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -4.97, curr_epsilon: 0.0052\n",
      "episode_num 1131, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -4.92, curr_epsilon: 0.0052\n",
      "episode_num 1132, curr_reward: 4.0, best_reward: 14.0, running_avg_reward: -4.77, curr_epsilon: 0.0052\n",
      "episode_num 1133, curr_reward: 2.0, best_reward: 14.0, running_avg_reward: -4.74, curr_epsilon: 0.0052\n",
      "episode_num 1134, curr_reward: 1.0, best_reward: 14.0, running_avg_reward: -4.74, curr_epsilon: 0.0052\n",
      "episode_num 1135, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -4.68, curr_epsilon: 0.0052\n",
      "episode_num 1136, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -4.65, curr_epsilon: 0.0052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 1137, curr_reward: -15.0, best_reward: 14.0, running_avg_reward: -4.73, curr_epsilon: 0.0052\n",
      "episode_num 1138, curr_reward: 7.0, best_reward: 14.0, running_avg_reward: -4.63, curr_epsilon: 0.0052\n",
      "episode_num 1139, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -4.59, curr_epsilon: 0.0052\n",
      "episode_num 1140, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -4.66, curr_epsilon: 0.0052\n",
      "episode_num 1141, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -4.66, curr_epsilon: 0.0052\n",
      "episode_num 1142, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -4.76, curr_epsilon: 0.0052\n",
      "episode_num 1143, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -4.91, curr_epsilon: 0.0052\n",
      "episode_num 1144, curr_reward: -11.0, best_reward: 14.0, running_avg_reward: -5.03, curr_epsilon: 0.0052\n",
      "episode_num 1145, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.04, curr_epsilon: 0.0052\n",
      "episode_num 1146, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.13, curr_epsilon: 0.0052\n",
      "episode_num 1147, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -5.11, curr_epsilon: 0.0052\n",
      "episode_num 1148, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -5.15, curr_epsilon: 0.0052\n",
      "episode_num 1149, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.28, curr_epsilon: 0.0052\n",
      "episode_num 1150, curr_reward: -15.0, best_reward: 14.0, running_avg_reward: -5.36, curr_epsilon: 0.0052\n",
      "episode_num 1151, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -5.35, curr_epsilon: 0.0052\n",
      "episode_num 1152, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -5.34, curr_epsilon: 0.0052\n",
      "episode_num 1153, curr_reward: 2.0, best_reward: 14.0, running_avg_reward: -5.35, curr_epsilon: 0.0052\n",
      "episode_num 1154, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -5.4, curr_epsilon: 0.0052\n",
      "episode_num 1155, curr_reward: 2.0, best_reward: 14.0, running_avg_reward: -5.33, curr_epsilon: 0.0052\n",
      "episode_num 1156, curr_reward: -13.0, best_reward: 14.0, running_avg_reward: -5.37, curr_epsilon: 0.0052\n",
      "episode_num 1157, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -5.38, curr_epsilon: 0.0052\n",
      "episode_num 1158, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.26, curr_epsilon: 0.0052\n",
      "episode_num 1159, curr_reward: 4.0, best_reward: 14.0, running_avg_reward: -5.1, curr_epsilon: 0.0052\n",
      "episode_num 1160, curr_reward: 3.0, best_reward: 14.0, running_avg_reward: -4.94, curr_epsilon: 0.0052\n",
      "episode_num 1161, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.01, curr_epsilon: 0.0052\n",
      "episode_num 1162, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -4.93, curr_epsilon: 0.0052\n",
      "episode_num 1163, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.03, curr_epsilon: 0.0052\n",
      "episode_num 1164, curr_reward: 1.0, best_reward: 14.0, running_avg_reward: -4.98, curr_epsilon: 0.0051\n",
      "episode_num 1165, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.02, curr_epsilon: 0.0051\n",
      "episode_num 1166, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.15, curr_epsilon: 0.0051\n",
      "episode_num 1167, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.27, curr_epsilon: 0.0051\n",
      "episode_num 1168, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.18, curr_epsilon: 0.0051\n",
      "episode_num 1169, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.14, curr_epsilon: 0.0051\n",
      "episode_num 1170, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -5.04, curr_epsilon: 0.0051\n",
      "episode_num 1171, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.17, curr_epsilon: 0.0051\n",
      "episode_num 1172, curr_reward: 6.0, best_reward: 14.0, running_avg_reward: -4.97, curr_epsilon: 0.0051\n",
      "episode_num 1173, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.08, curr_epsilon: 0.0051\n",
      "episode_num 1174, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.1, curr_epsilon: 0.0051\n",
      "episode_num 1175, curr_reward: 8.0, best_reward: 14.0, running_avg_reward: -5.01, curr_epsilon: 0.0051\n",
      "episode_num 1176, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -4.98, curr_epsilon: 0.0051\n",
      "episode_num 1177, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -5.01, curr_epsilon: 0.0051\n",
      "episode_num 1178, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.03, curr_epsilon: 0.0051\n",
      "episode_num 1179, curr_reward: -19.0, best_reward: 14.0, running_avg_reward: -5.19, curr_epsilon: 0.0051\n",
      "episode_num 1180, curr_reward: -16.0, best_reward: 14.0, running_avg_reward: -5.25, curr_epsilon: 0.0051\n",
      "episode_num 1181, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -5.45, curr_epsilon: 0.0051\n",
      "episode_num 1182, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.49, curr_epsilon: 0.0051\n",
      "episode_num 1183, curr_reward: 5.0, best_reward: 14.0, running_avg_reward: -5.31, curr_epsilon: 0.0051\n",
      "episode_num 1184, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.33, curr_epsilon: 0.0051\n",
      "episode_num 1185, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.4, curr_epsilon: 0.0051\n",
      "episode_num 1186, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -5.34, curr_epsilon: 0.0051\n",
      "episode_num 1187, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.37, curr_epsilon: 0.0051\n",
      "episode_num 1188, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.36, curr_epsilon: 0.0051\n",
      "episode_num 1189, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.4, curr_epsilon: 0.0051\n",
      "episode_num 1190, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.36, curr_epsilon: 0.0051\n",
      "episode_num 1191, curr_reward: -17.0, best_reward: 14.0, running_avg_reward: -5.4, curr_epsilon: 0.0051\n",
      "episode_num 1192, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.4, curr_epsilon: 0.0051\n",
      "episode_num 1193, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.43, curr_epsilon: 0.0051\n",
      "episode_num 1194, curr_reward: -16.0, best_reward: 14.0, running_avg_reward: -5.52, curr_epsilon: 0.0051\n",
      "episode_num 1195, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.5, curr_epsilon: 0.0051\n",
      "episode_num 1196, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.62, curr_epsilon: 0.0051\n",
      "episode_num 1197, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.62, curr_epsilon: 0.0051\n",
      "episode_num 1198, curr_reward: -13.0, best_reward: 14.0, running_avg_reward: -5.69, curr_epsilon: 0.0051\n",
      "episode_num 1199, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -5.71, curr_epsilon: 0.0051\n",
      "episode_num 1200, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.85, curr_epsilon: 0.0051\n",
      "episode_num 1201, curr_reward: 5.0, best_reward: 14.0, running_avg_reward: -5.72, curr_epsilon: 0.0051\n",
      "episode_num 1202, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.72, curr_epsilon: 0.0051\n",
      "episode_num 1203, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.73, curr_epsilon: 0.0051\n",
      "episode_num 1204, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.94, curr_epsilon: 0.0051\n",
      "episode_num 1205, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -6.02, curr_epsilon: 0.0051\n",
      "episode_num 1206, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -6.07, curr_epsilon: 0.0051\n",
      "episode_num 1207, curr_reward: 5.0, best_reward: 14.0, running_avg_reward: -6.0, curr_epsilon: 0.0051\n",
      "episode_num 1208, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -6.14, curr_epsilon: 0.0051\n",
      "episode_num 1209, curr_reward: -17.0, best_reward: 14.0, running_avg_reward: -6.26, curr_epsilon: 0.0051\n",
      "episode_num 1210, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -6.3, curr_epsilon: 0.0051\n",
      "episode_num 1211, curr_reward: 4.0, best_reward: 14.0, running_avg_reward: -6.15, curr_epsilon: 0.0051\n",
      "episode_num 1212, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -6.19, curr_epsilon: 0.0051\n",
      "episode_num 1213, curr_reward: -15.0, best_reward: 14.0, running_avg_reward: -6.21, curr_epsilon: 0.0051\n",
      "episode_num 1214, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -6.32, curr_epsilon: 0.0051\n",
      "episode_num 1215, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -6.29, curr_epsilon: 0.0051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 1216, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -6.5, curr_epsilon: 0.0051\n",
      "episode_num 1217, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -6.56, curr_epsilon: 0.0051\n",
      "episode_num 1218, curr_reward: -15.0, best_reward: 14.0, running_avg_reward: -6.57, curr_epsilon: 0.0051\n",
      "episode_num 1219, curr_reward: 1.0, best_reward: 14.0, running_avg_reward: -6.47, curr_epsilon: 0.0051\n",
      "episode_num 1220, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -6.55, curr_epsilon: 0.0051\n",
      "episode_num 1221, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -6.48, curr_epsilon: 0.0051\n",
      "episode_num 1222, curr_reward: 8.0, best_reward: 14.0, running_avg_reward: -6.35, curr_epsilon: 0.0051\n",
      "episode_num 1223, curr_reward: -15.0, best_reward: 14.0, running_avg_reward: -6.46, curr_epsilon: 0.0051\n",
      "episode_num 1224, curr_reward: -16.0, best_reward: 14.0, running_avg_reward: -6.56, curr_epsilon: 0.0051\n",
      "episode_num 1225, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -6.48, curr_epsilon: 0.0051\n",
      "episode_num 1226, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -6.53, curr_epsilon: 0.0051\n",
      "episode_num 1227, curr_reward: 3.0, best_reward: 14.0, running_avg_reward: -6.42, curr_epsilon: 0.0051\n",
      "episode_num 1228, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -6.42, curr_epsilon: 0.0051\n",
      "episode_num 1229, curr_reward: -15.0, best_reward: 14.0, running_avg_reward: -6.48, curr_epsilon: 0.0051\n",
      "episode_num 1230, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -6.41, curr_epsilon: 0.0051\n",
      "episode_num 1231, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -6.48, curr_epsilon: 0.0051\n",
      "episode_num 1232, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -6.53, curr_epsilon: 0.0051\n",
      "episode_num 1233, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -6.62, curr_epsilon: 0.0051\n",
      "episode_num 1234, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -6.65, curr_epsilon: 0.0051\n",
      "episode_num 1235, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -6.68, curr_epsilon: 0.0051\n",
      "episode_num 1236, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -6.63, curr_epsilon: 0.0051\n",
      "episode_num 1237, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -6.52, curr_epsilon: 0.0051\n",
      "episode_num 1238, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -6.61, curr_epsilon: 0.0051\n",
      "episode_num 1239, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -6.58, curr_epsilon: 0.0051\n",
      "episode_num 1240, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -6.63, curr_epsilon: 0.0051\n",
      "episode_num 1241, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -6.65, curr_epsilon: 0.0051\n",
      "episode_num 1242, curr_reward: 1.0, best_reward: 14.0, running_avg_reward: -6.5, curr_epsilon: 0.0051\n",
      "episode_num 1243, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -6.49, curr_epsilon: 0.0051\n",
      "episode_num 1244, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -6.46, curr_epsilon: 0.0051\n",
      "episode_num 1245, curr_reward: 2.0, best_reward: 14.0, running_avg_reward: -6.3, curr_epsilon: 0.0051\n",
      "episode_num 1246, curr_reward: 6.0, best_reward: 14.0, running_avg_reward: -6.1, curr_epsilon: 0.0051\n",
      "episode_num 1247, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -6.02, curr_epsilon: 0.0051\n",
      "episode_num 1248, curr_reward: -13.0, best_reward: 14.0, running_avg_reward: -6.03, curr_epsilon: 0.0051\n",
      "episode_num 1249, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -5.91, curr_epsilon: 0.0051\n",
      "episode_num 1250, curr_reward: -16.0, best_reward: 14.0, running_avg_reward: -5.92, curr_epsilon: 0.0051\n",
      "episode_num 1251, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -5.93, curr_epsilon: 0.0051\n",
      "episode_num 1252, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -5.92, curr_epsilon: 0.0051\n",
      "episode_num 1253, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.99, curr_epsilon: 0.0051\n",
      "episode_num 1254, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.97, curr_epsilon: 0.0051\n",
      "episode_num 1255, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -6.05, curr_epsilon: 0.0051\n",
      "episode_num 1256, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.98, curr_epsilon: 0.0051\n",
      "episode_num 1257, curr_reward: 4.0, best_reward: 14.0, running_avg_reward: -5.9, curr_epsilon: 0.0051\n",
      "episode_num 1258, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.83, curr_epsilon: 0.0051\n",
      "episode_num 1259, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -5.91, curr_epsilon: 0.0051\n",
      "episode_num 1260, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -6.03, curr_epsilon: 0.0051\n",
      "episode_num 1261, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.9, curr_epsilon: 0.0051\n",
      "episode_num 1262, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -5.91, curr_epsilon: 0.0051\n",
      "episode_num 1263, curr_reward: 3.0, best_reward: 14.0, running_avg_reward: -5.8, curr_epsilon: 0.0051\n",
      "episode_num 1264, curr_reward: -2.0, best_reward: 14.0, running_avg_reward: -5.83, curr_epsilon: 0.005\n",
      "episode_num 1265, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.87, curr_epsilon: 0.005\n",
      "episode_num 1266, curr_reward: -11.0, best_reward: 14.0, running_avg_reward: -5.9, curr_epsilon: 0.005\n",
      "episode_num 1267, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -5.79, curr_epsilon: 0.005\n",
      "episode_num 1268, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.8, curr_epsilon: 0.005\n",
      "episode_num 1269, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.77, curr_epsilon: 0.005\n",
      "episode_num 1270, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.85, curr_epsilon: 0.005\n",
      "episode_num 1271, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.85, curr_epsilon: 0.005\n",
      "episode_num 1272, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -5.94, curr_epsilon: 0.005\n",
      "episode_num 1273, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.9, curr_epsilon: 0.005\n",
      "episode_num 1274, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -5.84, curr_epsilon: 0.005\n",
      "episode_num 1275, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.93, curr_epsilon: 0.005\n",
      "episode_num 1276, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.92, curr_epsilon: 0.005\n",
      "episode_num 1277, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -5.97, curr_epsilon: 0.005\n",
      "episode_num 1278, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -5.94, curr_epsilon: 0.005\n",
      "episode_num 1279, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -5.82, curr_epsilon: 0.005\n",
      "episode_num 1280, curr_reward: -15.0, best_reward: 14.0, running_avg_reward: -5.81, curr_epsilon: 0.005\n",
      "episode_num 1281, curr_reward: 1.0, best_reward: 14.0, running_avg_reward: -5.68, curr_epsilon: 0.005\n",
      "episode_num 1282, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.67, curr_epsilon: 0.005\n",
      "episode_num 1283, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.73, curr_epsilon: 0.005\n",
      "episode_num 1284, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -5.66, curr_epsilon: 0.005\n",
      "episode_num 1285, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.68, curr_epsilon: 0.005\n",
      "episode_num 1286, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -5.71, curr_epsilon: 0.005\n",
      "episode_num 1287, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.67, curr_epsilon: 0.005\n",
      "episode_num 1288, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -5.69, curr_epsilon: 0.005\n",
      "episode_num 1289, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.7, curr_epsilon: 0.005\n",
      "episode_num 1290, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -5.72, curr_epsilon: 0.005\n",
      "episode_num 1291, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -5.67, curr_epsilon: 0.005\n",
      "episode_num 1292, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -5.68, curr_epsilon: 0.005\n",
      "episode_num 1293, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.73, curr_epsilon: 0.005\n",
      "episode_num 1294, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.65, curr_epsilon: 0.005\n",
      "episode_num 1295, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.66, curr_epsilon: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 1296, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.64, curr_epsilon: 0.005\n",
      "episode_num 1297, curr_reward: -18.0, best_reward: 14.0, running_avg_reward: -5.81, curr_epsilon: 0.005\n",
      "episode_num 1298, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -5.73, curr_epsilon: 0.005\n",
      "episode_num 1299, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.7, curr_epsilon: 0.005\n",
      "episode_num 1300, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -5.69, curr_epsilon: 0.005\n",
      "episode_num 1301, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -5.88, curr_epsilon: 0.005\n",
      "episode_num 1302, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -5.91, curr_epsilon: 0.005\n",
      "episode_num 1303, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -5.86, curr_epsilon: 0.005\n",
      "episode_num 1304, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -5.9, curr_epsilon: 0.005\n",
      "episode_num 1305, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -5.91, curr_epsilon: 0.005\n",
      "episode_num 1306, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -6.0, curr_epsilon: 0.005\n",
      "episode_num 1307, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -6.13, curr_epsilon: 0.005\n",
      "episode_num 1308, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -6.19, curr_epsilon: 0.005\n",
      "episode_num 1309, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -6.14, curr_epsilon: 0.005\n",
      "episode_num 1310, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -6.18, curr_epsilon: 0.005\n",
      "episode_num 1311, curr_reward: -11.0, best_reward: 14.0, running_avg_reward: -6.33, curr_epsilon: 0.005\n",
      "episode_num 1312, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -6.34, curr_epsilon: 0.005\n",
      "episode_num 1313, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -6.31, curr_epsilon: 0.005\n",
      "episode_num 1314, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -6.29, curr_epsilon: 0.005\n",
      "episode_num 1315, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -6.31, curr_epsilon: 0.005\n",
      "episode_num 1316, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -6.22, curr_epsilon: 0.005\n",
      "episode_num 1317, curr_reward: -19.0, best_reward: 14.0, running_avg_reward: -6.34, curr_epsilon: 0.005\n",
      "episode_num 1318, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -6.25, curr_epsilon: 0.005\n",
      "episode_num 1319, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -6.34, curr_epsilon: 0.005\n",
      "episode_num 1320, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -6.26, curr_epsilon: 0.005\n",
      "episode_num 1321, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -6.29, curr_epsilon: 0.005\n",
      "episode_num 1322, curr_reward: -4.0, best_reward: 14.0, running_avg_reward: -6.41, curr_epsilon: 0.005\n",
      "episode_num 1323, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -6.4, curr_epsilon: 0.005\n",
      "episode_num 1324, curr_reward: -15.0, best_reward: 14.0, running_avg_reward: -6.39, curr_epsilon: 0.005\n",
      "episode_num 1325, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -6.44, curr_epsilon: 0.005\n",
      "episode_num 1326, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -6.46, curr_epsilon: 0.005\n",
      "episode_num 1327, curr_reward: 3.0, best_reward: 14.0, running_avg_reward: -6.46, curr_epsilon: 0.005\n",
      "episode_num 1328, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -6.59, curr_epsilon: 0.005\n",
      "episode_num 1329, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -6.5, curr_epsilon: 0.005\n",
      "episode_num 1330, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -6.61, curr_epsilon: 0.005\n",
      "episode_num 1331, curr_reward: 5.0, best_reward: 14.0, running_avg_reward: -6.46, curr_epsilon: 0.005\n",
      "episode_num 1332, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -6.48, curr_epsilon: 0.005\n",
      "episode_num 1333, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -6.53, curr_epsilon: 0.005\n",
      "episode_num 1334, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -6.59, curr_epsilon: 0.005\n",
      "episode_num 1335, curr_reward: 2.0, best_reward: 14.0, running_avg_reward: -6.51, curr_epsilon: 0.005\n",
      "episode_num 1336, curr_reward: -1.0, best_reward: 14.0, running_avg_reward: -6.49, curr_epsilon: 0.005\n",
      "episode_num 1337, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -6.52, curr_epsilon: 0.005\n",
      "episode_num 1338, curr_reward: -7.0, best_reward: 14.0, running_avg_reward: -6.57, curr_epsilon: 0.005\n",
      "episode_num 1339, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -6.65, curr_epsilon: 0.005\n",
      "episode_num 1340, curr_reward: -13.0, best_reward: 14.0, running_avg_reward: -6.72, curr_epsilon: 0.005\n",
      "episode_num 1341, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -6.75, curr_epsilon: 0.005\n",
      "episode_num 1342, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -6.82, curr_epsilon: 0.005\n",
      "episode_num 1343, curr_reward: -16.0, best_reward: 14.0, running_avg_reward: -6.93, curr_epsilon: 0.005\n",
      "episode_num 1344, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -6.94, curr_epsilon: 0.005\n",
      "episode_num 1345, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -7.1, curr_epsilon: 0.005\n",
      "episode_num 1346, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -7.25, curr_epsilon: 0.005\n",
      "episode_num 1347, curr_reward: -9.0, best_reward: 14.0, running_avg_reward: -7.3, curr_epsilon: 0.005\n",
      "episode_num 1348, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -7.23, curr_epsilon: 0.005\n",
      "episode_num 1349, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -7.24, curr_epsilon: 0.005\n",
      "episode_num 1350, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -7.13, curr_epsilon: 0.005\n",
      "episode_num 1351, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -7.23, curr_epsilon: 0.005\n",
      "episode_num 1352, curr_reward: -14.0, best_reward: 14.0, running_avg_reward: -7.35, curr_epsilon: 0.005\n",
      "episode_num 1353, curr_reward: -12.0, best_reward: 14.0, running_avg_reward: -7.42, curr_epsilon: 0.005\n",
      "episode_num 1354, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -7.47, curr_epsilon: 0.005\n",
      "episode_num 1355, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -7.49, curr_epsilon: 0.005\n",
      "episode_num 1356, curr_reward: -10.0, best_reward: 14.0, running_avg_reward: -7.53, curr_epsilon: 0.005\n",
      "episode_num 1357, curr_reward: -15.0, best_reward: 14.0, running_avg_reward: -7.72, curr_epsilon: 0.005\n",
      "episode_num 1358, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -7.76, curr_epsilon: 0.005\n",
      "episode_num 1359, curr_reward: -6.0, best_reward: 14.0, running_avg_reward: -7.78, curr_epsilon: 0.005\n",
      "episode_num 1360, curr_reward: 8.0, best_reward: 14.0, running_avg_reward: -7.61, curr_epsilon: 0.005\n",
      "episode_num 1361, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -7.63, curr_epsilon: 0.005\n",
      "episode_num 1362, curr_reward: -5.0, best_reward: 14.0, running_avg_reward: -7.66, curr_epsilon: 0.005\n",
      "episode_num 1363, curr_reward: -3.0, best_reward: 14.0, running_avg_reward: -7.72, curr_epsilon: 0.005\n",
      "episode_num 1364, curr_reward: -8.0, best_reward: 14.0, running_avg_reward: -7.78, curr_epsilon: 0.005\n",
      "episode_num 1365, curr_reward: 16.0, best_reward: 16.0, running_avg_reward: -7.48, curr_epsilon: 0.005\n",
      "episode_num 1366, curr_reward: -3.0, best_reward: 16.0, running_avg_reward: -7.4, curr_epsilon: 0.005\n",
      "episode_num 1367, curr_reward: -3.0, best_reward: 16.0, running_avg_reward: -7.4, curr_epsilon: 0.005\n",
      "episode_num 1368, curr_reward: -1.0, best_reward: 16.0, running_avg_reward: -7.35, curr_epsilon: 0.005\n",
      "episode_num 1369, curr_reward: -3.0, best_reward: 16.0, running_avg_reward: -7.32, curr_epsilon: 0.005\n",
      "episode_num 1370, curr_reward: 2.0, best_reward: 16.0, running_avg_reward: -7.2, curr_epsilon: 0.005\n",
      "episode_num 1371, curr_reward: -9.0, best_reward: 16.0, running_avg_reward: -7.15, curr_epsilon: 0.005\n",
      "episode_num 1372, curr_reward: -4.0, best_reward: 16.0, running_avg_reward: -7.16, curr_epsilon: 0.005\n",
      "episode_num 1373, curr_reward: -14.0, best_reward: 16.0, running_avg_reward: -7.24, curr_epsilon: 0.005\n",
      "episode_num 1374, curr_reward: 5.0, best_reward: 16.0, running_avg_reward: -7.16, curr_epsilon: 0.005\n",
      "episode_num 1375, curr_reward: 1.0, best_reward: 16.0, running_avg_reward: -7.14, curr_epsilon: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 1376, curr_reward: -8.0, best_reward: 16.0, running_avg_reward: -7.21, curr_epsilon: 0.005\n",
      "episode_num 1377, curr_reward: -10.0, best_reward: 16.0, running_avg_reward: -7.24, curr_epsilon: 0.005\n",
      "episode_num 1378, curr_reward: -6.0, best_reward: 16.0, running_avg_reward: -7.27, curr_epsilon: 0.005\n",
      "episode_num 1379, curr_reward: -8.0, best_reward: 16.0, running_avg_reward: -7.28, curr_epsilon: 0.005\n",
      "episode_num 1380, curr_reward: -10.0, best_reward: 16.0, running_avg_reward: -7.23, curr_epsilon: 0.005\n",
      "episode_num 1381, curr_reward: 2.0, best_reward: 16.0, running_avg_reward: -7.22, curr_epsilon: 0.005\n",
      "episode_num 1382, curr_reward: -14.0, best_reward: 16.0, running_avg_reward: -7.27, curr_epsilon: 0.005\n",
      "episode_num 1383, curr_reward: -4.0, best_reward: 16.0, running_avg_reward: -7.3, curr_epsilon: 0.005\n",
      "episode_num 1384, curr_reward: -5.0, best_reward: 16.0, running_avg_reward: -7.34, curr_epsilon: 0.005\n",
      "episode_num 1385, curr_reward: -13.0, best_reward: 16.0, running_avg_reward: -7.37, curr_epsilon: 0.005\n",
      "episode_num 1386, curr_reward: -7.0, best_reward: 16.0, running_avg_reward: -7.38, curr_epsilon: 0.005\n",
      "episode_num 1387, curr_reward: -10.0, best_reward: 16.0, running_avg_reward: -7.43, curr_epsilon: 0.005\n",
      "episode_num 1388, curr_reward: -14.0, best_reward: 16.0, running_avg_reward: -7.45, curr_epsilon: 0.005\n",
      "episode_num 1389, curr_reward: -13.0, best_reward: 16.0, running_avg_reward: -7.48, curr_epsilon: 0.005\n",
      "episode_num 1390, curr_reward: -16.0, best_reward: 16.0, running_avg_reward: -7.57, curr_epsilon: 0.005\n",
      "episode_num 1391, curr_reward: -12.0, best_reward: 16.0, running_avg_reward: -7.57, curr_epsilon: 0.005\n",
      "episode_num 1392, curr_reward: -9.0, best_reward: 16.0, running_avg_reward: -7.59, curr_epsilon: 0.005\n",
      "episode_num 1393, curr_reward: -3.0, best_reward: 16.0, running_avg_reward: -7.52, curr_epsilon: 0.005\n",
      "episode_num 1394, curr_reward: -12.0, best_reward: 16.0, running_avg_reward: -7.56, curr_epsilon: 0.005\n",
      "episode_num 1395, curr_reward: -10.0, best_reward: 16.0, running_avg_reward: -7.57, curr_epsilon: 0.005\n",
      "episode_num 1396, curr_reward: -8.0, best_reward: 16.0, running_avg_reward: -7.57, curr_epsilon: 0.005\n",
      "episode_num 1397, curr_reward: -2.0, best_reward: 16.0, running_avg_reward: -7.41, curr_epsilon: 0.005\n",
      "episode_num 1398, curr_reward: -1.0, best_reward: 16.0, running_avg_reward: -7.37, curr_epsilon: 0.005\n",
      "episode_num 1399, curr_reward: 3.0, best_reward: 16.0, running_avg_reward: -7.25, curr_epsilon: 0.005\n",
      "episode_num 1400, curr_reward: -2.0, best_reward: 16.0, running_avg_reward: -7.18, curr_epsilon: 0.005\n",
      "episode_num 1401, curr_reward: -10.0, best_reward: 16.0, running_avg_reward: -7.14, curr_epsilon: 0.005\n",
      "episode_num 1402, curr_reward: -7.0, best_reward: 16.0, running_avg_reward: -7.13, curr_epsilon: 0.005\n",
      "episode_num 1403, curr_reward: -7.0, best_reward: 16.0, running_avg_reward: -7.16, curr_epsilon: 0.005\n",
      "episode_num 1404, curr_reward: -7.0, best_reward: 16.0, running_avg_reward: -7.11, curr_epsilon: 0.005\n",
      "episode_num 1405, curr_reward: -7.0, best_reward: 16.0, running_avg_reward: -7.08, curr_epsilon: 0.005\n",
      "episode_num 1406, curr_reward: -6.0, best_reward: 16.0, running_avg_reward: -7.04, curr_epsilon: 0.005\n",
      "episode_num 1407, curr_reward: -7.0, best_reward: 16.0, running_avg_reward: -7.03, curr_epsilon: 0.005\n",
      "episode_num 1408, curr_reward: -7.0, best_reward: 16.0, running_avg_reward: -6.96, curr_epsilon: 0.005\n",
      "episode_num 1409, curr_reward: -1.0, best_reward: 16.0, running_avg_reward: -6.85, curr_epsilon: 0.005\n",
      "episode_num 1410, curr_reward: -12.0, best_reward: 16.0, running_avg_reward: -6.88, curr_epsilon: 0.005\n",
      "episode_num 1411, curr_reward: -2.0, best_reward: 16.0, running_avg_reward: -6.79, curr_epsilon: 0.005\n",
      "episode_num 1412, curr_reward: 8.0, best_reward: 16.0, running_avg_reward: -6.62, curr_epsilon: 0.005\n",
      "episode_num 1413, curr_reward: -9.0, best_reward: 16.0, running_avg_reward: -6.59, curr_epsilon: 0.005\n",
      "episode_num 1414, curr_reward: 2.0, best_reward: 16.0, running_avg_reward: -6.5, curr_epsilon: 0.005\n",
      "episode_num 1415, curr_reward: -9.0, best_reward: 16.0, running_avg_reward: -6.52, curr_epsilon: 0.005\n",
      "episode_num 1416, curr_reward: -16.0, best_reward: 16.0, running_avg_reward: -6.67, curr_epsilon: 0.005\n",
      "episode_num 1417, curr_reward: -10.0, best_reward: 16.0, running_avg_reward: -6.58, curr_epsilon: 0.005\n",
      "episode_num 1418, curr_reward: -4.0, best_reward: 16.0, running_avg_reward: -6.56, curr_epsilon: 0.005\n",
      "episode_num 1419, curr_reward: 5.0, best_reward: 16.0, running_avg_reward: -6.43, curr_epsilon: 0.005\n",
      "episode_num 1420, curr_reward: -4.0, best_reward: 16.0, running_avg_reward: -6.46, curr_epsilon: 0.005\n",
      "episode_num 1421, curr_reward: 6.0, best_reward: 16.0, running_avg_reward: -6.31, curr_epsilon: 0.005\n",
      "episode_num 1422, curr_reward: 1.0, best_reward: 16.0, running_avg_reward: -6.26, curr_epsilon: 0.005\n",
      "episode_num 1423, curr_reward: -1.0, best_reward: 16.0, running_avg_reward: -6.13, curr_epsilon: 0.005\n",
      "episode_num 1424, curr_reward: -4.0, best_reward: 16.0, running_avg_reward: -6.02, curr_epsilon: 0.005\n",
      "episode_num 1425, curr_reward: -11.0, best_reward: 16.0, running_avg_reward: -6.07, curr_epsilon: 0.005\n",
      "episode_num 1426, curr_reward: -10.0, best_reward: 16.0, running_avg_reward: -6.05, curr_epsilon: 0.005\n",
      "episode_num 1427, curr_reward: -6.0, best_reward: 16.0, running_avg_reward: -6.14, curr_epsilon: 0.005\n",
      "episode_num 1428, curr_reward: 5.0, best_reward: 16.0, running_avg_reward: -5.95, curr_epsilon: 0.005\n",
      "episode_num 1429, curr_reward: 1.0, best_reward: 16.0, running_avg_reward: -5.88, curr_epsilon: 0.005\n",
      "episode_num 1430, curr_reward: -9.0, best_reward: 16.0, running_avg_reward: -5.85, curr_epsilon: 0.005\n",
      "episode_num 1431, curr_reward: -3.0, best_reward: 16.0, running_avg_reward: -5.93, curr_epsilon: 0.005\n",
      "episode_num 1432, curr_reward: -5.0, best_reward: 16.0, running_avg_reward: -5.95, curr_epsilon: 0.005\n",
      "episode_num 1433, curr_reward: -4.0, best_reward: 16.0, running_avg_reward: -5.87, curr_epsilon: 0.005\n",
      "episode_num 1434, curr_reward: 11.0, best_reward: 16.0, running_avg_reward: -5.68, curr_epsilon: 0.005\n",
      "episode_num 1435, curr_reward: -3.0, best_reward: 16.0, running_avg_reward: -5.73, curr_epsilon: 0.005\n",
      "episode_num 1436, curr_reward: -12.0, best_reward: 16.0, running_avg_reward: -5.84, curr_epsilon: 0.005\n",
      "episode_num 1437, curr_reward: 4.0, best_reward: 16.0, running_avg_reward: -5.73, curr_epsilon: 0.005\n",
      "episode_num 1438, curr_reward: -4.0, best_reward: 16.0, running_avg_reward: -5.7, curr_epsilon: 0.005\n",
      "episode_num 1439, curr_reward: -1.0, best_reward: 16.0, running_avg_reward: -5.61, curr_epsilon: 0.005\n",
      "episode_num 1440, curr_reward: 5.0, best_reward: 16.0, running_avg_reward: -5.43, curr_epsilon: 0.005\n",
      "episode_num 1441, curr_reward: -1.0, best_reward: 16.0, running_avg_reward: -5.38, curr_epsilon: 0.005\n",
      "episode_num 1442, curr_reward: -4.0, best_reward: 16.0, running_avg_reward: -5.36, curr_epsilon: 0.005\n",
      "episode_num 1443, curr_reward: 4.0, best_reward: 16.0, running_avg_reward: -5.16, curr_epsilon: 0.005\n",
      "episode_num 1444, curr_reward: 1.0, best_reward: 16.0, running_avg_reward: -5.06, curr_epsilon: 0.005\n",
      "episode_num 1445, curr_reward: -8.0, best_reward: 16.0, running_avg_reward: -5.0, curr_epsilon: 0.005\n",
      "episode_num 1446, curr_reward: -4.0, best_reward: 16.0, running_avg_reward: -4.95, curr_epsilon: 0.005\n",
      "episode_num 1447, curr_reward: 1.0, best_reward: 16.0, running_avg_reward: -4.85, curr_epsilon: 0.005\n",
      "episode_num 1448, curr_reward: -2.0, best_reward: 16.0, running_avg_reward: -4.81, curr_epsilon: 0.005\n",
      "episode_num 1449, curr_reward: -8.0, best_reward: 16.0, running_avg_reward: -4.86, curr_epsilon: 0.005\n",
      "episode_num 1450, curr_reward: -6.0, best_reward: 16.0, running_avg_reward: -4.87, curr_epsilon: 0.005\n",
      "episode_num 1451, curr_reward: -5.0, best_reward: 16.0, running_avg_reward: -4.78, curr_epsilon: 0.005\n",
      "episode_num 1452, curr_reward: -5.0, best_reward: 16.0, running_avg_reward: -4.69, curr_epsilon: 0.005\n",
      "episode_num 1453, curr_reward: 1.0, best_reward: 16.0, running_avg_reward: -4.56, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -4.47 achieved!\n",
      "episode_num 1454, curr_reward: -1.0, best_reward: 16.0, running_avg_reward: -4.47, curr_epsilon: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointing current model weights. highest running_average_reward of -4.38 achieved!\n",
      "episode_num 1455, curr_reward: 1.0, best_reward: 16.0, running_avg_reward: -4.38, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -4.15 achieved!\n",
      "episode_num 1456, curr_reward: 13.0, best_reward: 16.0, running_avg_reward: -4.15, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -4.03 achieved!\n",
      "episode_num 1457, curr_reward: -3.0, best_reward: 16.0, running_avg_reward: -4.03, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -3.95 achieved!\n",
      "episode_num 1458, curr_reward: 3.0, best_reward: 16.0, running_avg_reward: -3.95, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -3.86 achieved!\n",
      "episode_num 1459, curr_reward: 3.0, best_reward: 16.0, running_avg_reward: -3.86, curr_epsilon: 0.005\n",
      "episode_num 1460, curr_reward: 5.0, best_reward: 16.0, running_avg_reward: -3.89, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -3.77 achieved!\n",
      "episode_num 1461, curr_reward: 9.0, best_reward: 16.0, running_avg_reward: -3.77, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -3.73 achieved!\n",
      "episode_num 1462, curr_reward: -1.0, best_reward: 16.0, running_avg_reward: -3.73, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -3.64 achieved!\n",
      "episode_num 1463, curr_reward: 6.0, best_reward: 16.0, running_avg_reward: -3.64, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -3.4 achieved!\n",
      "episode_num 1464, curr_reward: 16.0, best_reward: 16.0, running_avg_reward: -3.4, curr_epsilon: 0.005\n",
      "episode_num 1465, curr_reward: -1.0, best_reward: 16.0, running_avg_reward: -3.57, curr_epsilon: 0.005\n",
      "episode_num 1466, curr_reward: 9.0, best_reward: 16.0, running_avg_reward: -3.45, curr_epsilon: 0.005\n",
      "episode_num 1467, curr_reward: -6.0, best_reward: 16.0, running_avg_reward: -3.48, curr_epsilon: 0.005\n",
      "episode_num 1468, curr_reward: -7.0, best_reward: 16.0, running_avg_reward: -3.54, curr_epsilon: 0.005\n",
      "episode_num 1469, curr_reward: -5.0, best_reward: 16.0, running_avg_reward: -3.56, curr_epsilon: 0.005\n",
      "episode_num 1470, curr_reward: 10.0, best_reward: 16.0, running_avg_reward: -3.48, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -3.26 achieved!\n",
      "episode_num 1471, curr_reward: 13.0, best_reward: 16.0, running_avg_reward: -3.26, curr_epsilon: 0.005\n",
      "episode_num 1472, curr_reward: -4.0, best_reward: 16.0, running_avg_reward: -3.26, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -3.05 achieved!\n",
      "episode_num 1473, curr_reward: 7.0, best_reward: 16.0, running_avg_reward: -3.05, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -3.02 achieved!\n",
      "episode_num 1474, curr_reward: 8.0, best_reward: 16.0, running_avg_reward: -3.02, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -3.0 achieved!\n",
      "episode_num 1475, curr_reward: 3.0, best_reward: 16.0, running_avg_reward: -3.0, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -2.89 achieved!\n",
      "episode_num 1476, curr_reward: 3.0, best_reward: 16.0, running_avg_reward: -2.89, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -2.7 achieved!\n",
      "episode_num 1477, curr_reward: 9.0, best_reward: 16.0, running_avg_reward: -2.7, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -2.69 achieved!\n",
      "episode_num 1478, curr_reward: -5.0, best_reward: 16.0, running_avg_reward: -2.69, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -2.65 achieved!\n",
      "episode_num 1479, curr_reward: -4.0, best_reward: 16.0, running_avg_reward: -2.65, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -2.52 achieved!\n",
      "episode_num 1480, curr_reward: 3.0, best_reward: 16.0, running_avg_reward: -2.52, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -2.42 achieved!\n",
      "episode_num 1481, curr_reward: 12.0, best_reward: 16.0, running_avg_reward: -2.42, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -2.29 achieved!\n",
      "episode_num 1482, curr_reward: -1.0, best_reward: 16.0, running_avg_reward: -2.29, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -2.23 achieved!\n",
      "episode_num 1483, curr_reward: 2.0, best_reward: 16.0, running_avg_reward: -2.23, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -2.07 achieved!\n",
      "episode_num 1484, curr_reward: 11.0, best_reward: 16.0, running_avg_reward: -2.07, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -1.86 achieved!\n",
      "episode_num 1485, curr_reward: 8.0, best_reward: 16.0, running_avg_reward: -1.86, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -1.77 achieved!\n",
      "episode_num 1486, curr_reward: 2.0, best_reward: 16.0, running_avg_reward: -1.77, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -1.64 achieved!\n",
      "episode_num 1487, curr_reward: 3.0, best_reward: 16.0, running_avg_reward: -1.64, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -1.47 achieved!\n",
      "episode_num 1488, curr_reward: 3.0, best_reward: 16.0, running_avg_reward: -1.47, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -1.33 achieved!\n",
      "episode_num 1489, curr_reward: 1.0, best_reward: 16.0, running_avg_reward: -1.33, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -1.23 achieved!\n",
      "episode_num 1490, curr_reward: -6.0, best_reward: 16.0, running_avg_reward: -1.23, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -1.13 achieved!\n",
      "episode_num 1491, curr_reward: -2.0, best_reward: 16.0, running_avg_reward: -1.13, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -1.01 achieved!\n",
      "episode_num 1492, curr_reward: 3.0, best_reward: 16.0, running_avg_reward: -1.01, curr_epsilon: 0.005\n",
      "episode_num 1493, curr_reward: -4.0, best_reward: 16.0, running_avg_reward: -1.02, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -0.91 achieved!\n",
      "episode_num 1494, curr_reward: -1.0, best_reward: 16.0, running_avg_reward: -0.91, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -0.82 achieved!\n",
      "episode_num 1495, curr_reward: -1.0, best_reward: 16.0, running_avg_reward: -0.82, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -0.57 achieved!\n",
      "episode_num 1496, curr_reward: 17.0, best_reward: 17.0, running_avg_reward: -0.57, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -0.48 achieved!\n",
      "episode_num 1497, curr_reward: 7.0, best_reward: 17.0, running_avg_reward: -0.48, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -0.43 achieved!\n",
      "episode_num 1498, curr_reward: 4.0, best_reward: 17.0, running_avg_reward: -0.43, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -0.35 achieved!\n",
      "episode_num 1499, curr_reward: 11.0, best_reward: 17.0, running_avg_reward: -0.35, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -0.22 achieved!\n",
      "episode_num 1500, curr_reward: 11.0, best_reward: 17.0, running_avg_reward: -0.22, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of -0.05 achieved!\n",
      "episode_num 1501, curr_reward: 7.0, best_reward: 17.0, running_avg_reward: -0.05, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.01 achieved!\n",
      "episode_num 1502, curr_reward: -1.0, best_reward: 17.0, running_avg_reward: 0.01, curr_epsilon: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointing current model weights. highest running_average_reward of 0.11 achieved!\n",
      "episode_num 1503, curr_reward: 3.0, best_reward: 17.0, running_avg_reward: 0.11, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.2 achieved!\n",
      "episode_num 1504, curr_reward: 2.0, best_reward: 17.0, running_avg_reward: 0.2, curr_epsilon: 0.005\n",
      "episode_num 1505, curr_reward: -8.0, best_reward: 17.0, running_avg_reward: 0.19, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.3 achieved!\n",
      "episode_num 1506, curr_reward: 5.0, best_reward: 17.0, running_avg_reward: 0.3, curr_epsilon: 0.005\n",
      "episode_num 1507, curr_reward: -9.0, best_reward: 17.0, running_avg_reward: 0.28, curr_epsilon: 0.005\n",
      "episode_num 1508, curr_reward: -11.0, best_reward: 17.0, running_avg_reward: 0.24, curr_epsilon: 0.005\n",
      "episode_num 1509, curr_reward: 2.0, best_reward: 17.0, running_avg_reward: 0.27, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.31 achieved!\n",
      "episode_num 1510, curr_reward: -8.0, best_reward: 17.0, running_avg_reward: 0.31, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.34 achieved!\n",
      "episode_num 1511, curr_reward: 1.0, best_reward: 17.0, running_avg_reward: 0.34, curr_epsilon: 0.005\n",
      "episode_num 1512, curr_reward: -6.0, best_reward: 17.0, running_avg_reward: 0.2, curr_epsilon: 0.005\n",
      "episode_num 1513, curr_reward: -9.0, best_reward: 17.0, running_avg_reward: 0.2, curr_epsilon: 0.005\n",
      "episode_num 1514, curr_reward: -1.0, best_reward: 17.0, running_avg_reward: 0.17, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.37 achieved!\n",
      "episode_num 1515, curr_reward: 11.0, best_reward: 17.0, running_avg_reward: 0.37, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.52 achieved!\n",
      "episode_num 1516, curr_reward: -1.0, best_reward: 17.0, running_avg_reward: 0.52, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.58 achieved!\n",
      "episode_num 1517, curr_reward: -4.0, best_reward: 17.0, running_avg_reward: 0.58, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.61 achieved!\n",
      "episode_num 1518, curr_reward: -1.0, best_reward: 17.0, running_avg_reward: 0.61, curr_epsilon: 0.005\n",
      "episode_num 1519, curr_reward: -7.0, best_reward: 17.0, running_avg_reward: 0.49, curr_epsilon: 0.005\n",
      "episode_num 1520, curr_reward: -8.0, best_reward: 17.0, running_avg_reward: 0.45, curr_epsilon: 0.005\n",
      "episode_num 1521, curr_reward: 8.0, best_reward: 17.0, running_avg_reward: 0.47, curr_epsilon: 0.005\n",
      "episode_num 1522, curr_reward: 11.0, best_reward: 17.0, running_avg_reward: 0.57, curr_epsilon: 0.005\n",
      "episode_num 1523, curr_reward: -8.0, best_reward: 17.0, running_avg_reward: 0.5, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.64 achieved!\n",
      "episode_num 1524, curr_reward: 10.0, best_reward: 17.0, running_avg_reward: 0.64, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.67 achieved!\n",
      "episode_num 1525, curr_reward: -8.0, best_reward: 17.0, running_avg_reward: 0.67, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.7 achieved!\n",
      "episode_num 1526, curr_reward: -7.0, best_reward: 17.0, running_avg_reward: 0.7, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.82 achieved!\n",
      "episode_num 1527, curr_reward: 6.0, best_reward: 17.0, running_avg_reward: 0.82, curr_epsilon: 0.005\n",
      "episode_num 1528, curr_reward: -1.0, best_reward: 17.0, running_avg_reward: 0.76, curr_epsilon: 0.005\n",
      "episode_num 1529, curr_reward: -3.0, best_reward: 17.0, running_avg_reward: 0.72, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 0.91 achieved!\n",
      "episode_num 1530, curr_reward: 10.0, best_reward: 17.0, running_avg_reward: 0.91, curr_epsilon: 0.005\n",
      "episode_num 1531, curr_reward: -4.0, best_reward: 17.0, running_avg_reward: 0.9, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.04 achieved!\n",
      "episode_num 1532, curr_reward: 9.0, best_reward: 17.0, running_avg_reward: 1.04, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.1 achieved!\n",
      "episode_num 1533, curr_reward: 2.0, best_reward: 17.0, running_avg_reward: 1.1, curr_epsilon: 0.005\n",
      "episode_num 1534, curr_reward: -7.0, best_reward: 17.0, running_avg_reward: 0.92, curr_epsilon: 0.005\n",
      "episode_num 1535, curr_reward: -9.0, best_reward: 17.0, running_avg_reward: 0.86, curr_epsilon: 0.005\n",
      "episode_num 1536, curr_reward: -8.0, best_reward: 17.0, running_avg_reward: 0.9, curr_epsilon: 0.005\n",
      "episode_num 1537, curr_reward: -2.0, best_reward: 17.0, running_avg_reward: 0.84, curr_epsilon: 0.005\n",
      "episode_num 1538, curr_reward: 2.0, best_reward: 17.0, running_avg_reward: 0.9, curr_epsilon: 0.005\n",
      "episode_num 1539, curr_reward: 8.0, best_reward: 17.0, running_avg_reward: 0.99, curr_epsilon: 0.005\n",
      "episode_num 1540, curr_reward: -2.0, best_reward: 17.0, running_avg_reward: 0.92, curr_epsilon: 0.005\n",
      "episode_num 1541, curr_reward: 14.0, best_reward: 17.0, running_avg_reward: 1.07, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.13 achieved!\n",
      "episode_num 1542, curr_reward: 2.0, best_reward: 17.0, running_avg_reward: 1.13, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.22 achieved!\n",
      "episode_num 1543, curr_reward: 13.0, best_reward: 17.0, running_avg_reward: 1.22, curr_epsilon: 0.005\n",
      "episode_num 1544, curr_reward: -18.0, best_reward: 17.0, running_avg_reward: 1.03, curr_epsilon: 0.005\n",
      "episode_num 1545, curr_reward: 9.0, best_reward: 17.0, running_avg_reward: 1.2, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.31 achieved!\n",
      "episode_num 1546, curr_reward: 7.0, best_reward: 17.0, running_avg_reward: 1.31, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.35 achieved!\n",
      "episode_num 1547, curr_reward: 5.0, best_reward: 17.0, running_avg_reward: 1.35, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.46 achieved!\n",
      "episode_num 1548, curr_reward: 9.0, best_reward: 17.0, running_avg_reward: 1.46, curr_epsilon: 0.005\n",
      "episode_num 1549, curr_reward: -9.0, best_reward: 17.0, running_avg_reward: 1.45, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.49 achieved!\n",
      "episode_num 1550, curr_reward: -2.0, best_reward: 17.0, running_avg_reward: 1.49, curr_epsilon: 0.005\n",
      "episode_num 1551, curr_reward: -9.0, best_reward: 17.0, running_avg_reward: 1.45, curr_epsilon: 0.005\n",
      "episode_num 1552, curr_reward: -2.0, best_reward: 17.0, running_avg_reward: 1.48, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.61 achieved!\n",
      "episode_num 1553, curr_reward: 14.0, best_reward: 17.0, running_avg_reward: 1.61, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.65 achieved!\n",
      "episode_num 1554, curr_reward: 3.0, best_reward: 17.0, running_avg_reward: 1.65, curr_epsilon: 0.005\n",
      "episode_num 1555, curr_reward: -2.0, best_reward: 17.0, running_avg_reward: 1.62, curr_epsilon: 0.005\n",
      "episode_num 1556, curr_reward: 5.0, best_reward: 17.0, running_avg_reward: 1.54, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.75 achieved!\n",
      "episode_num 1557, curr_reward: 18.0, best_reward: 18.0, running_avg_reward: 1.75, curr_epsilon: 0.005\n",
      "episode_num 1558, curr_reward: -4.0, best_reward: 18.0, running_avg_reward: 1.68, curr_epsilon: 0.005\n",
      "episode_num 1559, curr_reward: 1.0, best_reward: 18.0, running_avg_reward: 1.66, curr_epsilon: 0.005\n",
      "episode_num 1560, curr_reward: 2.0, best_reward: 18.0, running_avg_reward: 1.63, curr_epsilon: 0.005\n",
      "episode_num 1561, curr_reward: 5.0, best_reward: 18.0, running_avg_reward: 1.59, curr_epsilon: 0.005\n",
      "episode_num 1562, curr_reward: 14.0, best_reward: 18.0, running_avg_reward: 1.74, curr_epsilon: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num 1563, curr_reward: -3.0, best_reward: 18.0, running_avg_reward: 1.65, curr_epsilon: 0.005\n",
      "episode_num 1564, curr_reward: -3.0, best_reward: 18.0, running_avg_reward: 1.46, curr_epsilon: 0.005\n",
      "episode_num 1565, curr_reward: -3.0, best_reward: 18.0, running_avg_reward: 1.44, curr_epsilon: 0.005\n",
      "episode_num 1566, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 1.33, curr_epsilon: 0.005\n",
      "episode_num 1567, curr_reward: 4.0, best_reward: 18.0, running_avg_reward: 1.43, curr_epsilon: 0.005\n",
      "episode_num 1568, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 1.49, curr_epsilon: 0.005\n",
      "episode_num 1569, curr_reward: -11.0, best_reward: 18.0, running_avg_reward: 1.43, curr_epsilon: 0.005\n",
      "episode_num 1570, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 1.32, curr_epsilon: 0.005\n",
      "episode_num 1571, curr_reward: 3.0, best_reward: 18.0, running_avg_reward: 1.22, curr_epsilon: 0.005\n",
      "episode_num 1572, curr_reward: 11.0, best_reward: 18.0, running_avg_reward: 1.37, curr_epsilon: 0.005\n",
      "episode_num 1573, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 1.28, curr_epsilon: 0.005\n",
      "episode_num 1574, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 1.18, curr_epsilon: 0.005\n",
      "episode_num 1575, curr_reward: 13.0, best_reward: 18.0, running_avg_reward: 1.28, curr_epsilon: 0.005\n",
      "episode_num 1576, curr_reward: 1.0, best_reward: 18.0, running_avg_reward: 1.26, curr_epsilon: 0.005\n",
      "episode_num 1577, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 1.15, curr_epsilon: 0.005\n",
      "episode_num 1578, curr_reward: 8.0, best_reward: 18.0, running_avg_reward: 1.28, curr_epsilon: 0.005\n",
      "episode_num 1579, curr_reward: 18.0, best_reward: 18.0, running_avg_reward: 1.5, curr_epsilon: 0.005\n",
      "episode_num 1580, curr_reward: -3.0, best_reward: 18.0, running_avg_reward: 1.44, curr_epsilon: 0.005\n",
      "episode_num 1581, curr_reward: 3.0, best_reward: 18.0, running_avg_reward: 1.35, curr_epsilon: 0.005\n",
      "episode_num 1582, curr_reward: 7.0, best_reward: 18.0, running_avg_reward: 1.43, curr_epsilon: 0.005\n",
      "episode_num 1583, curr_reward: 11.0, best_reward: 18.0, running_avg_reward: 1.52, curr_epsilon: 0.005\n",
      "episode_num 1584, curr_reward: 3.0, best_reward: 18.0, running_avg_reward: 1.44, curr_epsilon: 0.005\n",
      "episode_num 1585, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 1.34, curr_epsilon: 0.005\n",
      "episode_num 1586, curr_reward: -9.0, best_reward: 18.0, running_avg_reward: 1.23, curr_epsilon: 0.005\n",
      "episode_num 1587, curr_reward: 6.0, best_reward: 18.0, running_avg_reward: 1.26, curr_epsilon: 0.005\n",
      "episode_num 1588, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 1.21, curr_epsilon: 0.005\n",
      "episode_num 1589, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 1.19, curr_epsilon: 0.005\n",
      "episode_num 1590, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 1.24, curr_epsilon: 0.005\n",
      "episode_num 1591, curr_reward: 1.0, best_reward: 18.0, running_avg_reward: 1.27, curr_epsilon: 0.005\n",
      "episode_num 1592, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 1.22, curr_epsilon: 0.005\n",
      "episode_num 1593, curr_reward: -4.0, best_reward: 18.0, running_avg_reward: 1.22, curr_epsilon: 0.005\n",
      "episode_num 1594, curr_reward: -8.0, best_reward: 18.0, running_avg_reward: 1.15, curr_epsilon: 0.005\n",
      "episode_num 1595, curr_reward: -9.0, best_reward: 18.0, running_avg_reward: 1.07, curr_epsilon: 0.005\n",
      "episode_num 1596, curr_reward: 2.0, best_reward: 18.0, running_avg_reward: 0.92, curr_epsilon: 0.005\n",
      "episode_num 1597, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 0.84, curr_epsilon: 0.005\n",
      "episode_num 1598, curr_reward: 12.0, best_reward: 18.0, running_avg_reward: 0.92, curr_epsilon: 0.005\n",
      "episode_num 1599, curr_reward: 5.0, best_reward: 18.0, running_avg_reward: 0.86, curr_epsilon: 0.005\n",
      "episode_num 1600, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 0.73, curr_epsilon: 0.005\n",
      "episode_num 1601, curr_reward: 7.0, best_reward: 18.0, running_avg_reward: 0.73, curr_epsilon: 0.005\n",
      "episode_num 1602, curr_reward: 12.0, best_reward: 18.0, running_avg_reward: 0.86, curr_epsilon: 0.005\n",
      "episode_num 1603, curr_reward: -9.0, best_reward: 18.0, running_avg_reward: 0.74, curr_epsilon: 0.005\n",
      "episode_num 1604, curr_reward: 9.0, best_reward: 18.0, running_avg_reward: 0.81, curr_epsilon: 0.005\n",
      "episode_num 1605, curr_reward: -6.0, best_reward: 18.0, running_avg_reward: 0.83, curr_epsilon: 0.005\n",
      "episode_num 1606, curr_reward: 11.0, best_reward: 18.0, running_avg_reward: 0.89, curr_epsilon: 0.005\n",
      "episode_num 1607, curr_reward: 15.0, best_reward: 18.0, running_avg_reward: 1.13, curr_epsilon: 0.005\n",
      "episode_num 1608, curr_reward: 7.0, best_reward: 18.0, running_avg_reward: 1.31, curr_epsilon: 0.005\n",
      "episode_num 1609, curr_reward: 2.0, best_reward: 18.0, running_avg_reward: 1.31, curr_epsilon: 0.005\n",
      "episode_num 1610, curr_reward: -6.0, best_reward: 18.0, running_avg_reward: 1.33, curr_epsilon: 0.005\n",
      "episode_num 1611, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 1.31, curr_epsilon: 0.005\n",
      "episode_num 1612, curr_reward: 15.0, best_reward: 18.0, running_avg_reward: 1.52, curr_epsilon: 0.005\n",
      "episode_num 1613, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 1.6, curr_epsilon: 0.005\n",
      "episode_num 1614, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 1.6, curr_epsilon: 0.005\n",
      "episode_num 1615, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 1.48, curr_epsilon: 0.005\n",
      "episode_num 1616, curr_reward: 14.0, best_reward: 18.0, running_avg_reward: 1.63, curr_epsilon: 0.005\n",
      "episode_num 1617, curr_reward: 2.0, best_reward: 18.0, running_avg_reward: 1.69, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.86 achieved!\n",
      "episode_num 1618, curr_reward: 16.0, best_reward: 18.0, running_avg_reward: 1.86, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 1.96 achieved!\n",
      "episode_num 1619, curr_reward: 3.0, best_reward: 18.0, running_avg_reward: 1.96, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 2.13 achieved!\n",
      "episode_num 1620, curr_reward: 9.0, best_reward: 18.0, running_avg_reward: 2.13, curr_epsilon: 0.005\n",
      "episode_num 1621, curr_reward: 4.0, best_reward: 18.0, running_avg_reward: 2.09, curr_epsilon: 0.005\n",
      "episode_num 1622, curr_reward: -12.0, best_reward: 18.0, running_avg_reward: 1.86, curr_epsilon: 0.005\n",
      "episode_num 1623, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 1.92, curr_epsilon: 0.005\n",
      "episode_num 1624, curr_reward: 3.0, best_reward: 18.0, running_avg_reward: 1.85, curr_epsilon: 0.005\n",
      "episode_num 1625, curr_reward: -4.0, best_reward: 18.0, running_avg_reward: 1.89, curr_epsilon: 0.005\n",
      "episode_num 1626, curr_reward: 5.0, best_reward: 18.0, running_avg_reward: 2.01, curr_epsilon: 0.005\n",
      "episode_num 1627, curr_reward: 4.0, best_reward: 18.0, running_avg_reward: 1.99, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 2.15 achieved!\n",
      "episode_num 1628, curr_reward: 15.0, best_reward: 18.0, running_avg_reward: 2.15, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 2.28 achieved!\n",
      "episode_num 1629, curr_reward: 10.0, best_reward: 18.0, running_avg_reward: 2.28, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 2.31 achieved!\n",
      "episode_num 1630, curr_reward: 13.0, best_reward: 18.0, running_avg_reward: 2.31, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 2.33 achieved!\n",
      "episode_num 1631, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 2.33, curr_epsilon: 0.005\n",
      "episode_num 1632, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 2.23, curr_epsilon: 0.005\n",
      "episode_num 1633, curr_reward: 11.0, best_reward: 18.0, running_avg_reward: 2.32, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 2.34 achieved!\n",
      "episode_num 1634, curr_reward: -5.0, best_reward: 18.0, running_avg_reward: 2.34, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 2.41 achieved!\n",
      "episode_num 1635, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 2.41, curr_epsilon: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointing current model weights. highest running_average_reward of 2.46 achieved!\n",
      "episode_num 1636, curr_reward: -3.0, best_reward: 18.0, running_avg_reward: 2.46, curr_epsilon: 0.005\n",
      "episode_num 1637, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 2.46, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 2.49 achieved!\n",
      "episode_num 1638, curr_reward: 5.0, best_reward: 18.0, running_avg_reward: 2.49, curr_epsilon: 0.005\n",
      "episode_num 1639, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 2.4, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 2.57 achieved!\n",
      "episode_num 1640, curr_reward: 15.0, best_reward: 18.0, running_avg_reward: 2.57, curr_epsilon: 0.005\n",
      "episode_num 1641, curr_reward: 4.0, best_reward: 18.0, running_avg_reward: 2.47, curr_epsilon: 0.005\n",
      "episode_num 1642, curr_reward: -2.0, best_reward: 18.0, running_avg_reward: 2.43, curr_epsilon: 0.005\n",
      "episode_num 1643, curr_reward: -4.0, best_reward: 18.0, running_avg_reward: 2.26, curr_epsilon: 0.005\n",
      "episode_num 1644, curr_reward: 3.0, best_reward: 18.0, running_avg_reward: 2.47, curr_epsilon: 0.005\n",
      "episode_num 1645, curr_reward: 11.0, best_reward: 18.0, running_avg_reward: 2.49, curr_epsilon: 0.005\n",
      "episode_num 1646, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 2.41, curr_epsilon: 0.005\n",
      "episode_num 1647, curr_reward: 1.0, best_reward: 18.0, running_avg_reward: 2.37, curr_epsilon: 0.005\n",
      "episode_num 1648, curr_reward: -1.0, best_reward: 18.0, running_avg_reward: 2.27, curr_epsilon: 0.005\n",
      "episode_num 1649, curr_reward: 14.0, best_reward: 18.0, running_avg_reward: 2.5, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 2.61 achieved!\n",
      "episode_num 1650, curr_reward: 9.0, best_reward: 18.0, running_avg_reward: 2.61, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 2.85 achieved!\n",
      "episode_num 1651, curr_reward: 15.0, best_reward: 18.0, running_avg_reward: 2.85, curr_epsilon: 0.005\n",
      "checkpointing current model weights. highest running_average_reward of 3.04 achieved!\n",
      "episode_num 1652, curr_reward: 17.0, best_reward: 18.0, running_avg_reward: 3.04, curr_epsilon: 0.005\n"
     ]
    }
   ],
   "source": [
    "env = wrap_env(ENV)\n",
    "dvc = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "mdl, tgt_mdl = models_init(env, dvc)\n",
    "opt = Adam(mdl.parameters(), lr=LR)\n",
    "rpl_bfr = RepBfr(MEM_CAP)\n",
    "train(env, mdl, tgt_mdl, opt, rpl_bfr, dvc)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
