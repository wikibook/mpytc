{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.333043\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.943062\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.304336\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 1.238040\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.958737\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.493184\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.643627\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.265545\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.560548\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.261553\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.306239\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.300254\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.261871\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.253512\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.103370\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.244615\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.246581\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.426752\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.164684\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.063364\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.220145\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.410414\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.323727\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.294908\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.091494\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.188713\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.144609\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.325974\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.031163\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.036093\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.214139\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.156931\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.096676\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.183215\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.033010\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.106560\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.167177\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.116418\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.347938\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.072275\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.069344\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.088382\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.096705\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.128923\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.127995\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.125990\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.148126\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.409140\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.133046\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.120051\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.219374\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.152537\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.350288\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.273461\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.261697\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.085179\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.033157\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.081473\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.004736\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.185923\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.293117\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.064243\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.044118\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.028630\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.117261\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.266663\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.032758\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.084564\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.135888\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.040037\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.019712\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.150382\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.067946\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.221487\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.148346\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.125404\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.116576\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.081610\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.040039\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.111751\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.027178\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.039238\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.046624\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.036259\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.042893\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.021309\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.078010\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.067319\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.019019\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.109955\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.075527\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.050420\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.058509\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.042245\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.067011\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.025555\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.170165\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.015962\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.320396\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.014895\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.273716\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.201027\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.324443\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.125436\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.077653\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.022727\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.153698\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.192000\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.106195\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.015246\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.056208\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.023621\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.072618\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.049412\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.078804\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.019641\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.073145\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.084906\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.165430\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.138081\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.255426\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.105837\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.195803\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.108841\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.005558\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.065162\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.048789\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.070413\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.078550\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.015537\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.130916\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.075333\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.092604\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.027642\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.071713\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.148932\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.044493\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.094794\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.040495\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.023940\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.014863\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.080790\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.016429\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.042312\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.122510\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.018108\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.042726\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.033287\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.004960\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.159315\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.163358\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.133813\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.067080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.189037\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.027550\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.030760\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.047113\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.201450\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.163309\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.230963\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.208547\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.193209\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.052333\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.071918\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.104872\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.100245\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.116145\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.052742\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.207156\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.055222\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.100362\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.146641\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.297926\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.023830\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.019826\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.210100\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.008069\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.099603\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.028852\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.015403\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.209963\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.084345\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.005287\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.226485\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.167074\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.045646\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.013449\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.024415\n",
      "\n",
      "Test dataset: Overall Loss: 0.0522, Overall Accuracy: 9825/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.283887\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.001043\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.002641\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.017910\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.035087\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.006630\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.309971\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.174572\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.071286\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.008842\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.043614\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.009304\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.094230\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.013201\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.004689\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.152283\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.042221\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.013336\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.012721\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.021102\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.438950\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.002385\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.035253\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.048008\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.002561\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.011579\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.026800\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.270779\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.001698\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.051953\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.145548\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.172020\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.022827\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.016456\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.051300\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.069796\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.190750\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.267316\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.303864\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.044847\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.153253\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.010190\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.125087\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.002830\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.001888\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.064268\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.002603\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.061247\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.217783\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.062138\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.012053\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.006884\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.067185\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.011688\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.017941\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.316944\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.055586\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.340394\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.086103\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.035126\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.018281\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.128952\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.050536\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.026000\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.035401\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.156032\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.109075\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.002974\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.098302\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.052790\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.048912\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.072185\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.032872\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.285448\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.028597\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.077726\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.055067\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.244763\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.054967\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.087433\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.023282\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.175667\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.201598\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.005322\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.013716\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.041442\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.008199\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.303657\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.029500\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.045069\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.212420\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.111298\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.010411\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.020577\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.155358\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.211448\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.039495\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.048508\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.016975\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.008171\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.063109\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.068897\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.048091\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.090008\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.106342\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.023091\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.007353\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.016746\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.015652\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.440961\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.054698\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.013922\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.102710\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.081401\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.003860\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.009652\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.006531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.122305\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.145930\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.016439\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.045070\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.018864\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.007952\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.016014\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.068150\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.050566\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.011531\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.005553\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.014355\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.008815\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.008510\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.002213\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.156662\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.224040\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.254498\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.144746\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.003408\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.033842\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.011321\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.172604\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.005733\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.002113\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.183646\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.025354\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.284551\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.008812\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.094086\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.010397\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.039547\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.110431\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.039305\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.199064\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.002353\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.004221\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.047369\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.177750\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.003290\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.225221\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.076244\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.008935\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.005751\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.012980\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.001507\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.006742\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.117215\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.022851\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.050420\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.056454\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.003290\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.026371\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.071585\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.070469\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.003723\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.003024\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.242654\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.125871\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.005245\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.000520\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.188250\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.044744\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.029123\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.002372\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.011676\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.032765\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.002476\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.003405\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.095505\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.004768\n",
      "\n",
      "Test dataset: Overall Loss: 0.0401, Overall Accuracy: 9871/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 2\n",
      "Ground truth is : 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = \"./convnet.pth\"\n",
    "torch.save(model.state_dict(), PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
