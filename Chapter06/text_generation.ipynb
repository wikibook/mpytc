{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout=0.3):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.model_name = 'transformer'\n",
    "        self.mask_source = None\n",
    "        self.position_enc = PosEnc(num_inputs, dropout)\n",
    "        layers_enc = TransformerEncoderLayer(num_inputs, num_heads, num_hidden, dropout)\n",
    "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers)\n",
    "        self.enc = nn.Embedding(num_token, num_inputs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dec = nn.Linear(num_inputs, num_token)\n",
    "        self.init_params()\n",
    "\n",
    "    def _gen_sqr_nxt_mask(self, size):\n",
    "        msk = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        msk = msk.float().masked_fill(msk == 0, float('-inf'))\n",
    "        msk = msk.masked_fill(msk == 1, float(0.0))\n",
    "        return msk\n",
    "\n",
    "    def init_params(self):\n",
    "        initial_rng = 0.12\n",
    "        self.enc.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "        self.dec.bias.data.zero_()\n",
    "        self.dec.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "\n",
    "    def forward(self, source):\n",
    "        if self.mask_source is None or self.mask_source.size(0) != len(source):\n",
    "            dvc = source.device\n",
    "            msk = self._gen_sqr_nxt_mask(len(source)).to(dvc)\n",
    "            self.mask_source = msk\n",
    "\n",
    "        source = self.enc(source) * math.sqrt(self.num_inputs)\n",
    "        source = self.position_enc(source)\n",
    "        op = self.enc_transformer(source, self.mask_source)\n",
    "        op = self.dec(op)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d_m, dropout=0.2, size_limit=5000):\n",
    "        super(PosEnc, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        p_enc = torch.zeros(size_limit, d_m)\n",
    "        pos = torch.arange(0, size_limit, dtype=torch.float).unsqueeze(1)\n",
    "        divider = torch.exp(torch.arange(0, d_m, 2).float() * (-math.log(10000.0) / d_m))\n",
    "        p_enc[:, 0::2] = torch.sin(pos * divider)\n",
    "        p_enc[:, 1::2] = torch.cos(pos * divider)\n",
    "        p_enc = p_enc.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('p_enc', p_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.p_enc[:x.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"), lower=True, eos_token='<eos>', init_token='<sos>')\n",
    "training_text, validation_text, testing_text = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(training_text)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gen_batches(text_dataset, batch_size):\n",
    "    text_dataset = TEXT.numericalize([text_dataset.examples[0].text])\n",
    "    # divide text dataset into parts of size equal to batch_size\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    # remove data points that lie outside batches (remainders)\n",
    "    text_dataset = text_dataset.narrow(0, 0, num_batches * batch_size)\n",
    "    # distribute dataset across batches evenly\n",
    "    text_dataset = text_dataset.view(batch_size, -1).t().contiguous()\n",
    "    return text_dataset.to(device)\n",
    "\n",
    "training_batch_size = 32\n",
    "evaluation_batch_size = 16\n",
    "\n",
    "training_data = gen_batches(training_text, training_batch_size)\n",
    "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
    "testing_data = gen_batches(testing_text, evaluation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 64\n",
    "def return_batch(src, k):\n",
    "    sequence_length = min(max_seq_len, len(src) - 1 - k)\n",
    "    sequence_data = src[k:k+sequence_length]\n",
    "    sequence_label = src[k+1:k+1+sequence_length].view(-1)\n",
    "    return sequence_data, sequence_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(TEXT.vocab.stoi) # vocabulary size\n",
    "embedding_size = 256 # dimension of embedding layer\n",
    "num_hidden_params = 256 # transformer encoder's hidden (feed forward) layer dimension\n",
    "num_layers = 2 # num of transformer encoder layers within transformer encoder\n",
    "num_heads = 2 # num of heads in (multi head) attention models\n",
    "dropout = 0.25 # value (fraction) of dropout\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lrate = 4.0 # learning rate\n",
    "transformer_model = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, \n",
    "                                     dropout).to(device)\n",
    "optim_module = torch.optim.SGD(transformer_model.parameters(), lr=lrate)\n",
    "sched_module = torch.optim.lr_scheduler.StepLR(optim_module, 1.0, gamma=0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    transformer_model.train()\n",
    "    loss_total = 0.\n",
    "    time_start = time.time()\n",
    "    num_tokens = len(TEXT.vocab.stoi)\n",
    "    for b, i in enumerate(range(0, training_data.size(0) - 1, max_seq_len)):\n",
    "        train_data_batch, train_label_batch = return_batch(training_data, i)\n",
    "        optim_module.zero_grad()\n",
    "        op = transformer_model(train_data_batch)\n",
    "        loss_curr = loss_func(op.view(-1, num_tokens), train_label_batch)\n",
    "        loss_curr.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.6)\n",
    "        optim_module.step()\n",
    "\n",
    "        loss_total += loss_curr.item()\n",
    "        interval = 100\n",
    "        if b % interval == 0 and b > 0:\n",
    "            loss_interval = loss_total / interval\n",
    "            time_delta = time.time() - time_start\n",
    "            print(f\"epoch {ep}, {b}/{len(training_data)//max_seq_len} batches, training loss {loss_interval:.2f}, training perplexity {math.exp(loss_interval):.2f}\")\n",
    "            loss_total = 0\n",
    "            time_start = time.time()\n",
    "\n",
    "def eval_model(eval_model_obj, eval_data_source):\n",
    "    eval_model_obj.eval() \n",
    "    loss_total = 0.\n",
    "    num_tokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, eval_data_source.size(0) - 1, max_seq_len):\n",
    "            eval_data, eval_label = return_batch(eval_data_source, j)\n",
    "            op = eval_model_obj(eval_data)\n",
    "            op_flat = op.view(-1, num_tokens)\n",
    "            loss_total += len(eval_data) * loss_func(op_flat, eval_label).item()\n",
    "    return loss_total / (len(eval_data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, 100/1018 batches, training loss 8.63, training perplexity 5614.45\n",
      "epoch 1, 200/1018 batches, training loss 7.23, training perplexity 1380.31\n",
      "epoch 1, 300/1018 batches, training loss 6.79, training perplexity 892.50\n",
      "epoch 1, 400/1018 batches, training loss 6.55, training perplexity 701.84\n",
      "epoch 1, 500/1018 batches, training loss 6.45, training perplexity 634.57\n",
      "epoch 1, 600/1018 batches, training loss 6.32, training perplexity 553.86\n",
      "epoch 1, 700/1018 batches, training loss 6.24, training perplexity 513.65\n",
      "epoch 1, 800/1018 batches, training loss 6.13, training perplexity 459.07\n",
      "epoch 1, 900/1018 batches, training loss 6.11, training perplexity 450.48\n",
      "epoch 1, 1000/1018 batches, training loss 6.07, training perplexity 433.88\n",
      "\n",
      "epoch 1, validation loss 5.82, validation perplexity 337.70\n",
      "\n",
      "epoch 2, 100/1018 batches, training loss 5.98, training perplexity 395.15\n",
      "epoch 2, 200/1018 batches, training loss 5.90, training perplexity 363.99\n",
      "epoch 2, 300/1018 batches, training loss 5.83, training perplexity 338.74\n",
      "epoch 2, 400/1018 batches, training loss 5.79, training perplexity 326.08\n",
      "epoch 2, 500/1018 batches, training loss 5.81, training perplexity 333.72\n",
      "epoch 2, 600/1018 batches, training loss 5.76, training perplexity 318.66\n",
      "epoch 2, 700/1018 batches, training loss 5.77, training perplexity 321.75\n",
      "epoch 2, 800/1018 batches, training loss 5.64, training perplexity 281.86\n",
      "epoch 2, 900/1018 batches, training loss 5.67, training perplexity 291.02\n",
      "epoch 2, 1000/1018 batches, training loss 5.71, training perplexity 300.97\n",
      "\n",
      "epoch 2, validation loss 5.63, validation perplexity 278.58\n",
      "\n",
      "epoch 3, 100/1018 batches, training loss 5.66, training perplexity 287.48\n",
      "epoch 3, 200/1018 batches, training loss 5.59, training perplexity 268.82\n",
      "epoch 3, 300/1018 batches, training loss 5.54, training perplexity 255.26\n",
      "epoch 3, 400/1018 batches, training loss 5.51, training perplexity 248.34\n",
      "epoch 3, 500/1018 batches, training loss 5.54, training perplexity 254.87\n",
      "epoch 3, 600/1018 batches, training loss 5.52, training perplexity 249.04\n",
      "epoch 3, 700/1018 batches, training loss 5.53, training perplexity 252.00\n",
      "epoch 3, 800/1018 batches, training loss 5.39, training perplexity 218.15\n",
      "epoch 3, 900/1018 batches, training loss 5.44, training perplexity 229.63\n",
      "epoch 3, 1000/1018 batches, training loss 5.48, training perplexity 240.72\n",
      "\n",
      "epoch 3, validation loss 5.40, validation perplexity 221.83\n",
      "\n",
      "epoch 4, 100/1018 batches, training loss 5.46, training perplexity 234.99\n",
      "epoch 4, 200/1018 batches, training loss 5.39, training perplexity 220.20\n",
      "epoch 4, 300/1018 batches, training loss 5.36, training perplexity 211.95\n",
      "epoch 4, 400/1018 batches, training loss 5.34, training perplexity 207.68\n",
      "epoch 4, 500/1018 batches, training loss 5.36, training perplexity 211.72\n",
      "epoch 4, 600/1018 batches, training loss 5.34, training perplexity 207.99\n",
      "epoch 4, 700/1018 batches, training loss 5.36, training perplexity 212.20\n",
      "epoch 4, 800/1018 batches, training loss 5.20, training perplexity 181.77\n",
      "epoch 4, 900/1018 batches, training loss 5.26, training perplexity 193.30\n",
      "epoch 4, 1000/1018 batches, training loss 5.32, training perplexity 203.74\n",
      "\n",
      "epoch 4, validation loss 5.35, validation perplexity 209.77\n",
      "\n",
      "epoch 5, 100/1018 batches, training loss 5.31, training perplexity 201.49\n",
      "epoch 5, 200/1018 batches, training loss 5.24, training perplexity 188.74\n",
      "epoch 5, 300/1018 batches, training loss 5.21, training perplexity 182.43\n",
      "epoch 5, 400/1018 batches, training loss 5.20, training perplexity 180.85\n",
      "epoch 5, 500/1018 batches, training loss 5.21, training perplexity 183.26\n",
      "epoch 5, 600/1018 batches, training loss 5.20, training perplexity 181.39\n",
      "epoch 5, 700/1018 batches, training loss 5.22, training perplexity 184.16\n",
      "epoch 5, 800/1018 batches, training loss 5.06, training perplexity 157.45\n",
      "epoch 5, 900/1018 batches, training loss 5.13, training perplexity 168.55\n",
      "epoch 5, 1000/1018 batches, training loss 5.18, training perplexity 177.71\n",
      "\n",
      "epoch 5, validation loss 5.25, validation perplexity 191.34\n",
      "\n",
      "epoch 6, 100/1018 batches, training loss 5.18, training perplexity 177.00\n",
      "epoch 6, 200/1018 batches, training loss 5.11, training perplexity 165.34\n",
      "epoch 6, 300/1018 batches, training loss 5.09, training perplexity 162.07\n",
      "epoch 6, 400/1018 batches, training loss 5.08, training perplexity 160.66\n",
      "epoch 6, 500/1018 batches, training loss 5.09, training perplexity 162.77\n",
      "epoch 6, 600/1018 batches, training loss 5.09, training perplexity 161.63\n",
      "epoch 6, 700/1018 batches, training loss 5.10, training perplexity 164.79\n",
      "epoch 6, 800/1018 batches, training loss 4.94, training perplexity 139.78\n",
      "epoch 6, 900/1018 batches, training loss 5.01, training perplexity 149.44\n",
      "epoch 6, 1000/1018 batches, training loss 5.06, training perplexity 158.02\n",
      "\n",
      "epoch 6, validation loss 5.17, validation perplexity 176.13\n",
      "\n",
      "epoch 7, 100/1018 batches, training loss 5.07, training perplexity 159.31\n",
      "epoch 7, 200/1018 batches, training loss 5.00, training perplexity 148.27\n",
      "epoch 7, 300/1018 batches, training loss 4.98, training perplexity 145.96\n",
      "epoch 7, 400/1018 batches, training loss 4.98, training perplexity 145.43\n",
      "epoch 7, 500/1018 batches, training loss 4.99, training perplexity 146.53\n",
      "epoch 7, 600/1018 batches, training loss 4.98, training perplexity 145.70\n",
      "epoch 7, 700/1018 batches, training loss 5.01, training perplexity 149.36\n",
      "epoch 7, 800/1018 batches, training loss 4.84, training perplexity 126.40\n",
      "epoch 7, 900/1018 batches, training loss 4.91, training perplexity 135.97\n",
      "epoch 7, 1000/1018 batches, training loss 4.96, training perplexity 143.18\n",
      "\n",
      "epoch 7, validation loss 5.15, validation perplexity 171.81\n",
      "\n",
      "epoch 8, 100/1018 batches, training loss 4.98, training perplexity 144.99\n",
      "epoch 8, 200/1018 batches, training loss 4.91, training perplexity 135.37\n",
      "epoch 8, 300/1018 batches, training loss 4.89, training perplexity 133.54\n",
      "epoch 8, 400/1018 batches, training loss 4.89, training perplexity 132.93\n",
      "epoch 8, 500/1018 batches, training loss 4.90, training perplexity 134.35\n",
      "epoch 8, 600/1018 batches, training loss 4.89, training perplexity 133.37\n",
      "epoch 8, 700/1018 batches, training loss 4.92, training perplexity 136.73\n",
      "epoch 8, 800/1018 batches, training loss 4.75, training perplexity 116.09\n",
      "epoch 8, 900/1018 batches, training loss 4.82, training perplexity 124.44\n",
      "epoch 8, 1000/1018 batches, training loss 4.88, training perplexity 131.09\n",
      "\n",
      "epoch 8, validation loss 5.11, validation perplexity 166.43\n",
      "\n",
      "epoch 9, 100/1018 batches, training loss 4.90, training perplexity 134.42\n",
      "epoch 9, 200/1018 batches, training loss 4.83, training perplexity 124.90\n",
      "epoch 9, 300/1018 batches, training loss 4.82, training perplexity 124.05\n",
      "epoch 9, 400/1018 batches, training loss 4.81, training perplexity 123.31\n",
      "epoch 9, 500/1018 batches, training loss 4.82, training perplexity 124.30\n",
      "epoch 9, 600/1018 batches, training loss 4.82, training perplexity 123.87\n",
      "epoch 9, 700/1018 batches, training loss 4.85, training perplexity 127.15\n",
      "epoch 9, 800/1018 batches, training loss 4.68, training perplexity 107.60\n",
      "epoch 9, 900/1018 batches, training loss 4.75, training perplexity 115.65\n",
      "epoch 9, 1000/1018 batches, training loss 4.80, training perplexity 121.10\n",
      "\n",
      "epoch 9, validation loss 5.10, validation perplexity 163.38\n",
      "\n",
      "epoch 10, 100/1018 batches, training loss 4.83, training perplexity 125.39\n",
      "epoch 10, 200/1018 batches, training loss 4.75, training perplexity 116.02\n",
      "epoch 10, 300/1018 batches, training loss 4.75, training perplexity 115.87\n",
      "epoch 10, 400/1018 batches, training loss 4.75, training perplexity 115.86\n",
      "epoch 10, 500/1018 batches, training loss 4.76, training perplexity 116.17\n",
      "epoch 10, 600/1018 batches, training loss 4.75, training perplexity 115.96\n",
      "epoch 10, 700/1018 batches, training loss 4.78, training perplexity 118.95\n",
      "epoch 10, 800/1018 batches, training loss 4.62, training perplexity 101.31\n",
      "epoch 10, 900/1018 batches, training loss 4.69, training perplexity 108.76\n",
      "epoch 10, 1000/1018 batches, training loss 4.73, training perplexity 113.81\n",
      "\n",
      "epoch 10, validation loss 5.08, validation perplexity 160.96\n",
      "\n",
      "epoch 11, 100/1018 batches, training loss 4.78, training perplexity 118.51\n",
      "epoch 11, 200/1018 batches, training loss 4.69, training perplexity 109.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, 300/1018 batches, training loss 4.70, training perplexity 109.89\n",
      "epoch 11, 400/1018 batches, training loss 4.70, training perplexity 109.53\n",
      "epoch 11, 500/1018 batches, training loss 4.70, training perplexity 109.91\n",
      "epoch 11, 600/1018 batches, training loss 4.70, training perplexity 110.13\n",
      "epoch 11, 700/1018 batches, training loss 4.72, training perplexity 112.66\n",
      "epoch 11, 800/1018 batches, training loss 4.57, training perplexity 96.17\n",
      "epoch 11, 900/1018 batches, training loss 4.64, training perplexity 103.05\n",
      "epoch 11, 1000/1018 batches, training loss 4.68, training perplexity 107.53\n",
      "\n",
      "epoch 11, validation loss 5.07, validation perplexity 159.83\n",
      "\n",
      "epoch 12, 100/1018 batches, training loss 4.72, training perplexity 112.17\n",
      "epoch 12, 200/1018 batches, training loss 4.65, training perplexity 104.32\n",
      "epoch 12, 300/1018 batches, training loss 4.65, training perplexity 104.52\n",
      "epoch 12, 400/1018 batches, training loss 4.65, training perplexity 104.79\n",
      "epoch 12, 500/1018 batches, training loss 4.65, training perplexity 104.86\n",
      "epoch 12, 600/1018 batches, training loss 4.65, training perplexity 104.85\n",
      "epoch 12, 700/1018 batches, training loss 4.68, training perplexity 107.57\n",
      "epoch 12, 800/1018 batches, training loss 4.52, training perplexity 91.74\n",
      "epoch 12, 900/1018 batches, training loss 4.59, training perplexity 98.33\n",
      "epoch 12, 1000/1018 batches, training loss 4.63, training perplexity 102.73\n",
      "\n",
      "epoch 12, validation loss 5.07, validation perplexity 158.38\n",
      "\n",
      "epoch 13, 100/1018 batches, training loss 4.68, training perplexity 107.78\n",
      "epoch 13, 200/1018 batches, training loss 4.60, training perplexity 99.70\n",
      "epoch 13, 300/1018 batches, training loss 4.61, training perplexity 100.51\n",
      "epoch 13, 400/1018 batches, training loss 4.61, training perplexity 100.45\n",
      "epoch 13, 500/1018 batches, training loss 4.61, training perplexity 100.73\n",
      "epoch 13, 600/1018 batches, training loss 4.61, training perplexity 100.60\n",
      "epoch 13, 700/1018 batches, training loss 4.64, training perplexity 103.52\n",
      "epoch 13, 800/1018 batches, training loss 4.48, training perplexity 87.99\n",
      "epoch 13, 900/1018 batches, training loss 4.55, training perplexity 94.94\n",
      "epoch 13, 1000/1018 batches, training loss 4.59, training perplexity 98.38\n",
      "\n",
      "epoch 13, validation loss 5.06, validation perplexity 156.83\n",
      "\n",
      "epoch 14, 100/1018 batches, training loss 4.64, training perplexity 103.72\n",
      "epoch 14, 200/1018 batches, training loss 4.57, training perplexity 96.08\n",
      "epoch 14, 300/1018 batches, training loss 4.57, training perplexity 96.81\n",
      "epoch 14, 400/1018 batches, training loss 4.57, training perplexity 96.82\n",
      "epoch 14, 500/1018 batches, training loss 4.58, training perplexity 97.09\n",
      "epoch 14, 600/1018 batches, training loss 4.58, training perplexity 97.27\n",
      "epoch 14, 700/1018 batches, training loss 4.60, training perplexity 99.35\n",
      "epoch 14, 800/1018 batches, training loss 4.44, training perplexity 85.17\n",
      "epoch 14, 900/1018 batches, training loss 4.52, training perplexity 91.61\n",
      "epoch 14, 1000/1018 batches, training loss 4.55, training perplexity 94.93\n",
      "\n",
      "epoch 14, validation loss 5.05, validation perplexity 156.21\n",
      "\n",
      "epoch 15, 100/1018 batches, training loss 4.61, training perplexity 100.60\n",
      "epoch 15, 200/1018 batches, training loss 4.54, training perplexity 93.36\n",
      "epoch 15, 300/1018 batches, training loss 4.54, training perplexity 93.79\n",
      "epoch 15, 400/1018 batches, training loss 4.54, training perplexity 93.75\n",
      "epoch 15, 500/1018 batches, training loss 4.55, training perplexity 94.21\n",
      "epoch 15, 600/1018 batches, training loss 4.55, training perplexity 94.35\n",
      "epoch 15, 700/1018 batches, training loss 4.57, training perplexity 96.84\n",
      "epoch 15, 800/1018 batches, training loss 4.42, training perplexity 82.84\n",
      "epoch 15, 900/1018 batches, training loss 4.49, training perplexity 89.00\n",
      "epoch 15, 1000/1018 batches, training loss 4.52, training perplexity 92.24\n",
      "\n",
      "epoch 15, validation loss 5.04, validation perplexity 155.20\n",
      "\n",
      "epoch 16, 100/1018 batches, training loss 4.58, training perplexity 97.83\n",
      "epoch 16, 200/1018 batches, training loss 4.51, training perplexity 90.76\n",
      "epoch 16, 300/1018 batches, training loss 4.51, training perplexity 91.37\n",
      "epoch 16, 400/1018 batches, training loss 4.52, training perplexity 91.79\n",
      "epoch 16, 500/1018 batches, training loss 4.52, training perplexity 92.02\n",
      "epoch 16, 600/1018 batches, training loss 4.52, training perplexity 91.64\n",
      "epoch 16, 700/1018 batches, training loss 4.55, training perplexity 94.33\n",
      "epoch 16, 800/1018 batches, training loss 4.39, training perplexity 80.75\n",
      "epoch 16, 900/1018 batches, training loss 4.46, training perplexity 86.69\n",
      "epoch 16, 1000/1018 batches, training loss 4.49, training perplexity 89.48\n",
      "\n",
      "epoch 16, validation loss 5.05, validation perplexity 155.90\n",
      "\n",
      "epoch 17, 100/1018 batches, training loss 4.56, training perplexity 95.65\n",
      "epoch 17, 200/1018 batches, training loss 4.49, training perplexity 88.96\n",
      "epoch 17, 300/1018 batches, training loss 4.49, training perplexity 89.48\n",
      "epoch 17, 400/1018 batches, training loss 4.50, training perplexity 89.58\n",
      "epoch 17, 500/1018 batches, training loss 4.50, training perplexity 90.10\n",
      "epoch 17, 600/1018 batches, training loss 4.49, training perplexity 89.46\n",
      "epoch 17, 700/1018 batches, training loss 4.53, training perplexity 92.34\n",
      "epoch 17, 800/1018 batches, training loss 4.37, training perplexity 79.28\n",
      "epoch 17, 900/1018 batches, training loss 4.44, training perplexity 84.64\n",
      "epoch 17, 1000/1018 batches, training loss 4.47, training perplexity 87.72\n",
      "\n",
      "epoch 17, validation loss 5.04, validation perplexity 154.87\n",
      "\n",
      "epoch 18, 100/1018 batches, training loss 4.54, training perplexity 93.88\n",
      "epoch 18, 200/1018 batches, training loss 4.46, training perplexity 86.89\n",
      "epoch 18, 300/1018 batches, training loss 4.47, training perplexity 87.63\n",
      "epoch 18, 400/1018 batches, training loss 4.47, training perplexity 87.79\n",
      "epoch 18, 500/1018 batches, training loss 4.48, training perplexity 87.99\n",
      "epoch 18, 600/1018 batches, training loss 4.48, training perplexity 88.20\n",
      "epoch 18, 700/1018 batches, training loss 4.51, training perplexity 90.84\n",
      "epoch 18, 800/1018 batches, training loss 4.35, training perplexity 77.49\n",
      "epoch 18, 900/1018 batches, training loss 4.42, training perplexity 83.16\n",
      "epoch 18, 1000/1018 batches, training loss 4.45, training perplexity 85.89\n",
      "\n",
      "epoch 18, validation loss 5.04, validation perplexity 155.08\n",
      "\n",
      "epoch 19, 100/1018 batches, training loss 4.52, training perplexity 92.26\n",
      "epoch 19, 200/1018 batches, training loss 4.45, training perplexity 85.55\n",
      "epoch 19, 300/1018 batches, training loss 4.46, training perplexity 86.15\n",
      "epoch 19, 400/1018 batches, training loss 4.46, training perplexity 86.49\n",
      "epoch 19, 500/1018 batches, training loss 4.46, training perplexity 86.92\n",
      "epoch 19, 600/1018 batches, training loss 4.46, training perplexity 86.59\n",
      "epoch 19, 700/1018 batches, training loss 4.49, training perplexity 89.10\n",
      "epoch 19, 800/1018 batches, training loss 4.34, training perplexity 76.35\n",
      "epoch 19, 900/1018 batches, training loss 4.41, training perplexity 81.95\n",
      "epoch 19, 1000/1018 batches, training loss 4.43, training perplexity 84.18\n",
      "\n",
      "epoch 19, validation loss 5.03, validation perplexity 153.44\n",
      "\n",
      "epoch 20, 100/1018 batches, training loss 4.51, training perplexity 90.74\n",
      "epoch 20, 200/1018 batches, training loss 4.43, training perplexity 84.24\n",
      "epoch 20, 300/1018 batches, training loss 4.44, training perplexity 85.11\n",
      "epoch 20, 400/1018 batches, training loss 4.44, training perplexity 84.97\n",
      "epoch 20, 500/1018 batches, training loss 4.45, training perplexity 85.37\n",
      "epoch 20, 600/1018 batches, training loss 4.45, training perplexity 85.49\n",
      "epoch 20, 700/1018 batches, training loss 4.48, training perplexity 87.88\n",
      "epoch 20, 800/1018 batches, training loss 4.32, training perplexity 75.37\n",
      "epoch 20, 900/1018 batches, training loss 4.39, training perplexity 80.89\n",
      "epoch 20, 1000/1018 batches, training loss 4.42, training perplexity 83.39\n",
      "\n",
      "epoch 20, validation loss 5.03, validation perplexity 152.68\n",
      "\n",
      "epoch 21, 100/1018 batches, training loss 4.49, training perplexity 89.34\n",
      "epoch 21, 200/1018 batches, training loss 4.42, training perplexity 83.13\n",
      "epoch 21, 300/1018 batches, training loss 4.43, training perplexity 84.13\n",
      "epoch 21, 400/1018 batches, training loss 4.43, training perplexity 83.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, 500/1018 batches, training loss 4.43, training perplexity 83.93\n",
      "epoch 21, 600/1018 batches, training loss 4.43, training perplexity 84.29\n",
      "epoch 21, 700/1018 batches, training loss 4.46, training perplexity 86.58\n",
      "epoch 21, 800/1018 batches, training loss 4.31, training perplexity 74.16\n",
      "epoch 21, 900/1018 batches, training loss 4.38, training perplexity 80.12\n",
      "epoch 21, 1000/1018 batches, training loss 4.41, training perplexity 82.29\n",
      "\n",
      "epoch 21, validation loss 5.04, validation perplexity 153.79\n",
      "\n",
      "epoch 22, 100/1018 batches, training loss 4.48, training perplexity 88.31\n",
      "epoch 22, 200/1018 batches, training loss 4.41, training perplexity 82.67\n",
      "epoch 22, 300/1018 batches, training loss 4.42, training perplexity 83.34\n",
      "epoch 22, 400/1018 batches, training loss 4.42, training perplexity 83.06\n",
      "epoch 22, 500/1018 batches, training loss 4.42, training perplexity 83.42\n",
      "epoch 22, 600/1018 batches, training loss 4.42, training perplexity 83.23\n",
      "epoch 22, 700/1018 batches, training loss 4.45, training perplexity 85.73\n",
      "epoch 22, 800/1018 batches, training loss 4.30, training perplexity 73.73\n",
      "epoch 22, 900/1018 batches, training loss 4.37, training perplexity 79.07\n",
      "epoch 22, 1000/1018 batches, training loss 4.40, training perplexity 81.33\n",
      "\n",
      "epoch 22, validation loss 5.02, validation perplexity 152.15\n",
      "\n",
      "epoch 23, 100/1018 batches, training loss 4.48, training perplexity 87.92\n",
      "epoch 23, 200/1018 batches, training loss 4.40, training perplexity 81.63\n",
      "epoch 23, 300/1018 batches, training loss 4.41, training perplexity 82.40\n",
      "epoch 23, 400/1018 batches, training loss 4.41, training perplexity 82.46\n",
      "epoch 23, 500/1018 batches, training loss 4.41, training perplexity 82.62\n",
      "epoch 23, 600/1018 batches, training loss 4.41, training perplexity 82.47\n",
      "epoch 23, 700/1018 batches, training loss 4.45, training perplexity 85.24\n",
      "epoch 23, 800/1018 batches, training loss 4.29, training perplexity 73.03\n",
      "epoch 23, 900/1018 batches, training loss 4.36, training perplexity 78.47\n",
      "epoch 23, 1000/1018 batches, training loss 4.39, training perplexity 80.77\n",
      "\n",
      "epoch 23, validation loss 5.03, validation perplexity 152.58\n",
      "\n",
      "epoch 24, 100/1018 batches, training loss 4.46, training perplexity 86.85\n",
      "epoch 24, 200/1018 batches, training loss 4.40, training perplexity 81.21\n",
      "epoch 24, 300/1018 batches, training loss 4.41, training perplexity 81.92\n",
      "epoch 24, 400/1018 batches, training loss 4.41, training perplexity 81.90\n",
      "epoch 24, 500/1018 batches, training loss 4.41, training perplexity 82.12\n",
      "epoch 24, 600/1018 batches, training loss 4.41, training perplexity 81.98\n",
      "epoch 24, 700/1018 batches, training loss 4.44, training perplexity 84.48\n",
      "epoch 24, 800/1018 batches, training loss 4.28, training perplexity 72.38\n",
      "epoch 24, 900/1018 batches, training loss 4.36, training perplexity 77.90\n",
      "epoch 24, 1000/1018 batches, training loss 4.38, training perplexity 79.98\n",
      "\n",
      "epoch 24, validation loss 5.02, validation perplexity 151.43\n",
      "\n",
      "epoch 25, 100/1018 batches, training loss 4.46, training perplexity 86.33\n",
      "epoch 25, 200/1018 batches, training loss 4.39, training perplexity 80.61\n",
      "epoch 25, 300/1018 batches, training loss 4.40, training perplexity 81.46\n",
      "epoch 25, 400/1018 batches, training loss 4.40, training perplexity 81.23\n",
      "epoch 25, 500/1018 batches, training loss 4.40, training perplexity 81.51\n",
      "epoch 25, 600/1018 batches, training loss 4.40, training perplexity 81.40\n",
      "epoch 25, 700/1018 batches, training loss 4.43, training perplexity 83.68\n",
      "epoch 25, 800/1018 batches, training loss 4.28, training perplexity 71.94\n",
      "epoch 25, 900/1018 batches, training loss 4.35, training perplexity 77.48\n",
      "epoch 25, 1000/1018 batches, training loss 4.38, training perplexity 79.54\n",
      "\n",
      "epoch 25, validation loss 5.02, validation perplexity 151.41\n",
      "\n",
      "epoch 26, 100/1018 batches, training loss 4.45, training perplexity 85.92\n",
      "epoch 26, 200/1018 batches, training loss 4.38, training perplexity 79.95\n",
      "epoch 26, 300/1018 batches, training loss 4.39, training perplexity 80.84\n",
      "epoch 26, 400/1018 batches, training loss 4.39, training perplexity 80.80\n",
      "epoch 26, 500/1018 batches, training loss 4.40, training perplexity 81.18\n",
      "epoch 26, 600/1018 batches, training loss 4.39, training perplexity 80.90\n",
      "epoch 26, 700/1018 batches, training loss 4.42, training perplexity 83.40\n",
      "epoch 26, 800/1018 batches, training loss 4.27, training perplexity 71.54\n",
      "epoch 26, 900/1018 batches, training loss 4.34, training perplexity 76.52\n",
      "epoch 26, 1000/1018 batches, training loss 4.37, training perplexity 78.93\n",
      "\n",
      "epoch 26, validation loss 5.02, validation perplexity 151.18\n",
      "\n",
      "epoch 27, 100/1018 batches, training loss 4.45, training perplexity 85.42\n",
      "epoch 27, 200/1018 batches, training loss 4.38, training perplexity 79.67\n",
      "epoch 27, 300/1018 batches, training loss 4.39, training perplexity 80.47\n",
      "epoch 27, 400/1018 batches, training loss 4.39, training perplexity 80.54\n",
      "epoch 27, 500/1018 batches, training loss 4.39, training perplexity 80.83\n",
      "epoch 27, 600/1018 batches, training loss 4.39, training perplexity 80.49\n",
      "epoch 27, 700/1018 batches, training loss 4.42, training perplexity 82.89\n",
      "epoch 27, 800/1018 batches, training loss 4.27, training perplexity 71.30\n",
      "epoch 27, 900/1018 batches, training loss 4.34, training perplexity 76.51\n",
      "epoch 27, 1000/1018 batches, training loss 4.37, training perplexity 78.73\n",
      "\n",
      "epoch 27, validation loss 5.02, validation perplexity 151.37\n",
      "\n",
      "epoch 28, 100/1018 batches, training loss 4.45, training perplexity 85.34\n",
      "epoch 28, 200/1018 batches, training loss 4.37, training perplexity 79.33\n",
      "epoch 28, 300/1018 batches, training loss 4.39, training perplexity 80.53\n",
      "epoch 28, 400/1018 batches, training loss 4.38, training perplexity 79.86\n",
      "epoch 28, 500/1018 batches, training loss 4.39, training perplexity 80.50\n",
      "epoch 28, 600/1018 batches, training loss 4.39, training perplexity 80.24\n",
      "epoch 28, 700/1018 batches, training loss 4.41, training perplexity 82.53\n",
      "epoch 28, 800/1018 batches, training loss 4.27, training perplexity 71.26\n",
      "epoch 28, 900/1018 batches, training loss 4.34, training perplexity 76.46\n",
      "epoch 28, 1000/1018 batches, training loss 4.36, training perplexity 78.32\n",
      "\n",
      "epoch 28, validation loss 5.02, validation perplexity 150.82\n",
      "\n",
      "epoch 29, 100/1018 batches, training loss 4.44, training perplexity 85.07\n",
      "epoch 29, 200/1018 batches, training loss 4.37, training perplexity 79.41\n",
      "epoch 29, 300/1018 batches, training loss 4.38, training perplexity 80.02\n",
      "epoch 29, 400/1018 batches, training loss 4.38, training perplexity 79.85\n",
      "epoch 29, 500/1018 batches, training loss 4.39, training perplexity 80.39\n",
      "epoch 29, 600/1018 batches, training loss 4.38, training perplexity 80.00\n",
      "epoch 29, 700/1018 batches, training loss 4.41, training perplexity 82.44\n",
      "epoch 29, 800/1018 batches, training loss 4.26, training perplexity 70.62\n",
      "epoch 29, 900/1018 batches, training loss 4.33, training perplexity 76.28\n",
      "epoch 29, 1000/1018 batches, training loss 4.36, training perplexity 78.31\n",
      "\n",
      "epoch 29, validation loss 5.01, validation perplexity 150.28\n",
      "\n",
      "epoch 30, 100/1018 batches, training loss 4.44, training perplexity 84.80\n",
      "epoch 30, 200/1018 batches, training loss 4.37, training perplexity 79.34\n",
      "epoch 30, 300/1018 batches, training loss 4.38, training perplexity 79.90\n",
      "epoch 30, 400/1018 batches, training loss 4.38, training perplexity 79.98\n",
      "epoch 30, 500/1018 batches, training loss 4.38, training perplexity 80.22\n",
      "epoch 30, 600/1018 batches, training loss 4.38, training perplexity 79.94\n",
      "epoch 30, 700/1018 batches, training loss 4.41, training perplexity 82.45\n",
      "epoch 30, 800/1018 batches, training loss 4.26, training perplexity 70.66\n",
      "epoch 30, 900/1018 batches, training loss 4.33, training perplexity 76.00\n",
      "epoch 30, 1000/1018 batches, training loss 4.35, training perplexity 77.84\n",
      "\n",
      "epoch 30, validation loss 5.01, validation perplexity 150.11\n",
      "\n",
      "epoch 31, 100/1018 batches, training loss 4.44, training perplexity 84.60\n",
      "epoch 31, 200/1018 batches, training loss 4.37, training perplexity 79.08\n",
      "epoch 31, 300/1018 batches, training loss 4.38, training perplexity 80.01\n",
      "epoch 31, 400/1018 batches, training loss 4.38, training perplexity 79.85\n",
      "epoch 31, 500/1018 batches, training loss 4.38, training perplexity 80.12\n",
      "epoch 31, 600/1018 batches, training loss 4.38, training perplexity 79.74\n",
      "epoch 31, 700/1018 batches, training loss 4.41, training perplexity 82.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31, 800/1018 batches, training loss 4.26, training perplexity 70.57\n",
      "epoch 31, 900/1018 batches, training loss 4.33, training perplexity 76.10\n",
      "epoch 31, 1000/1018 batches, training loss 4.35, training perplexity 77.66\n",
      "\n",
      "epoch 31, validation loss 5.01, validation perplexity 149.84\n",
      "\n",
      "epoch 32, 100/1018 batches, training loss 4.43, training perplexity 84.29\n",
      "epoch 32, 200/1018 batches, training loss 4.37, training perplexity 79.22\n",
      "epoch 32, 300/1018 batches, training loss 4.38, training perplexity 79.67\n",
      "epoch 32, 400/1018 batches, training loss 4.37, training perplexity 79.27\n",
      "epoch 32, 500/1018 batches, training loss 4.38, training perplexity 79.94\n",
      "epoch 32, 600/1018 batches, training loss 4.38, training perplexity 79.58\n",
      "epoch 32, 700/1018 batches, training loss 4.41, training perplexity 82.00\n",
      "epoch 32, 800/1018 batches, training loss 4.25, training perplexity 70.27\n",
      "epoch 32, 900/1018 batches, training loss 4.33, training perplexity 75.91\n",
      "epoch 32, 1000/1018 batches, training loss 4.36, training perplexity 77.98\n",
      "\n",
      "epoch 32, validation loss 5.01, validation perplexity 149.63\n",
      "\n",
      "epoch 33, 100/1018 batches, training loss 4.44, training perplexity 84.43\n",
      "epoch 33, 200/1018 batches, training loss 4.37, training perplexity 78.98\n",
      "epoch 33, 300/1018 batches, training loss 4.38, training perplexity 79.72\n",
      "epoch 33, 400/1018 batches, training loss 4.38, training perplexity 79.68\n",
      "epoch 33, 500/1018 batches, training loss 4.38, training perplexity 80.14\n",
      "epoch 33, 600/1018 batches, training loss 4.38, training perplexity 79.74\n",
      "epoch 33, 700/1018 batches, training loss 4.41, training perplexity 82.09\n",
      "epoch 33, 800/1018 batches, training loss 4.25, training perplexity 70.43\n",
      "epoch 33, 900/1018 batches, training loss 4.33, training perplexity 75.77\n",
      "epoch 33, 1000/1018 batches, training loss 4.35, training perplexity 77.66\n",
      "\n",
      "epoch 33, validation loss 5.01, validation perplexity 149.26\n",
      "\n",
      "epoch 34, 100/1018 batches, training loss 4.44, training perplexity 84.59\n",
      "epoch 34, 200/1018 batches, training loss 4.37, training perplexity 79.13\n",
      "epoch 34, 300/1018 batches, training loss 4.38, training perplexity 79.70\n",
      "epoch 34, 400/1018 batches, training loss 4.38, training perplexity 79.53\n",
      "epoch 34, 500/1018 batches, training loss 4.38, training perplexity 80.18\n",
      "epoch 34, 600/1018 batches, training loss 4.38, training perplexity 79.56\n",
      "epoch 34, 700/1018 batches, training loss 4.41, training perplexity 82.03\n",
      "epoch 34, 800/1018 batches, training loss 4.25, training perplexity 70.27\n",
      "epoch 34, 900/1018 batches, training loss 4.33, training perplexity 75.67\n",
      "epoch 34, 1000/1018 batches, training loss 4.35, training perplexity 77.30\n",
      "\n",
      "epoch 34, validation loss 5.01, validation perplexity 149.31\n",
      "\n",
      "epoch 35, 100/1018 batches, training loss 4.44, training perplexity 84.59\n",
      "epoch 35, 200/1018 batches, training loss 4.37, training perplexity 79.14\n",
      "epoch 35, 300/1018 batches, training loss 4.38, training perplexity 79.78\n",
      "epoch 35, 400/1018 batches, training loss 4.37, training perplexity 79.38\n",
      "epoch 35, 500/1018 batches, training loss 4.38, training perplexity 80.09\n",
      "epoch 35, 600/1018 batches, training loss 4.38, training perplexity 79.54\n",
      "epoch 35, 700/1018 batches, training loss 4.41, training perplexity 82.09\n",
      "epoch 35, 800/1018 batches, training loss 4.26, training perplexity 70.50\n",
      "epoch 35, 900/1018 batches, training loss 4.33, training perplexity 75.62\n",
      "epoch 35, 1000/1018 batches, training loss 4.35, training perplexity 77.72\n",
      "\n",
      "epoch 35, validation loss 5.00, validation perplexity 149.11\n",
      "\n",
      "epoch 36, 100/1018 batches, training loss 4.43, training perplexity 84.33\n",
      "epoch 36, 200/1018 batches, training loss 4.37, training perplexity 79.17\n",
      "epoch 36, 300/1018 batches, training loss 4.38, training perplexity 79.67\n",
      "epoch 36, 400/1018 batches, training loss 4.37, training perplexity 79.43\n",
      "epoch 36, 500/1018 batches, training loss 4.39, training perplexity 80.37\n",
      "epoch 36, 600/1018 batches, training loss 4.38, training perplexity 79.86\n",
      "epoch 36, 700/1018 batches, training loss 4.41, training perplexity 81.99\n",
      "epoch 36, 800/1018 batches, training loss 4.25, training perplexity 70.45\n",
      "epoch 36, 900/1018 batches, training loss 4.33, training perplexity 75.66\n",
      "epoch 36, 1000/1018 batches, training loss 4.35, training perplexity 77.69\n",
      "\n",
      "epoch 36, validation loss 5.00, validation perplexity 148.60\n",
      "\n",
      "epoch 37, 100/1018 batches, training loss 4.44, training perplexity 84.41\n",
      "epoch 37, 200/1018 batches, training loss 4.37, training perplexity 79.20\n",
      "epoch 37, 300/1018 batches, training loss 4.38, training perplexity 79.52\n",
      "epoch 37, 400/1018 batches, training loss 4.38, training perplexity 79.71\n",
      "epoch 37, 500/1018 batches, training loss 4.38, training perplexity 80.00\n",
      "epoch 37, 600/1018 batches, training loss 4.38, training perplexity 79.72\n",
      "epoch 37, 700/1018 batches, training loss 4.40, training perplexity 81.77\n",
      "epoch 37, 800/1018 batches, training loss 4.25, training perplexity 70.36\n",
      "epoch 37, 900/1018 batches, training loss 4.33, training perplexity 75.94\n",
      "epoch 37, 1000/1018 batches, training loss 4.35, training perplexity 77.65\n",
      "\n",
      "epoch 37, validation loss 5.00, validation perplexity 148.22\n",
      "\n",
      "epoch 38, 100/1018 batches, training loss 4.44, training perplexity 84.99\n",
      "epoch 38, 200/1018 batches, training loss 4.37, training perplexity 79.19\n",
      "epoch 38, 300/1018 batches, training loss 4.38, training perplexity 79.68\n",
      "epoch 38, 400/1018 batches, training loss 4.37, training perplexity 79.23\n",
      "epoch 38, 500/1018 batches, training loss 4.38, training perplexity 79.98\n",
      "epoch 38, 600/1018 batches, training loss 4.38, training perplexity 79.96\n",
      "epoch 38, 700/1018 batches, training loss 4.41, training perplexity 81.89\n",
      "epoch 38, 800/1018 batches, training loss 4.25, training perplexity 70.34\n",
      "epoch 38, 900/1018 batches, training loss 4.32, training perplexity 75.50\n",
      "epoch 38, 1000/1018 batches, training loss 4.36, training perplexity 77.88\n",
      "\n",
      "epoch 38, validation loss 5.00, validation perplexity 148.25\n",
      "\n",
      "epoch 39, 100/1018 batches, training loss 4.44, training perplexity 84.49\n",
      "epoch 39, 200/1018 batches, training loss 4.37, training perplexity 79.02\n",
      "epoch 39, 300/1018 batches, training loss 4.38, training perplexity 79.67\n",
      "epoch 39, 400/1018 batches, training loss 4.38, training perplexity 79.56\n",
      "epoch 39, 500/1018 batches, training loss 4.38, training perplexity 80.05\n",
      "epoch 39, 600/1018 batches, training loss 4.38, training perplexity 79.76\n",
      "epoch 39, 700/1018 batches, training loss 4.40, training perplexity 81.86\n",
      "epoch 39, 800/1018 batches, training loss 4.25, training perplexity 70.37\n",
      "epoch 39, 900/1018 batches, training loss 4.33, training perplexity 75.65\n",
      "epoch 39, 1000/1018 batches, training loss 4.35, training perplexity 77.86\n",
      "\n",
      "epoch 39, validation loss 5.00, validation perplexity 148.02\n",
      "\n",
      "epoch 40, 100/1018 batches, training loss 4.44, training perplexity 84.56\n",
      "epoch 40, 200/1018 batches, training loss 4.37, training perplexity 79.34\n",
      "epoch 40, 300/1018 batches, training loss 4.38, training perplexity 79.84\n",
      "epoch 40, 400/1018 batches, training loss 4.38, training perplexity 79.55\n",
      "epoch 40, 500/1018 batches, training loss 4.38, training perplexity 80.19\n",
      "epoch 40, 600/1018 batches, training loss 4.38, training perplexity 79.86\n",
      "epoch 40, 700/1018 batches, training loss 4.41, training perplexity 82.07\n",
      "epoch 40, 800/1018 batches, training loss 4.26, training perplexity 70.64\n",
      "epoch 40, 900/1018 batches, training loss 4.33, training perplexity 75.57\n",
      "epoch 40, 1000/1018 batches, training loss 4.36, training perplexity 77.94\n",
      "\n",
      "epoch 40, validation loss 5.00, validation perplexity 147.91\n",
      "\n",
      "epoch 41, 100/1018 batches, training loss 4.44, training perplexity 84.87\n",
      "epoch 41, 200/1018 batches, training loss 4.37, training perplexity 79.31\n",
      "epoch 41, 300/1018 batches, training loss 4.38, training perplexity 79.95\n",
      "epoch 41, 400/1018 batches, training loss 4.38, training perplexity 79.89\n",
      "epoch 41, 500/1018 batches, training loss 4.38, training perplexity 80.06\n",
      "epoch 41, 600/1018 batches, training loss 4.38, training perplexity 79.80\n",
      "epoch 41, 700/1018 batches, training loss 4.41, training perplexity 82.16\n",
      "epoch 41, 800/1018 batches, training loss 4.26, training perplexity 70.68\n",
      "epoch 41, 900/1018 batches, training loss 4.33, training perplexity 75.84\n",
      "epoch 41, 1000/1018 batches, training loss 4.35, training perplexity 77.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 41, validation loss 4.99, validation perplexity 147.50\n",
      "\n",
      "epoch 42, 100/1018 batches, training loss 4.44, training perplexity 84.77\n",
      "epoch 42, 200/1018 batches, training loss 4.37, training perplexity 79.33\n",
      "epoch 42, 300/1018 batches, training loss 4.38, training perplexity 79.99\n",
      "epoch 42, 400/1018 batches, training loss 4.38, training perplexity 79.87\n",
      "epoch 42, 500/1018 batches, training loss 4.38, training perplexity 80.10\n",
      "epoch 42, 600/1018 batches, training loss 4.38, training perplexity 79.91\n",
      "epoch 42, 700/1018 batches, training loss 4.41, training perplexity 82.19\n",
      "epoch 42, 800/1018 batches, training loss 4.26, training perplexity 70.55\n",
      "epoch 42, 900/1018 batches, training loss 4.33, training perplexity 75.83\n",
      "epoch 42, 1000/1018 batches, training loss 4.36, training perplexity 78.10\n",
      "\n",
      "epoch 42, validation loss 4.99, validation perplexity 147.36\n",
      "\n",
      "epoch 43, 100/1018 batches, training loss 4.44, training perplexity 85.17\n",
      "epoch 43, 200/1018 batches, training loss 4.38, training perplexity 79.49\n",
      "epoch 43, 300/1018 batches, training loss 4.38, training perplexity 80.15\n",
      "epoch 43, 400/1018 batches, training loss 4.38, training perplexity 79.77\n",
      "epoch 43, 500/1018 batches, training loss 4.38, training perplexity 79.86\n",
      "epoch 43, 600/1018 batches, training loss 4.38, training perplexity 80.00\n",
      "epoch 43, 700/1018 batches, training loss 4.41, training perplexity 82.00\n",
      "epoch 43, 800/1018 batches, training loss 4.26, training perplexity 70.66\n",
      "epoch 43, 900/1018 batches, training loss 4.33, training perplexity 75.84\n",
      "epoch 43, 1000/1018 batches, training loss 4.36, training perplexity 78.10\n",
      "\n",
      "epoch 43, validation loss 4.99, validation perplexity 146.88\n",
      "\n",
      "epoch 44, 100/1018 batches, training loss 4.44, training perplexity 85.07\n",
      "epoch 44, 200/1018 batches, training loss 4.38, training perplexity 79.63\n",
      "epoch 44, 300/1018 batches, training loss 4.38, training perplexity 80.08\n",
      "epoch 44, 400/1018 batches, training loss 4.38, training perplexity 79.92\n",
      "epoch 44, 500/1018 batches, training loss 4.38, training perplexity 80.21\n",
      "epoch 44, 600/1018 batches, training loss 4.38, training perplexity 80.07\n",
      "epoch 44, 700/1018 batches, training loss 4.41, training perplexity 82.12\n",
      "epoch 44, 800/1018 batches, training loss 4.26, training perplexity 70.86\n",
      "epoch 44, 900/1018 batches, training loss 4.33, training perplexity 75.92\n",
      "epoch 44, 1000/1018 batches, training loss 4.36, training perplexity 78.40\n",
      "\n",
      "epoch 44, validation loss 4.99, validation perplexity 146.86\n",
      "\n",
      "epoch 45, 100/1018 batches, training loss 4.45, training perplexity 85.41\n",
      "epoch 45, 200/1018 batches, training loss 4.38, training perplexity 79.61\n",
      "epoch 45, 300/1018 batches, training loss 4.38, training perplexity 80.22\n",
      "epoch 45, 400/1018 batches, training loss 4.38, training perplexity 79.90\n",
      "epoch 45, 500/1018 batches, training loss 4.39, training perplexity 80.24\n",
      "epoch 45, 600/1018 batches, training loss 4.38, training perplexity 80.22\n",
      "epoch 45, 700/1018 batches, training loss 4.41, training perplexity 82.43\n",
      "epoch 45, 800/1018 batches, training loss 4.26, training perplexity 71.06\n",
      "epoch 45, 900/1018 batches, training loss 4.33, training perplexity 75.92\n",
      "epoch 45, 1000/1018 batches, training loss 4.36, training perplexity 78.30\n",
      "\n",
      "epoch 45, validation loss 4.99, validation perplexity 146.51\n",
      "\n",
      "epoch 46, 100/1018 batches, training loss 4.45, training perplexity 85.27\n",
      "epoch 46, 200/1018 batches, training loss 4.37, training perplexity 79.37\n",
      "epoch 46, 300/1018 batches, training loss 4.38, training perplexity 80.17\n",
      "epoch 46, 400/1018 batches, training loss 4.38, training perplexity 80.10\n",
      "epoch 46, 500/1018 batches, training loss 4.38, training perplexity 80.23\n",
      "epoch 46, 600/1018 batches, training loss 4.38, training perplexity 80.20\n",
      "epoch 46, 700/1018 batches, training loss 4.41, training perplexity 82.49\n",
      "epoch 46, 800/1018 batches, training loss 4.26, training perplexity 71.03\n",
      "epoch 46, 900/1018 batches, training loss 4.33, training perplexity 76.09\n",
      "epoch 46, 1000/1018 batches, training loss 4.36, training perplexity 78.41\n",
      "\n",
      "epoch 46, validation loss 4.99, validation perplexity 146.24\n",
      "\n",
      "epoch 47, 100/1018 batches, training loss 4.45, training perplexity 85.30\n",
      "epoch 47, 200/1018 batches, training loss 4.38, training perplexity 79.66\n",
      "epoch 47, 300/1018 batches, training loss 4.39, training perplexity 80.54\n",
      "epoch 47, 400/1018 batches, training loss 4.38, training perplexity 80.22\n",
      "epoch 47, 500/1018 batches, training loss 4.39, training perplexity 80.41\n",
      "epoch 47, 600/1018 batches, training loss 4.38, training perplexity 80.09\n",
      "epoch 47, 700/1018 batches, training loss 4.42, training perplexity 82.73\n",
      "epoch 47, 800/1018 batches, training loss 4.26, training perplexity 71.00\n",
      "epoch 47, 900/1018 batches, training loss 4.33, training perplexity 76.20\n",
      "epoch 47, 1000/1018 batches, training loss 4.36, training perplexity 78.38\n",
      "\n",
      "epoch 47, validation loss 4.98, validation perplexity 146.06\n",
      "\n",
      "epoch 48, 100/1018 batches, training loss 4.45, training perplexity 85.22\n",
      "epoch 48, 200/1018 batches, training loss 4.38, training perplexity 79.79\n",
      "epoch 48, 300/1018 batches, training loss 4.39, training perplexity 80.47\n",
      "epoch 48, 400/1018 batches, training loss 4.38, training perplexity 79.97\n",
      "epoch 48, 500/1018 batches, training loss 4.39, training perplexity 80.54\n",
      "epoch 48, 600/1018 batches, training loss 4.39, training perplexity 80.33\n",
      "epoch 48, 700/1018 batches, training loss 4.41, training perplexity 82.47\n",
      "epoch 48, 800/1018 batches, training loss 4.27, training perplexity 71.19\n",
      "epoch 48, 900/1018 batches, training loss 4.33, training perplexity 76.23\n",
      "epoch 48, 1000/1018 batches, training loss 4.36, training perplexity 78.60\n",
      "\n",
      "epoch 48, validation loss 4.98, validation perplexity 145.82\n",
      "\n",
      "epoch 49, 100/1018 batches, training loss 4.45, training perplexity 85.76\n",
      "epoch 49, 200/1018 batches, training loss 4.38, training perplexity 79.81\n",
      "epoch 49, 300/1018 batches, training loss 4.39, training perplexity 80.35\n",
      "epoch 49, 400/1018 batches, training loss 4.39, training perplexity 80.26\n",
      "epoch 49, 500/1018 batches, training loss 4.39, training perplexity 80.30\n",
      "epoch 49, 600/1018 batches, training loss 4.38, training perplexity 80.14\n",
      "epoch 49, 700/1018 batches, training loss 4.41, training perplexity 82.58\n",
      "epoch 49, 800/1018 batches, training loss 4.26, training perplexity 71.02\n",
      "epoch 49, 900/1018 batches, training loss 4.33, training perplexity 76.24\n",
      "epoch 49, 1000/1018 batches, training loss 4.36, training perplexity 78.55\n",
      "\n",
      "epoch 49, validation loss 4.98, validation perplexity 145.84\n",
      "\n",
      "epoch 50, 100/1018 batches, training loss 4.45, training perplexity 85.55\n",
      "epoch 50, 200/1018 batches, training loss 4.38, training perplexity 79.68\n",
      "epoch 50, 300/1018 batches, training loss 4.39, training perplexity 80.61\n",
      "epoch 50, 400/1018 batches, training loss 4.39, training perplexity 80.27\n",
      "epoch 50, 500/1018 batches, training loss 4.39, training perplexity 80.31\n",
      "epoch 50, 600/1018 batches, training loss 4.38, training perplexity 80.17\n",
      "epoch 50, 700/1018 batches, training loss 4.41, training perplexity 82.47\n",
      "epoch 50, 800/1018 batches, training loss 4.26, training perplexity 71.00\n",
      "epoch 50, 900/1018 batches, training loss 4.33, training perplexity 76.24\n",
      "epoch 50, 1000/1018 batches, training loss 4.36, training perplexity 78.51\n",
      "\n",
      "epoch 50, validation loss 4.98, validation perplexity 145.72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_validation_loss = float(\"inf\")\n",
    "eps = 50\n",
    "best_model_so_far = None\n",
    "\n",
    "for ep in range(1, eps + 1):\n",
    "    ep_time_start = time.time()\n",
    "    train_model()\n",
    "    validation_loss = eval_model(transformer_model, validation_data)\n",
    "    print()\n",
    "    print(f\"epoch {ep:}, validation loss {validation_loss:.2f}, validation perplexity {math.exp(validation_loss):.2f}\")\n",
    "    print()\n",
    "\n",
    "    if validation_loss < min_validation_loss:\n",
    "        min_validation_loss = validation_loss\n",
    "        best_model_so_far = transformer_model\n",
    "\n",
    "    sched_module.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss 4.92, testing perplexity 136.85\n"
     ]
    }
   ],
   "source": [
    "testing_loss = eval_model(best_model_so_far, testing_data)\n",
    "print(f\"testing loss {testing_loss:.2f}, testing perplexity {math.exp(testing_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_pth = './transformer.pth'\n",
    "torch.save(best_model_so_far.state_dict(), mdl_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best trained model\n",
    "transformer_cached = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, \n",
    "                                     dropout).to(device)\n",
    "transformer_cached.load_state_dict(torch.load(mdl_pth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It will be used to the first season , and the\n"
     ]
    }
   ],
   "source": [
    "ln = 10\n",
    "sntc = 'It will _'\n",
    "sntc_split = sntc.split()\n",
    "torch.manual_seed(799)\n",
    "with torch.no_grad():\n",
    "    for i in range(ln):\n",
    "        sntc = ' '.join(sntc_split)\n",
    "        txt_ds = TEXT.numericalize([sntc_split])\n",
    "        num_b = txt_ds.size(0)\n",
    "        txt_ds = txt_ds.narrow(0, 0, num_b)\n",
    "        txt_ds = txt_ds.view(1, -1).t().contiguous().to(device)\n",
    "        ev_X, _ = return_batch(txt_ds, i+1)\n",
    "        op = transformer_cached(ev_X)\n",
    "        op_flat = op.view(-1, num_tokens)\n",
    "        res = TEXT.vocab.itos[op_flat.argmax(1)[0]]\n",
    "        sntc_split.insert(-1, res)\n",
    "print(sntc[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
