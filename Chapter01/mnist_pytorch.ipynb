{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.306125\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.623073\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 0.998695\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.953389\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 1.054391\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.393427\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.235708\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.284237\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.203838\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.292076\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.541438\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.411091\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.323946\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.296546\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.245537\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.505977\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.085587\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.193239\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.271752\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.253863\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.118738\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.138105\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.321052\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.248665\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.156183\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.080380\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.452029\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.279626\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.080688\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.055872\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.707169\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.557928\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.155603\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.184756\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.157801\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.146767\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.053524\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.242558\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.265810\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.328800\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.097469\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.172620\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.512803\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.318867\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.112234\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.228025\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.124887\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.171314\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.105865\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.348625\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.094032\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.052390\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.127645\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.139302\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.130473\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.130233\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.015999\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.154258\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.068885\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.041804\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.049387\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.059231\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.265435\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.225469\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.042348\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.283221\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.327328\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.030135\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.036622\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.027679\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.016683\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.007320\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.107536\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.069706\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.031448\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.088972\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.135416\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.061262\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.151354\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.026904\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.276452\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.145745\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.047698\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.218602\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.035469\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.062990\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.062250\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.015767\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.037681\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.039255\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.069786\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.531781\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.046213\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.034644\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.166643\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.108415\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.045014\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.079953\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.107433\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.038264\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.050863\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.248498\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.206447\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.008485\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.014419\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.389796\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.030377\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.017833\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.021203\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.407655\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.057148\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.020020\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.074923\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.119990\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.056903\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.080055\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.053022\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.135711\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.019722\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.010968\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.146617\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.004767\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.143455\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.014222\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.061148\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.016682\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.037607\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.052137\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.036558\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.066508\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.467837\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.163429\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.293707\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.001829\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.039151\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.161880\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.078123\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.059555\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.110429\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.187784\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.016655\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.022338\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.247052\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.031334\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.025583\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.141948\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.002185\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.087026\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.070295\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.022813\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.009519\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.035896\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.019081\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.195093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.020525\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.099966\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.005760\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.181415\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.023171\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.199064\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.016083\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.081108\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.019530\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.019478\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.057639\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.024935\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.019967\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.061209\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.020079\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.005709\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.174045\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.034105\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.013307\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.016652\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.104742\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.125356\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.013626\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.017631\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.009143\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.002521\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.028026\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.136143\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.029974\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.011801\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.087519\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.038683\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.065530\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.020779\n",
      "\n",
      "Test dataset: Overall Loss: 0.0537, Overall Accuracy: 9828/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.093643\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.002814\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.091251\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.076164\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.297487\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.025480\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.021241\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.095956\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.019420\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.021627\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.042636\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.032249\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.082223\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.001581\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.136491\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.160238\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.062409\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.123253\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.382645\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.064442\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.008793\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.151993\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.028848\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.022877\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.007498\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.011986\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.020524\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.051325\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.022165\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.044928\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.096222\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.004840\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.356069\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.058147\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.065284\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.089083\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.043277\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.013320\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.004100\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.321202\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.024610\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.037513\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.011071\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.055932\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.029711\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.195311\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.037828\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.063633\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.005364\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.003801\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.007853\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.018164\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.037415\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.126374\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.012573\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.019131\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.009665\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.028543\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.009554\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.019721\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.102569\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.117973\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.015409\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.004251\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.040685\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.005371\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.000976\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.006464\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.003544\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.066024\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.015158\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.010942\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.046487\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.009116\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.010374\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.005940\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.131002\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.067967\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.190824\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.078676\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.150366\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.062003\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.021158\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.002710\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.067187\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.676063\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.063514\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.001621\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.075475\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.004947\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.009329\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.004658\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.012837\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.012022\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.086399\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.016282\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.021320\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.023098\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.011966\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.015573\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.002900\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.051183\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.017080\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.113290\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.052394\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.006324\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.012563\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.158295\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.182156\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.008656\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.033024\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.062747\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.019955\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.029758\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.006914\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.010798\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.045478\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.053858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.002048\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.038252\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.295079\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.136733\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.101987\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.001379\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.087541\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.087263\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.083525\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.018027\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.113435\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.010311\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.009905\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.033104\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.129157\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.008110\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.045279\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.002742\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.134861\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.034957\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.002557\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.006538\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.137694\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.124644\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.015540\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.001489\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.014827\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.003033\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.075996\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.059429\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.199707\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.095703\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.013267\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.008151\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.010879\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.013054\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.021436\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.002176\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.082190\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.003117\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.044491\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.047525\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.099536\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.022413\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.090080\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.265148\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.057649\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.004042\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.039657\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.030423\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.026913\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.029736\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.003423\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.014260\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.006774\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.019563\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.054667\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.072877\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.112689\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.003503\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.002715\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.089225\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.184287\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.044174\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.097794\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.018629\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.062386\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.031968\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.009200\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.021790\n",
      "\n",
      "Test dataset: Overall Loss: 0.0489, Overall Accuracy: 9850/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 7\n",
      "Ground truth is : 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
